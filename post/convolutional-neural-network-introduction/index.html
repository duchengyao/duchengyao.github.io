<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/6.0.5/fancybox/fancybox.css" integrity="sha256-uTcjoMD6rPt4OyV3Rs02Slxl0BJGMNVKAm/1eYPt2go=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"s1nh.org","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":true,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="学到卷积神经网络，搜索到的资料一般都是这样的">
<meta property="og:type" content="article">
<meta property="og:title" content="什么是卷积神经网络">
<meta property="og:url" content="http://s1nh.org/post/convolutional-neural-network-introduction/index.html">
<meta property="og:site_name" content="S1NH">
<meta property="og:description" content="学到卷积神经网络，搜索到的资料一般都是这样的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_a1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_a2.jpg-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_a3.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_perceptron_1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_perceptron_2.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_perceptron_3.jpg-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_conv_0.gif">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_conv_1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_conv_2.jpg-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_conv_3.jpg-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Tensorflow_MNIST_3.png">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_inception_1.jpg-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_inception_2.jpg-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_resnet_1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_densenet_1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_conclu_2.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/CNN_conclu_1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_b1.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_b2.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_b3.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_b4.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_b5.png-QNthin">
<meta property="og:image" content="http://qiniu.s1nh.org/Blog_Convolution_b6.jpg-QNthin">
<meta property="article:published_time" content="2016-08-02T23:06:35.000Z">
<meta property="article:modified_time" content="2025-07-10T15:58:05.472Z">
<meta property="article:author" content="S1NH">
<meta property="article:tag" content="卷积">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiniu.s1nh.org/Blog_Convolution_a1.png-QNthin">


<link rel="canonical" href="http://s1nh.org/post/convolutional-neural-network-introduction/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://s1nh.org/post/convolutional-neural-network-introduction/","path":"post/convolutional-neural-network-introduction/","title":"什么是卷积神经网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>什么是卷积神经网络 | S1NH</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-44KGHGDYG3"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-44KGHGDYG3","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js" defer></script>

  <script src="/js/third-party/analytics/baidu-analytics.js" defer></script>
  <script async src="https://hm.baidu.com/hm.js?4c66a84272e0f7943a305accf6dbdf41"></script>






  <script defer data-domain="s1nh.org" src="http://home.s1nh.com:32801/js/script.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/6.0.5/fancybox/fancybox.umd.js" integrity="sha256-UiSieVaV/DXce2LW7QH+o77w+AIoAvSCPBkezriZ2DQ=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.7.0/mermaid.min.js","integrity":"sha256-4+IKDqhZ/sXjc8Wtl2/MsxI4e0s1KpEVdbEP7V/Lz8U="}}</script>
  <script src="/js/third-party/tags/mermaid.js" defer></script>


  <script src="/js/third-party/fancybox.js" defer></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">S1NH</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">世界在旅程的尽头终结</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0x00-%E7%9E%8E%E6%89%AF%E6%B7%A1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF"><span class="nav-text">0x00 瞎扯淡 - 什么是卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%BB%99%E5%8D%B7%E7%A7%AF%E4%B8%80%E4%B8%AA%E9%80%9A%E4%BF%97%E7%9A%84%E8%A7%A3%E9%87%8A%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%80%E4%B8%AA%E8%A1%80%E8%85%A5%E7%9A%84%E8%AE%B2%E8%A7%A3"><span class="nav-text">1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x01-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">0x01 什么是神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%89"><span class="nav-text">1. 神经元（感知机）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%88%91%E4%BB%AC%E6%8A%8A%E7%A5%9E%E7%BB%8F%E5%85%83%E8%BF%9E%E6%8E%A5%E8%B5%B7%E6%9D%A5%EF%BC%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%89"><span class="nav-text">2. 我们把神经元连接起来（多层感知机）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x02-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%86%E5%8F%B2"><span class="nav-text">0x02 卷积神经网络历史</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8D%B7%E7%A7%AF"><span class="nav-text">1. 图像的卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-LeNet"><span class="nav-text">2. LeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-AlexNet"><span class="nav-text">3. AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-VGG"><span class="nav-text">4. VGG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-GoogLeNet"><span class="nav-text">5. GoogLeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-ResNet"><span class="nav-text">6. ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-DenseNet"><span class="nav-text">7. DenseNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-%E6%80%BB%E7%BB%93"><span class="nav-text">8. 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0xFF-%E9%99%84%E5%BD%95-%E7%9E%8E%E6%89%AF%E6%B7%A1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF"><span class="nav-text">0xFF 附录 瞎扯淡 - 什么是卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%BB%99%E5%8D%B7%E7%A7%AF%E4%B8%80%E4%B8%AA%E9%80%9A%E4%BF%97%E7%9A%84%E8%A7%A3%E9%87%8A%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%80%E4%B8%AA%E8%A1%80%E8%85%A5%E7%9A%84%E8%AE%B2%E8%A7%A3-1"><span class="nav-text">1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%89%A9%E7%90%86%E6%84%8F%E4%B9%89"><span class="nav-text">2. 物理意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%9B%B4%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E6%9D%A5%E4%BA%86"><span class="nav-text">3. 更有意思的来了</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E7%BB%93%E8%AE%BA"><span class="nav-text">4. 结论</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="S1NH"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">S1NH</p>
  <div class="site-description" itemprop="description">no other developers required.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">105</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">88</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>
<div>
  <hr>
  <img src="/images/wechatpay-sss.jpg" alt="Buy me a SSRI.">
  <a href="/reward-list/" rel="section">"Buy me a coffee."</a>
</div>

<!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  if (document.domain.indexOf('com') != -1){
    _paq.push(["setCookieDomain", "*.s1nh.com"]);
  } else if(document.domain.indexOf('org') != -1){
    _paq.push(["setCookieDomain", "*.s1nh.org"]);
  }
  _paq.push(["setDomains", ["*.s1nh.org","*.s1nh.com"]]);
  _paq.push(["enableCrossDomainLinking"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://home.s1nh.com:32443/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    
    if (document.domain.indexOf('com') != -1){
      _paq.push(['setSiteId', '2']);
    } else if(document.domain.indexOf('org') != -1){
      _paq.push(['setSiteId', '3']);
    }
    
    // Will also collect the website data into Website ID = 1
    _paq.push(['addTracker', u+'matomo.php', 1]);
    
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>

<!-- Google Ads -->
<!--script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7052751443200899"
     crossorigin="anonymous"></script-->

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.92ez.com/" title="https:&#x2F;&#x2F;www.92ez.com&#x2F;" rel="noopener" target="_blank">一只猿 (关站升级中)</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://codewithzhangyi.com/about/" title="http:&#x2F;&#x2F;codewithzhangyi.com&#x2F;about&#x2F;" rel="noopener" target="_blank">好奇心狂热分子 (张怡，也关了)</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://findhao.net/" title="https:&#x2F;&#x2F;findhao.net&#x2F;" rel="noopener" target="_blank">FindHao 的自留地</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://s1nh.org/post/convolutional-neural-network-introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="S1NH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="S1NH">
      <meta itemprop="description" content="no other developers required.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="什么是卷积神经网络 | S1NH">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          什么是卷积神经网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2016-08-03 07:06:35" itemprop="dateCreated datePublished" datetime="2016-08-03T07:06:35+08:00">2016-08-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">算法与硬件</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>学到卷积神经网络，搜索到的资料一般都是这样的</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_a1.png-QNthin" alt="神经网络"></p>
<span id="more"></span>

<p>或是这样的</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_a2.jpg-QNthin" alt="神经网络"></p>
<p>这尼玛什么鬼，故弄玄虚忽悠投资人的吧。。再搜一下什么BP神经网络，深度神经网络，所有的教材都像下图一样，一大堆圈圈，无数跟线连起来，显得那么装逼，可菜鸟们就是看不懂到底是啥。</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_a3.png-QNthin" alt="神经网络"></p>
<!-- 有关其它神经网络的介绍请异步另一篇文章： -->


<h2 id="0x00-瞎扯淡-什么是卷积"><a href="#0x00-瞎扯淡-什么是卷积" class="headerlink" title="0x00 瞎扯淡 - 什么是卷积"></a>0x00 瞎扯淡 - 什么是卷积</h2><p>我第一次接触到卷积是本科2年级，为了做图像识别，准备把这些基础知识恶补一下。可是翻开教材，满书的“傅里叶变换”“线性时不变系统”…… 果断的就放弃了。。</p>
<ul>
<li>难懂之因：<em>为了数学美，拆卸了脚手架。教科书书常用“定义—定理”的体系，先给出数学定义，然后给出若干性质，从公式到公式，逐步推导。有的教科书采用用信号“反褶、平移、相乘、积分”给出几何解释，属于用数学解释数学，提问者不满足这种解释。这不是当年发明卷积的大师们的“需求–猜想—发现—证明—应用”的路径，大师们建设好“卷积”大厦后，为了数学美，拆卸了脚手架，现在人们看到的是炼成的钢铁，看不出钢铁是怎样炼成的。造成了部分非数学专业学生的一个难点。——<a target="_blank" rel="noopener" href="http://blog.sciencenet.cn/home.php?mod=space&uid=287179&do=blog&id=425373">唐常杰</a></em></li>
</ul>
<p>其实卷及根本就没那么麻烦，编书的老师简直就在卖弄自己的数学功底。看看下面的解释，卷积这个高达上的名词瞬间就变得通俗易懂了。</p>
<h4 id="1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解"><a href="#1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解" class="headerlink" title="1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解"></a>1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解</h4><p><code>因为关于卷积的内容太血腥，于是我把它移到了附录里面。按几下空格或者PageDown即可观看</code></p>
<h2 id="0x01-什么是神经网络"><a href="#0x01-什么是神经网络" class="headerlink" title="0x01 什么是神经网络"></a>0x01 什么是神经网络</h2><p>神经网络形象的可以表示成本文开始的那张图，一堆圈圈分成一层一层的被一些线连接起来。那么每一部分是什么意思呢，让我们从神经元开始讲起。</p>
<h3 id="1-神经元（感知机）"><a href="#1-神经元（感知机）" class="headerlink" title="1. 神经元（感知机）"></a>1. 神经元（感知机）</h3><p>学过高中生物的都知道，神经元分为<code>树突（dendrite）</code>、<code>轴突（axon）</code>和<code>胞体</code>。其中，树突具有接受刺激并将冲动传入细胞体的功能，细胞体对输入的刺激进行计算，最后由轴突的分枝把神经冲动传给其他神经元或效应器。上个世纪六十年代（1958年）<code>感知机</code>模型被提出，如下图所示。感知机的左边是的树突，可以接受传入信号（x1,x2,x3）；右边是轴突，负责输出信号。那么如何处理输入信号呢，为了简化模型，我们先认为<code>如果输入的都是1,那么输出1,否则输出0</code>。</p>
<p><img data-src="//qiniu.s1nh.org/CNN_perceptron_1.png-QNthin"></p>
<p>简化的感知机模型未免也太弱智了，必须所有的传入神经同时刺激胞体才能使此神经元有响应（激活）。于是科学家们把弱智的神经元胞体改造了一下，成为下面这样子（图片来自网络，所以下图的y对应于上图的output）</p>
<p><img data-src="//qiniu.s1nh.org/CNN_perceptron_2.png-QNthin"></p>
<p>这就有点高级了，首先，每个传入信号$x_i$都被标注了权重$w_i$，还增加了偏置<code>b</code>（bias)。其中c是组合函数（通常情况是求和），a为激活函数（这个有点复杂，有兴趣的自行google）。<strong>别被上面的图搞糊涂，其实神经元（感知机）就是一个数学公式：$y&#x3D;(x_1 * w_1+x_2 * w_2+…+x_n * w_n+b)*a$ 即首先对所有的输入数据乘以对应的权重并求和，最后与激活函数相乘。</strong></p>
<p>也许你会发现_卧槽这么简单，小学乘法呀，那有什么卵用_。感知机能（且一定能）将线性可分的数据集分开，想不明白的可以看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27224109">这篇文章</a>的第一节<code>感知机能做什么？</code>。除了区分线性可分数据集以外，缺失没什么卵用了，连异或（XOR）问题都解决不了。</p>
<h3 id="2-我们把神经元连接起来（多层感知机）"><a href="#2-我们把神经元连接起来（多层感知机）" class="headerlink" title="2. 我们把神经元连接起来（多层感知机）"></a>2. 我们把神经元连接起来（多层感知机）</h3><p>自从发现感知机对除了线性可分的数据外没什么卵用以后，感知机的研究进入了寒冬，直到有个大神把几个感知机链接起来成下面的样子：</p>
<p><img data-src="//qiniu.s1nh.org/CNN_perceptron_3.jpg-QNthin"></p>
<p>对于异或（XOR）问题，只要像上图一样，把三个神经元连接成两层，第一层神经元分别激活<code>x+y&gt;0</code> <code>x+y&lt;2</code>，第二层的神经元像刚开始的弱智神经元一样，如果第一层神经元的输出都为1，那么输出为1即可。</p>
<p>**这么简单的思路，为什么还会进入那么久的寒冬？**其实不是上个世纪的人智商太低，而是因为这种多层网络无法训练。也就是说，以当时的技术无法确定神经元中w的值，所以对于输入信号多层感知机就不知道怎么激活了。直到BP算法的出现（BP算法请自行google）。<em>其实1968年BP算法就被作者认识到了，并在1974年写在了毕业论文中，只是因为寒蝉效应没人理他。直到1986年这种方法才流行开来。</em></p>
<p>可以证明，<a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap4.html">多层神经元可以表达所有连续函数</a>。因此，理论上讲，只要网络足够大，层数足够多，神经网络是可以拟合各种问题的！（当然网络太大就过拟合了- -）</p>
<h2 id="0x02-卷积神经网络历史"><a href="#0x02-卷积神经网络历史" class="headerlink" title="0x02 卷积神经网络历史"></a>0x02 卷积神经网络历史</h2><p>假设我们要对100×100像素的图片进行分类，如果用上面的多层感知机来做（现在叫全连接层），那么输入数据就是10000个，如果再来几十层网络，每层几千个节点，那么估计也只有朝鲜的计算机才能算出来了吧&gt;_&lt;</p>
<h3 id="1-图像的卷积"><a href="#1-图像的卷积" class="headerlink" title="1. 图像的卷积"></a>1. 图像的卷积</h3><p>之前接触过图像算法的同鞋们对卷积一定很熟悉，什么<code>Laplacian</code>，<code>soble</code>算子之类的讲的就是卷积核。图像卷积的计算方法其实就是<strong>滑动窗口和加权平均</strong>（如下图所示）。对于原始图片，用一个固定的卷积核在图片上滑动，每滑动一个像素就计算对应位置的加权和并记录下来。</p>
<p><img data-src="//qiniu.s1nh.org/CNN_conv_0.gif"></p>
<p><img data-src="//qiniu.s1nh.org/CNN_conv_1.png-QNthin"></p>
<p>图像经过不同的卷积核后产生的新图像是不同的，比如下图的原始图片分别用了低通和高通卷积核后的结果。</p>
<p><img data-src="//qiniu.s1nh.org/CNN_conv_2.jpg-QNthin"></p>
<p>再比如下图（原始图片分别用之前说的Laplacian，soble卷积核卷一下）：</p>
<p><img data-src="//qiniu.s1nh.org/CNN_conv_3.jpg-QNthin"></p>
<p>经过观察以后，不难明白卷积对于图像的意义是<strong>提取图像不同频段的特征</strong></p>
<h3 id="2-LeNet"><a href="#2-LeNet" class="headerlink" title="2. LeNet"></a>2. LeNet</h3><p>上一章讲了1986年BP算法开始流行，证明了多层网络可以训练以后，Yann LeCun大牛像我一样意识到不能用全连接层进行图片分类后就开始了卷及神经网络的研究，并解决了应用在书信邮编中的手写数字的分类问题。其实只用模板匹配（只有10个神经元的单层感知机）的方法也能进行识别，但是效果只能达到92%（多层感知机为98%）。模板匹配的方法跟卷积一模一样，只是卷积核的大小等于输入图片的大小，我训练的模板如下图，代码见之前的博文：</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Tensorflow_MNIST_3.png"></p>
<p>LeNet的基本原理就是对输入图像一层一层的卷积，最后接一个softmax把结果分为10类。<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">论文</a>实在是太长了，有兴趣的同鞋可以看看。</p>
<h3 id="3-AlexNet"><a href="#3-AlexNet" class="headerlink" title="3. AlexNet"></a>3. AlexNet</h3><p>上个世纪的计算机硬件实在太差了，卷积网络又是个耗资源的方法，所以没被广泛应用。（同等资源下<code>传统特征+SVM</code>也可以取得相当的效果，看过上个世纪<code>Andrew Ng</code>机器学习课的应该都知道，神经网络的讲解几乎是一带而过，重点都是SVM、AdaBoost、随机森林、GBDT、LR、FTRL这些概念）</p>
<p>到了2012年，Alex借助GPGPU技术，使用了两个NVIDIA GPU进行并行训练（话说作者确实牛逼，用GPU就算了，还来了个多GPU并行），使得top5错误率一下子降低到了17.0%</p>
<p>除了<code>多GPU</code>+更深的<code>网络层数</code>，AlexNet还体现了<code>LRN</code> <code>Dropout</code> <code>ReLU</code>等现在流行的方法（再次膜拜）。</p>
<h3 id="4-VGG"><a href="#4-VGG" class="headerlink" title="4. VGG"></a>4. VGG</h3><p><strong>没有什么问题是多层卷积网络解决不了的，如果有，那就再加10层</strong>。于是<code>Karen Simonyan</code>就把AlexNet的网络由8层加深到19层，取名叫VGG，夺得了当年比赛的第二名。（其实也有新技术啦，比如把卷积核变小- -）</p>
<h3 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h3><p><code>当年VGG运气不好，碰到了GoogLeNet，否则就是比赛第一名了。</code></p>
<p>为什么GoogLeNet排名第一呢，是的，因为比VGG还要深十几层。但是，巨量参数容易产生过拟合也会增加计算量，说白了就是网络<code>无法训练</code>。因此GoogLeNet提出了一个叫做Inception的子结构，如下图：</p>
<p><img data-src="//qiniu.s1nh.org/CNN_inception_1.jpg-QNthin"></p>
<p>Inception将一个图像分别用不同大小的卷积核卷一下，然后连接起来，据说让参数量降低了好几倍。其中Inception其实有好多好多版本，有兴趣的同鞋可以自定Google。完整的GoogLeNet如下图。</p>
<p><img data-src="//qiniu.s1nh.org/CNN_inception_2.jpg-QNthin"></p>
<h3 id="6-ResNet"><a href="#6-ResNet" class="headerlink" title="6. ResNet"></a>6. ResNet</h3><p>GoogLeNet在某种程度上可以看作把网络加宽了，而ResNet不一样，依然坚信着<strong>没有最深只有更深</strong>。何凯明一口气把网络加深到了一百多层，然后在当年的ImageNet比赛中拿了第一。</p>
<p>然而其实并没有这么简单（据说朝鲜早就在研究1000层的神经网络了，也没见他们得奖）。大神设计了一个叫做残差的模块（如下图），一层网络不仅仅连接着下一层，还通过飞线跨过几层走到了后边。</p>
<p><img data-src="//qiniu.s1nh.org/CNN_resnet_1.png-QNthin"></p>
<h3 id="7-DenseNet"><a href="#7-DenseNet" class="headerlink" title="7. DenseNet"></a>7. DenseNet</h3><p>ResNet 虽然效果了不得，但速度确实有点慢。ResNet的思想是把一层的输出跨几层传递到后面，DenseNet的思路更简单粗暴，把所有的层都连接起来了（在连接处的区别为ResNet是求和，DenseNet是concat）,实验效果超好。</p>
<p><img data-src="//qiniu.s1nh.org/CNN_densenet_1.png-QNthin"></p>
<h3 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h3><p><img data-src="//qiniu.s1nh.org/CNN_conclu_2.png-QNthin"></p>
<p>做目标检测（YOLO）的官网有个<a target="_blank" rel="noopener" href="https://pjreddie.com/darknet/imagenet/#alexnet">统计</a>，我贴出来供参考：</p>
<p><img data-src="//qiniu.s1nh.org/CNN_conclu_1.png-QNthin"></p>
<ul>
<li><p><code>AlexNet</code>: The model that started a revolution! The original model was crazy with the split GPU thing so this is the model from some follow-up work.</p>
</li>
<li><p><code>Darknet Reference Model</code>: This model is designed to be small but powerful. It attains the same top-1 and top-5 performance as AlexNet but with 1&#x2F;10th the parameters. It uses mostly convolutional layers without the large fully connected layers at the end. It is about twice as fast as AlexNet on CPU making it more suitable for some vision applications.</p>
</li>
<li><p><code>VGG-16</code>: The Visual Geometry Group at Oxford developed the VGG-16 model for the ILSVRC-2014 competition. It is highly accurate and widely used for classification and detection. I adapted this version from the Caffe pre-trained model. It was trained for an additional 6 epochs to adjust to Darknet-specific image preprocessing (instead of mean subtraction Darknet adjusts images to fall between -1 and 1).</p>
</li>
<li><p><code>Extraction</code>: I developed this model as an offshoot of the GoogleNet model. It doesn’t use the “inception” modules, only 1x1 and 3x3 convolutional layers.</p>
</li>
<li><p><code>Darknet19</code>: I modified the Extraction network to be faster and more accurate. This network was sort of a merging of ideas from the Darknet Reference network and Extraction as well as numerous publications like Network In Network, Inception, and Batch Normalization.</p>
</li>
<li><p><code>Darknet19 448x448</code>: I trained Darknet19 for 10 more epochs with a larger input image size, 448x448. This model performs significantly better but is slower since the whole image is larger.</p>
</li>
<li><p><code>Resnet 50</code>: For some reason people love these networks even though they are so sloooooow. Whatever.</p>
</li>
<li><p><code>Resnet 152</code>: For some reason people love these networks even though they are so sloooooow. Whatever.</p>
</li>
<li><p><code>Densenet 201</code>: I love DenseNets! They are just so deep and so crazy and work so well. Like Resnet, still slow since they are sooooo many layers but at least they work really well!</p>
</li>
</ul>
<h2 id="0xFF-附录-瞎扯淡-什么是卷积"><a href="#0xFF-附录-瞎扯淡-什么是卷积" class="headerlink" title="0xFF 附录 瞎扯淡 - 什么是卷积"></a>0xFF 附录 瞎扯淡 - 什么是卷积</h2><p><code>接着第0章的卷积继续说</code></p>
<h4 id="1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解-1"><a href="#1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解-1" class="headerlink" title="1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解"></a>1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解</h4><p>比如说你的老板命令你干活，你却到楼下打台球去了，后来被老板发现，他非常气愤，扇了你一巴掌（注意，这就是输入信号，脉冲），于是你的脸上会渐渐地（贱贱地）鼓起来一个包，你的脸就是一个系统，而鼓起来的包就是你的脸对巴掌的响应，好，这样就和信号系统建立起来意义对应的联系。下面还需要一些假设来保证论证的严谨：假定你的脸是线性时不变系统，也就是说，无论什么时候老板打你一巴掌，打在你脸的同一位置（这似乎要求你的脸足够光滑，如果你说你长了很多青春痘，甚至整个脸皮处处连续处处不可导，那难度太大了，我就无话可说了哈哈），你的脸上总是会在相同的时间间隔内鼓起来一个相同高度的包来，并且假定以鼓起来的包的大小作为系统输出。好了，那么，下面可以进入核心内容——卷积了！</p>
<p>如果你每天都到地下去打台球，那么老板每天都要扇你一巴掌，不过当老板打你一巴掌后，你5分钟就消肿了，所以时间长了，你甚至就适应这种生活了……如果有一天，老板忍无可忍，以0.5秒的间隔开始不间断的扇你的过程，这样问题就来了，第一次扇你鼓起来的包还没消肿，第二个巴掌就来了，你脸上的包就可能鼓起来两倍高，老板不断扇你，脉冲不断作用在你脸上，效果不断叠加了，这样这些效果就可以求和了，结果就是你脸上的包的高度随时间变化的一个函数了（注意理解）；如果老板再狠一点，频率越来越高，以至于你都辨别不清时间间隔了，那么，求和就变成积分了。可以这样理解，在这个过程中的某一固定的时刻，你的脸上的包的鼓起程度和什么有关呢？和之前每次打你都有关！但是各次的贡献是不一样的，越早打的巴掌，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是你脸上的包的大小随时间变化的函数。本来你的包几分钟就可以消肿，可是如果连续打，几个小时也消不了肿了，这难道不是一种平滑过程么？反映到剑桥大学的公式上，f(a)就是第a个巴掌，g(x-a)就是第a个巴掌在x时刻的作用程度，乘起来再叠加就ok了，大家说是不是这个道理呢？我想这个例子已经非常形象了，你对卷积有了更加具体深刻的了解了吗？</p>
<p>看到这里，我们简直不能更明白，什么是卷积了</p>
<ul>
<li>时不变系统：就是系统的参数不随时间而变化，即不管输入信号作用的时间先后，输出信号响应的形状均相同。就是说不管老板什么时候打你，肿胀发展的程度是一样的</li>
<li>线性时不变系统：在时不变系统中满足叠加原理。即老板在t时刻打你N巴掌，那么肿胀的程度成N倍增加。</li>
</ul>
<h4 id="2-物理意义"><a href="#2-物理意义" class="headerlink" title="2. 物理意义"></a>2. 物理意义</h4><p>我们先画几个简单的函数图像，理解“卷积”的<strong>物理意义</strong>，暂时不管它为什么要对称，为什么要反转。</p>
<p>已知f(t)为老板在t时刻打了你f(t)巴掌。如下图，老板分别在第0秒时打了2巴掌，第1秒打了1巴掌，第2秒打了3巴掌；g(t)为打完巴掌后的t时刻肿胀的程度，下图刚打完时肿胀程度为2，等了1秒后变大为3，2秒后变成1，3秒后完全消肿。</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_b1.png-QNthin" title="左图是f(t)＝[2, 1, 3]，右图是g(t)&#x3D;[2, 3, 1]"></p>
<p>下面三个图$h_0(t)$，$h_1(t)$，$h_2(t)$，分别是f(0),f(1),f(2)单独作用时的h(x)值，也就是分别为第0秒打了f(0)&#x3D;2巴掌，第1秒打f(1)&#x3D;1巴掌，第2秒打了f(2)&#x3D;3巴掌的肿胀曲线。</p>
<p>计算方法为：$h_0(t) &#x3D; f(0) * g(t) &#x3D; 2 * [2, 3, 1] &#x3D; [4, 6, 2]$</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_b2.png-QNthin" title="$h0(t)$"></p>
<p>$h_1(t) &#x3D; f(1) * g(t) &#x3D; 1 * [2, 3, 1] $ 然后向右平移1个单位</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_b3.png-QNthin" title="$h_1(t)$"></p>
<p>$h_2(t) &#x3D; f(2) * g(t) &#x3D; &#x3D; 3 * [2, 3, 1] &#x3D; [6, 9, 3]$ 然后向右平移2个单位</p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_b4.png-QNthin" title="$h2(t)$"></p>
<p>很明显，要求t时间的作用程度h(t)，就是把上面三个函数加起来$h(t)&#x3D;h_0(t)+h_1(t)+h_2(t)$ </p>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_b5.png-QNthin" title="$h(t)$"></p>
<ul>
<li>说白了，卷积就是<strong>平移、叠加</strong>。（从这里，可以看到卷积的重要的物理意义是：一个函数（如：单位响应）在另一个函数（如：输入信号）上的<strong>加权叠加</strong>。）</li>
</ul>
<p>由上图可知，你在第三秒的时候肿胀程度为10. (记住这个结果，后面要放大招了）</p>
<h4 id="3-更有意思的来了"><a href="#3-更有意思的来了" class="headerlink" title="3. 更有意思的来了"></a>3. 更有意思的来了</h4><p>我们接下来做一个简单的初中数学题。求$(3x^2+x+2)*(x^2+3x+2)$ 中$x^3$的系数。</p>
<p><strong>普通方法</strong></p>
<p>常规做法是先把它合并同类项：</p>
<p>$(3x^2+x+2)*(x^2+3x+2)$<br>$&#x3D;(3x^4+9x^3+6x^2)+(x^3+3x^2+2x)+(2x^2+6x+4)$<br>$&#x3D;3x^4+10x^3+11x^2+8x+4$</p>
<p>进行完计算后，我们一眼就可以看出$x^3$的系数是10. （这种算法，我们一共进行了<strong>9次乘法运算和4次加法运算</strong>）</p>
<p><strong>文艺方法</strong></p>
<p>还是刚才那个题目，进行如下计算：</p>
<ol>
<li>我们把第一个多项式反过来，使其一个降幂排列，一个升幂排列。公式变成了$(2+x+3x^2)*(x^2+3x+2)$</li>
<li>平移：把第二个多项式每次向右平移一项</li>
<li>相乘：竖直对齐的项分别相乘</li>
<li>求和：相乘的结果相加</li>
</ol>
<p><img data-src="//qiniu.s1nh.org/Blog_Convolution_b6.jpg-QNthin"></p>
<p><strong>反褶，平移，相乘，求和</strong>，这就是卷积的计算过程。（我们在求$x^3$的系数时，只需要进行<strong>1次平移运算，2次乘法，1次加法运算</strong>）</p>
<h4 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h4><p>不知道有没有人注意到，我们刚才计算的<strong>方法1</strong>其实就是前面用那一大堆图描述的方法，注意看公式X的指数就是图像的横坐标，系数为纵坐标的值。（上题x^3的系数为10，第3秒时肿胀程度也恰好为10。）</p>
<p>运用在连续函数中，只需把最后一步的求和改成求积分即可。</p>
<p>参考：</p>
<ul>
<li></li>
<li><a target="_blank" rel="noopener" href="http://blog.sciencenet.cn/home.php?mod=space&uid=287179&do=blog&id=425373">辐射、服碘、补盐、空袭和卷积—–教学难点讨论之一</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>请我一杯咖啡吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay-ssss.jpg" alt="S1NH 微信">
        <span>微信</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>S1NH
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://s1nh.org/post/convolutional-neural-network-introduction/" title="什么是卷积神经网络">http://s1nh.org/post/convolutional-neural-network-introduction/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E5%8D%B7%E7%A7%AF/" rel="tag"># 卷积</a>
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/matplotlib-animation/" rel="prev" title="「草稿」使用 mplot3d 绘制动态 3D 图">
                  <i class="fa fa-angle-left"></i> 「草稿」使用 mplot3d 绘制动态 3D 图
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/post/to-be-a-PM/" rel="next" title="假装自己是个产品经理">
                  假装自己是个产品经理 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2015 – 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">S1NH</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/duchengyao" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"duchengyao/duchengyao.github.io","issue_term":"og:title","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js" defer></script>

</body>
</html>
