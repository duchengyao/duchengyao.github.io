<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/6.0.5/fancybox/fancybox.css" integrity="sha256-uTcjoMD6rPt4OyV3Rs02Slxl0BJGMNVKAm/1eYPt2go=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"s1nh.org","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.2","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":true,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="本资源来自网络，侵权请按ALT+F4 0x01 深度学习基础框架1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文(1) 人脸检测在人脸检测方面常用的用两个，一个是 Tinyface 能检测到比较小的人脸。可以先玩通demo另外一篇更为常用，如果你们对固定场景，比如视频对话，检测效果不错。而且他们能标注人脸关键点。文章：Joint Face Detection and Alignment u">
<meta property="og:type" content="article">
<meta property="og:title" content="计算机视觉技术调查报告">
<meta property="og:url" content="http://s1nh.org/post/cv-sjtu/index.html">
<meta property="og:site_name" content="S1NH">
<meta property="og:description" content="本资源来自网络，侵权请按ALT+F4 0x01 深度学习基础框架1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文(1) 人脸检测在人脸检测方面常用的用两个，一个是 Tinyface 能检测到比较小的人脸。可以先玩通demo另外一篇更为常用，如果你们对固定场景，比如视频对话，检测效果不错。而且他们能标注人脸关键点。文章：Joint Face Detection and Alignment u">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2017-07-14T05:05:06.000Z">
<meta property="article:modified_time" content="2025-07-10T15:58:05.473Z">
<meta property="article:author" content="S1NH">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://s1nh.org/post/cv-sjtu/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://s1nh.org/post/cv-sjtu/","path":"post/cv-sjtu/","title":"计算机视觉技术调查报告"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>计算机视觉技术调查报告 | S1NH</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-44KGHGDYG3"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-44KGHGDYG3","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js" defer></script>

  <script src="/js/third-party/analytics/baidu-analytics.js" defer></script>
  <script async src="https://hm.baidu.com/hm.js?4c66a84272e0f7943a305accf6dbdf41"></script>






  <script defer data-domain="s1nh.org" src="http://home.s1nh.com:32801/js/script.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/6.0.5/fancybox/fancybox.umd.js" integrity="sha256-UiSieVaV/DXce2LW7QH+o77w+AIoAvSCPBkezriZ2DQ=" crossorigin="anonymous" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script>
<script src="/js/third-party/search/local-search.js" defer></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.7.0/mermaid.min.js","integrity":"sha256-4+IKDqhZ/sXjc8Wtl2/MsxI4e0s1KpEVdbEP7V/Lz8U="}}</script>
  <script src="/js/third-party/tags/mermaid.js" defer></script>


  <script src="/js/third-party/fancybox.js" defer></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">S1NH</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">世界在旅程的尽头终结</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0x01-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A1%86%E6%9E%B6"><span class="nav-text">0x01 深度学习基础框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E3%80%81%E8%B7%9F%E8%B8%AA%E3%80%81%E8%AF%86%E5%88%AB%E3%80%81%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1%E7%9A%84%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6%E3%80%81%E7%AE%97%E6%B3%95%E3%80%81%E8%AE%BA%E6%96%87"><span class="nav-text">1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B"><span class="nav-text">(1) 人脸检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%BA%BA%E8%84%B8%E8%B7%9F%E8%B8%AA"><span class="nav-text">(2) 人脸跟踪</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB"><span class="nav-text">(3) 人脸识别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E4%BA%BA%E8%84%B8%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1"><span class="nav-text">(4) 人脸三维建模</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E3%80%81%E7%89%A9%E4%BD%93%E5%AE%9A%E4%BD%8D%E3%80%81%E8%A7%86%E9%A2%91%E4%B8%AD%E7%9A%84%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E3%80%81%E5%9C%BA%E6%99%AF%E5%88%86%E7%B1%BB%E3%80%81%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90%E7%9A%84%E7%AE%97%E6%B3%95%E3%80%81%E8%AE%BA%E6%96%87"><span class="nav-text">2.物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E5%92%8C%E7%89%A9%E4%BD%93%E5%AE%9A%E4%BD%8D"><span class="nav-text">(1) 物体探测和物体定位</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%A7%86%E9%A2%91%E4%B8%AD%E7%9A%84%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B"><span class="nav-text">(2) 视频中的物体探测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%9C%BA%E6%99%AF%E5%88%86%E7%B1%BB%E3%80%81%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90"><span class="nav-text">(3) 场景分类、场景分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%9B%BE%E7%89%87%E5%9C%BA%E6%99%AF%E6%8F%8F%E8%BF%B0%E7%9A%84%E7%AE%97%E6%B3%95%E3%80%81%E8%AE%BA%E6%96%87"><span class="nav-text">3.图片场景描述的算法、论文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%A7%86%E9%A2%91%E5%9C%BA%E6%99%AF%E6%8F%8F%E8%BF%B0%E7%9A%84%E7%AE%97%E6%B3%95%E3%80%81%E8%AE%BA%E6%96%87"><span class="nav-text">4.视频场景描述的算法、论文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E4%BA%BA%E8%84%B8%E3%80%81%E7%89%A9%E4%BD%93%E3%80%81%E5%9C%BA%E6%99%AF%E7%AD%89%E7%AE%97%E6%B3%95%E7%9A%84%E8%AE%AD%E7%BB%83%E9%9B%86%E5%A4%A7%E5%B0%8F%E3%80%81%E8%AE%AD%E7%BB%83%E5%91%A8%E6%9C%9F%E5%8F%8A%E8%83%BD%E8%BE%BE%E5%88%B0%E7%9A%84%E6%95%88%E6%9E%9C%EF%BC%8C%E4%B8%80%E4%BA%9B%E8%B0%83%E5%8F%82%E3%80%81%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7%E7%AD%89"><span class="nav-text">5.人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E6%A3%80%E6%B5%8B%E7%9A%84%E7%AE%97%E6%B3%95"><span class="nav-text">6 自然场景中的文字检测的算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E4%B8%AD%E4%BA%BA%E7%89%A9%EF%BC%88%E4%B8%94%E6%9C%89%E9%81%AE%E6%8C%A1%EF%BC%8C%E5%A6%82%E4%BA%BA%E8%84%B8%E8%A2%AB%E9%81%AE%E6%8C%A1%EF%BC%89%E8%BE%83%E5%A4%9A%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B%E5%8F%8A%E5%86%8D%E8%AF%86%E5%88%AB%E3%80%82"><span class="nav-text">7 自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x02-%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="nav-text">0x02 相关问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x03-%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF"><span class="nav-text">0x03 学术前沿</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BD%93%E5%89%8D%E5%9B%BE%E5%83%8F%E3%80%81%E8%A7%86%E9%A2%91%E9%A2%86%E5%9F%9F%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%89%8D%E6%B2%BF%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E3%80%81%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%E5%8F%8A%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="nav-text">1.当前图像、视频领域的学术前沿研究方向、算法框架及相关论文</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="nav-text">(1) 生成对抗模型：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%B7%B1%E5%BA%A6%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0"><span class="nav-text">(2) 深度增强学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1"><span class="nav-text">(3) 人体姿态估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94"><span class="nav-text">(4) 视觉问答</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-Mask-R-CNN"><span class="nav-text">(5) Mask-R-CNN</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="S1NH"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">S1NH</p>
  <div class="site-description" itemprop="description">no other developers required.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">105</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">88</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>
<div>
  <hr>
  <img src="/images/wechatpay-sss.jpg" alt="Buy me a SSRI.">
  <a href="/reward-list/" rel="section">"Buy me a coffee."</a>
</div>

<!-- Matomo -->
<script>
  var _paq = window._paq = window._paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  if (document.domain.indexOf('com') != -1){
    _paq.push(["setCookieDomain", "*.s1nh.com"]);
  } else if(document.domain.indexOf('org') != -1){
    _paq.push(["setCookieDomain", "*.s1nh.org"]);
  }
  _paq.push(["setDomains", ["*.s1nh.org","*.s1nh.com"]]);
  _paq.push(["enableCrossDomainLinking"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://home.s1nh.com:32443/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    
    if (document.domain.indexOf('com') != -1){
      _paq.push(['setSiteId', '2']);
    } else if(document.domain.indexOf('org') != -1){
      _paq.push(['setSiteId', '3']);
    }
    
    // Will also collect the website data into Website ID = 1
    _paq.push(['addTracker', u+'matomo.php', 1]);
    
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>

<!-- Google Ads -->
<!--script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7052751443200899"
     crossorigin="anonymous"></script-->

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.92ez.com/" title="https:&#x2F;&#x2F;www.92ez.com&#x2F;" rel="noopener" target="_blank">一只猿 (关站升级中)</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://codewithzhangyi.com/about/" title="http:&#x2F;&#x2F;codewithzhangyi.com&#x2F;about&#x2F;" rel="noopener" target="_blank">好奇心狂热分子 (张怡，也关了)</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://findhao.net/" title="https:&#x2F;&#x2F;findhao.net&#x2F;" rel="noopener" target="_blank">FindHao 的自留地</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://s1nh.org/post/cv-sjtu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="S1NH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="S1NH">
      <meta itemprop="description" content="no other developers required.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="计算机视觉技术调查报告 | S1NH">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          计算机视觉技术调查报告
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-07-14 13:05:06" itemprop="dateCreated datePublished" datetime="2017-07-14T13:05:06+08:00">2017-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">算法与硬件</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><code>本资源来自网络，侵权请按ALT+F4</code></p>
<h2 id="0x01-深度学习基础框架"><a href="#0x01-深度学习基础框架" class="headerlink" title="0x01 深度学习基础框架"></a>0x01 深度学习基础框架</h2><h3 id="1-人脸检测、跟踪、识别、三维建模的开源框架、算法、论文"><a href="#1-人脸检测、跟踪、识别、三维建模的开源框架、算法、论文" class="headerlink" title="1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文"></a>1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文</h3><h4 id="1-人脸检测"><a href="#1-人脸检测" class="headerlink" title="(1) 人脸检测"></a>(1) 人脸检测</h4><p>在人脸检测方面常用的用两个，一个是 <a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~peiyunh/tiny/">Tinyface</a> 能检测到比较小的人脸。可以先玩通<a target="_blank" rel="noopener" href="https://github.com/peiyunh/tiny">demo</a><br>另外一篇更为常用，如果你们对固定场景，比如视频对话，检测效果不错。而且他们能标注人脸关键点。<br>文章：Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networksb(<a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/fddb/results.html">(Code)</a>) 很多最近的创业公司用这个，但是如果场景特殊需要 re-train 一下 </p>
<h4 id="2-人脸跟踪"><a href="#2-人脸跟踪" class="headerlink" title="(2) 人脸跟踪"></a>(2) 人脸跟踪</h4><p>因为这个已经不是学术前沿问题，所以 CVPR、ICCV 上没有文章研究这一块<br>人脸跟踪这一块 openCV 有一个比较好的教程 <a target="_blank" rel="noopener" href="http://opencv-java-tutorials.readthedocs.io/en/latest/06-face-detection-and-tracking.html">http://opencv-java-tutorials.readthedocs.io/en/latest/06-face-detection-and-tracking.html</a><br>但是，工程上通用的做法还是，逐帧用最好的 face detector(比如 tiny face)检测后，用 optical flow 串起来。因为现在 face detection 已经很快了，没必要用 tracking 来加速，能做到很快。具体怎么弄我们可以当面讨论。<br>我的学生找了一下开源库（仅作参考，不建议用），但是不是正规的文章 <a target="_blank" rel="noopener" href="https://github.com/kylemcdonald/ofxFaceTracker">https://github.com/kylemcdonald/ofxFaceTracker</a> </p>
<span id="more"></span>

<h4 id="3-人脸识别"><a href="#3-人脸识别" class="headerlink" title="(3) 人脸识别"></a>(3) 人脸识别</h4><p>主要人脸识别的文章都在 LFW 上， <a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/results.html">http://vis-www.cs.umass.edu/lfw/results.html</a> 我们主要可以看 Table 6 Mean classification accuracy û and standard error of the mean SE。<br>不过这些靠前的方法很多没有代码。目前创业公司普遍用[1]。这个我指导过别人使用，[2]的口碑也不错但不是最新的。以建议你们用[1]加大数据 train 就行了。以后，有更好开源代码出来，我再更新。 </p>
<p>[1] A Discriminative Feature Learning Approach for Deep Face Recognition[C] Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qia. ECCV 2016. (<a target="_blank" rel="noopener" href="https://github.com/ydwen/caffe-face">(Code)</a>)<br>[2] OpenFace: A general-purpose face recognition library with mobile applications Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev， CMU-CS-16-118, CMU School of Computer Science，2016 (<a target="_blank" rel="noopener" href="https://cmusatyalab.github.io/openface/">(Code)</a>)</p>
<h4 id="4-人脸三维建模"><a href="#4-人脸三维建模" class="headerlink" title="(4) 人脸三维建模"></a>(4) 人脸三维建模</h4><p>这一块属于比较前沿的方面，所以基本没公开数据库，也没公开代码</p>
<p>业内公认比较好的是：<br>文章：Real-time Facial Animation on Mobile Devices <a target="_blank" rel="noopener" href="http://www.kunzhou.net/2013/mface-gmod.pdf">(Code)</a></p>
<p>下面这两篇是基于关键点的<br>文章：3D Shape Regression for Real-time Facial Animation <a target="_blank" rel="noopener" href="http://www.kunzhou.net/2013/vface.pdf">(Code)</a></p>
<p>文章：Face Alignment Across Large Poses: A 3D Solution  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.07212">(Code)</a><br>注：这问题属于坑比较多的，大家也不放代码用来赚钱，目前，是浙大的 Zhou Kun 有成熟的技术，需要购买的话，我可以去联系。</p>
<h3 id="2-物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文"><a href="#2-物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文" class="headerlink" title="2.物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文"></a>2.物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文</h3><h4 id="1-物体探测和物体定位"><a href="#1-物体探测和物体定位" class="headerlink" title="(1) 物体探测和物体定位"></a>(1) 物体探测和物体定位</h4><p>物体探测也称物体检测（object detection）包括物体定位（object localization）和物体识别（object recognition）两部分。一般讲是先定位物体在哪里，然后识别是什么（猫，狗，车）。但是自从 faster RCNN 后，物体定位和物体识别就同时一起做了。目前主要的开源代码是 SSD，faster RCNN, Yolo。各有优劣，SSD 和 faster RCNN 是 recall 比较高，Yolo 是 precision 比较高。 综合上来看，如果一定要选一个的话，我推荐 Yolo。注意，目前的 object detection 是用 mAP 来衡量，但 mAP 差个几个点范围内很难说明实际效果的好坏。我们组写了一个 object detection 的详细结束文件（和这份文件一起交付）。</p>
<p>另外还有一篇是刚刚出来的 Mask RCNN，性能比现在的物体检测器都好。 文章在这里 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a></p>
<p>具体中文讨论在这里 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/57403701">https://www.zhihu.com/question/57403701</a> 但是，没有代码，我们组也在复现中</p>
<h4 id="2-视频中的物体探测"><a href="#2-视频中的物体探测" class="headerlink" title="(2) 视频中的物体探测"></a>(2) 视频中的物体探测</h4><p>学术上这个被称为多物体跟踪（mutli-object tracking），他的基本原理是跟踪和物体检测联合训练。 主要的算法可以在这里两个网站上查到：<br>这个是专门做多物体跟踪的： <a target="_blank" rel="noopener" href="https://motchallenge.net/">https://motchallenge.net/</a><br>里面有一些有文章<br>另一个网站是： <a target="_blank" rel="noopener" href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php">http://www.cvlibs.net/datasets/kitti/eval_tracking.php</a><br>这个虽然是无人车的，但原理差不多，你们可以用他们的模型</p>
<p>如果代码的话推荐这篇，是我朋友的工作， <a target="_blank" rel="noopener" href="http://yuxng.github.io/">http://yuxng.github.io/</a><br>这两篇文章都有代码，<br>Learning to Track: Online Multi-Object Tracking by Decision Making.<br>Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection.（这篇没有 model，需要的话，我可以发给你们做研究用，但是如果要商用请和作者联系。） </p>
<p>另外 multi-object tracking 都比较慢，如果要快点的话，可以使用 yolo 每帧做处理，然后简单复现这篇文章，就行了。<br>Seq-NMS for Video Object Detection <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.08465">https://arxiv.org/abs/1602.08465</a></p>
<h4 id="3-场景分类、场景分析"><a href="#3-场景分类、场景分析" class="headerlink" title="(3) 场景分类、场景分析"></a>(3) 场景分类、场景分析</h4><p>场景分类本质上一个图像分类问题，你们只要对图像的场景打上多类标签，进入分类器训练就行了，目前最好的分类器，在 ResNet 之后有两个更好 ResNext，kaiming he 之后的作品。 </p>
<p>• <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.05431">Aggregated Residual Transformations for Deep Neural Networks</a>, <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/ResNeXt">(Code)</a><br>• <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1602.07261">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/slim/nets/ince9ption_resnet_v2.py">(Code)</a></p>
<p>主流的场景分类数据库是 <a target="_blank" rel="noopener" href="http://places.csail.mit.edu/">MIT scene dataset</a>你们可以好好用来做 pre-train<br>这里有一个 demo，比较 cool，你们可以直接用 <a target="_blank" rel="noopener" href="http://places.csail.mit.edu/demo.html">http://places.csail.mit.edu/demo.html</a></p>
<h3 id="3-图片场景描述的算法、论文"><a href="#3-图片场景描述的算法、论文" class="headerlink" title="3.图片场景描述的算法、论文"></a>3.图片场景描述的算法、论文</h3><p>这个在学术上叫 image-captioning 对于这个问题 coco 有一个排名，你们可以在这个排名上找到比较靠前的文章和代码，但是衡量好坏的 metric 比较受诟病。也就是分数高的不一定效果好，所以这个还是你要自己感受一下。 <a target="_blank" rel="noopener" href="http://mscoco.org/dataset/#captions-leaderboard">http://mscoco.org/dataset/#captions-leaderboard</a> </p>
<p>我先简单列出几个有代表性的，且有代码的</p>
<p>• 论文：Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, arXiv:1411.4555. <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/im2txt">(Code)</a><br>• 论文：Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015. <a target="_blank" rel="noopener" href="https://github.com/karpathy/neuraltalk">(Code)</a><br>• 论文：Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, arXiv:1502.03044 &#x2F; ICML 2015 <a target="_blank" rel="noopener" href="https://github.com/kelvinxu/arctic-captions">(Code)</a><br>• 论文：Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, arXiv:1411.2539. <a target="_blank" rel="noopener" href="https://github.com/ryankiros/visual-semantic-embedding">(Code)</a><br>• Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, arXiv:1411.4389<a target="_blank" rel="noopener" href="https://github.com/garythung/torch-lrcn">(Code)</a><br>• 论文：Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv:1505.04467 代码：虽然没代码但是实现起来比较简单 </p>
<h3 id="4-视频场景描述的算法、论文"><a href="#4-视频场景描述的算法、论文" class="headerlink" title="4.视频场景描述的算法、论文"></a>4.视频场景描述的算法、论文</h3><p>我推荐几篇有代码的文章 </p>
<p>• Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence–Video to Text, arXiv:1505.00487. <a target="_blank" rel="noopener" href="https://gist.github.com/vsubhashini/38d087e140854fee4b14">(Code)</a><br>• Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729. <a target="_blank" rel="noopener" href="https://gist.github.com/vsubhashini/3761b9ad43f60db9ac3d">(Code)</a><br>• Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029 <a target="_blank" rel="noopener" href="https://github.com/yaoli/arctic-capgen-vid">(Code)</a></p>
<h3 id="5-人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等"><a href="#5-人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等" class="headerlink" title="5.人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等"></a>5.人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等</h3><p>• <strong>人脸数据库</strong> LFW，<a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/">http://vis-www.cs.umass.edu/lfw/</a> 6千对人脸图片，用于验证（判断是否为某个人）<br>• <strong>FDDB</strong>，<a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/fddb/">http://vis-www.cs.umass.edu/fddb/</a> 2800 张用于检测，测试<br>• <strong>CELEBA</strong> <a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a> 20 万多张，用于配准，检测，属性分析，<br>• <strong>AFLW</strong> <a target="_blank" rel="noopener" href="https://lrs.icg.tugraz.at/research/aflw/">https://lrs.icg.tugraz.at/research/aflw/</a> 2 万 3 千多图片，有检测，经常用于训练。<br>• <strong>megaface</strong> <a target="_blank" rel="noopener" href="http://megaface.cs.washington.edu/">http://megaface.cs.washington.edu/</a> 100 万张图片，人脸识别和验证都有（现阶段比较热门的数据集）<br>• <strong>中科院数据库</strong> <a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html">http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html</a> 50 万人脸验证数据，1 万多个人<br>• <strong>物体检测数据库</strong> 如果你们做小规模验证可以使用 VOC dataset <a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/">http://host.robots.ox.ac.uk/pascal/VOC/</a> 大概是万级别的数量<br>• 如果大规模话有两个数据集 <strong>Imagenet</strong> <a target="_blank" rel="noopener" href="http://image-net.org/">http://image-net.org/</a> 和 <strong>COCO</strong> <a target="_blank" rel="noopener" href="http://mscoco.org/">http://mscoco.org/</a> 这两个都百万级别的<br>• 调参技巧这个很难说，要看具体情况，很难说给出一个普遍的定论。主要是 learning rate 吧，开始的时候比较大，后面比较稳定的时候慢慢减小。 </p>
<h3 id="6-自然场景中的文字检测的算法"><a href="#6-自然场景中的文字检测的算法" class="headerlink" title="6 自然场景中的文字检测的算法"></a>6 自然场景中的文字检测的算法</h3><p>这个问题的话，大量的算法和论文库在这里： <a target="_blank" rel="noopener" href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition">https://github.com/chongyangtao/Awesome-Scene-Text-Recognition</a><br>把这些文章和代码看完就差不多了。这一块做得最好的华中科技大学的 xiang bai。需要购买技术的话，我可以联系。</p>
<p>如果代码推荐的话，可以试一下<br><a target="_blank" rel="noopener" href="https://github.com/baidu-research/warp-ctc">https://github.com/baidu-research/warp-ctc</a><br><a target="_blank" rel="noopener" href="https://github.com/bgshih/crnn">https://github.com/bgshih/crnn</a> </p>
<p>可以是这样两篇文章看看：<br>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.<br>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. </p>
<h3 id="7-自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。"><a href="#7-自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。" class="headerlink" title="7 自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。"></a>7 自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。</h3><p>这个学术界有一个专门的 topic，叫做 Person Re-identification。这一块是中山大学的 weishi zhen 做得最好，需要购买技术，我可以办你们联系。我做了一个调研如下（推荐第一个）， </p>
<p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.07528">Xiao T, Li H, Ouyang W, et al. Learning deep feature representations with domain guided dropout for person re-identification[C]&#x2F;&#x2F;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1249-1258.</a>, <a target="_blank" rel="noopener" href="https://github.com/Cysu/dgd_person_reid">(Code)</a></p>
<p>[2] Yang Yang, LongyinWen, Siwei Lyu, Stan Z. Li,Unsupervised Learning of Multi-Level Descriptors for Person Re-Identification, Association for the Advancement of Artificial Intelligence (AAAI), San Francisco, California, USA, 2017<br>(Code): need email to author </p>
<p>[3] <a target="_blank" rel="noopener" href="http://t.cn/R6imzBZ">Matsukawa T, Okabe T, Suzuki E, et al. Hierarchical gaussian descriptor for person re-identification[C]&#x2F;&#x2F;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1363-1372.</a>, <a target="_blank" rel="noopener" href="http://www.i.kyushu-u.ac.jp/~matsukawa/ReID_files/ReID_GOG_v1.01.zip">(Code)</a></p>
<p>[4] <a target="_blank" rel="noopener" href="http://isee.sysu.edu.cn/files/resource/Top-push_Video-based_Person_Re-identification.pdf">You J, Wu A, Li X, et al. Top-push video-based person re-identification[C]&#x2F;&#x2F;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1345-1353. </a>, <a target="_blank" rel="noopener" href="http://isee.sysu.edu.cn/files/resource/TDL.zip">(Code)</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.05666.pdf">Zheng Z, Zheng L, Yang Y. A Discriminatively Learned CNN Embedding for Person Re-identification[J]. arXiv preprint arXiv:1611.05666, 2016.</a>, <a target="_blank" rel="noopener" href="https://github.com/layumi/2016_person_re-ID">(Code)</a></p>
<p>[6] <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf">Ahmed E, Jones M, Marks T K. An improved deep learning architecture for person re-identification[C]&#x2F;&#x2F;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3908-3916.</a>, <a target="_blank" rel="noopener" href="https://github.com/Deep-Learning-Person-Re-Identification/Implementaion-1">(Code)</a></p>
<p>[7] <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liao_Person_Re-Identification_by_2015_CVPR_paper.pdf">Liao S, Hu Y, Zhu X, et al. Person re-identification by local maximal occurrence representation and metric learning[C]&#x2F;&#x2F;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 2197-2206.</a>, <a target="_blank" rel="noopener" href="http://www.openpr.org.cn/index.php/(Code)-For-Image-ProcessingAnd-Computer-Vision/102-LOMO-Feature-Extraction-and-XQDA-Metric-Learning-for-Person-Re-identification/View-details.html">(Code)</a></p>
<p>[8] <a target="_blank" rel="noopener" href="http://t.cn/R6i1zBW">Zheng L, Wang S, Tian L, et al. Query-adaptive late fusion for image search and person re-identification[C]&#x2F;&#x2F;Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1741-1750.</a> (<a target="_blank" rel="noopener" href="https://drive.google.com/open?id=0B6tjyrV1YrHedWZ2UGlWVHFiQUE">(Code) google drive</a>, <a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1nt3hvex">(Code) baidu pan</a>)</p>
<p>[9] <a target="_blank" rel="noopener" href="http://t.cn/R6irPZf">Yang Y, Yang J, Yan J, et al. Salient color names for person re-identification[C]&#x2F;&#x2F;European Conference on Computer Vision. Springer International Publishing, 2014: 536-551.</a>, (Code): need email to author </p>
<p>[10] <a target="_blank" rel="noopener" href="http://t.cn/R6iBIQm">Bazzani L, Cristani M, Murino V. Symmetry-driven accumulation of local features for human characterization and re-identification[J]. Computer Vision and Image Understanding, 2013, 117(2): 130-144.</a>, <a target="_blank" rel="noopener" href="https://github.com/lorisbaz/SDALF">(Code)</a></p>
<p>[11] <a target="_blank" rel="noopener" href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/papers/FeiXiong_ECCV14.pdf">Xiong F, Gou M, Camps O, et al. Person re-identification using kernel-based metric learning methods[C]&#x2F;&#x2F;European conference on computer vision. Springer International Publishing, 2014: 1-16.</a>, <a target="_blank" rel="noopener" href="https://github.com/NEU-Gou/kernel-metric-learning-reid">(Code)</a></p>
<p>[12] <a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~rzhao/project/salience_cvpr13/zhaoOWcvpr13.pdf">Zhao R, Ouyang W, Wang X. Unsupervised salience learning for person re-identification[C]&#x2F;&#x2F;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2013: 3586-3593.</a>, <a target="_blank" rel="noopener" href="https://github.com/Robert0812/salience_reid">(Code)</a></p>
<p>[13] <a target="_blank" rel="noopener" href="http://t.cn/R6iRdLN">Farenzena M, Bazzani L, Perina A, et al. Person re-identification by symmetry-driven accumulation of local features[C]&#x2F;&#x2F;Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010: 2360-2367. </a>, <a target="_blank" rel="noopener" href="https://github.com/lorisbaz/SDALF">(Code)</a></p>
<h2 id="0x02-相关问题"><a href="#0x02-相关问题" class="headerlink" title="0x02 相关问题"></a>0x02 相关问题</h2><p>1、低分辨率下的视频人脸识别（32*32 以下）<br>答： 这种一般没办法弄，你们可以尝试的做去 block，或者去噪预处理，会有一点点提升。  </p>
<p>2、视屏中，带遮挡（遮挡会移动）的人脸识别<br>答： 如果遮挡不大的话，深度学习对这方面是有一定鲁棒性的  </p>
<p>3、现有的人脸检测准确率不高的情况下，是否有办法通过结合深度学习和传统人脸检测方法来达到快速的高效方法<br>答：如果，你们人脸区间固定（比如人脸登录），可以先提取深度学习特征，然后做 add-boosting.做检测框。 Fast-RCNN 差不多能用。 To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection </p>
<p>4、在训练数据不全面的情况（如不存在带眼镜的情况下），如果被识别人带了眼镜等饰物，如何不受干扰。（对于眼镜的情况，除了在数据增广中寻找眼睛加上遮挡，还有其他办法吗）<br>答：这个使用办法解决的，一般用深度学习产生去眼镜照片，再识别。 Robust Deep Auto-encoder for Occluded Face Recognition </p>
<h2 id="0x03-学术前沿"><a href="#0x03-学术前沿" class="headerlink" title="0x03 学术前沿"></a>0x03 学术前沿</h2><h3 id="1-当前图像、视频领域的学术前沿研究方向、算法框架及相关论文"><a href="#1-当前图像、视频领域的学术前沿研究方向、算法框架及相关论文" class="headerlink" title="1.当前图像、视频领域的学术前沿研究方向、算法框架及相关论文"></a>1.当前图像、视频领域的学术前沿研究方向、算法框架及相关论文</h3><p>目前，比较前沿的大概有几个方向： </p>
<h4 id="1-生成对抗模型："><a href="#1-生成对抗模型：" class="headerlink" title="(1) 生成对抗模型："></a>(1) 生成对抗模型：</h4><p>• Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Networks, NIPS, 2014. （最早的一篇）<br>• Mehdi Mirza, Simon Osindero， Conditional Generative Adversarial Nets，arXiv:1411.1784 [cs.LG] （比较出名的一篇） </p>
<p>这些都是近期作品</p>
<p>• Jost Tobias Springenberg, “Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks”, ICLR 2016<br>• Harrison Edwards, Amos Storkey, “Censoring Representations with an Adversary”, ICLR 2016,<br>• Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros, “Generative Visual Manipulation on the Natural Image Manifold”, ECCV 2016.<br>• Mixing Convolutional and Adversarial Networks ◦Alec Radford, Luke Metz, Soumith Chintala, “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”, ICLR 2016.   </p>
<h4 id="2-深度增强学习"><a href="#2-深度增强学习" class="headerlink" title="(2) 深度增强学习"></a>(2) 深度增强学习</h4><p>• <a target="_blank" rel="noopener" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">Reinforcement Learning Course by David Silver</a><br>• <a target="_blank" rel="noopener" href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf">Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</a><br>• Highly related with Silver’s course, you can read&#x2F;skip the corresponding chapters while taking the courses </p>
<p>一些开始的论文 f Deep Reinforcement Learning (DQN)<br>• Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013, December 20). Playing Atari with Deep Reinforcement Learning. arXiv.org. SJTU Machine Vision and Intelligence Group page 7<br>• Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. </p>
<p>一些最近的论文<br>• Wang, Z., de Freitas, N., &amp; Lanctot, M. (2015). Dueling Network Architectures for Deep Reinforcement Learning. CoRR.<br>• van Hasselt, H., Guez, A., &amp; Silver, D. (2016). Deep Reinforcement Learning with Double Q-Learning. AAAI.<br>• Hausknecht, M. J., &amp; Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. AAAI.<br>• Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S., &amp; Munos, R. (2015). Increasing the Action Gap - New Operators for Reinforcement Learning. CoRR, cs.AI.<br>• Osband, I., Blundell, C., Pritzel, A., &amp; Van Roy, B. (2016, February 15). Deep Exploration via Bootstrapped DQN. arXiv.org.<br>• Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2015). Prioritized Experience Replay. CoRR.<br>• Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016, February 5). Asynchronous Methods for Deep Reinforcement Learning. arXiv.org. </p>
<p>一些应用有关的论文<br>• Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484–489.<br>• Mnih, V., Heess, N., &amp; Graves, A. (2014). Recurrent models of visual attention. In Advances in Neural Information Processing Systems (pp. 2204-2212). SJTU Machine Vision and Intelligence Group page 8<br>• Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., &amp; Farhadi, A. (2016, September 17). Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. arXiv.org. </p>
<h4 id="3-人体姿态估计"><a href="#3-人体姿态估计" class="headerlink" title="(3) 人体姿态估计"></a>(3) 人体姿态估计</h4><p>代表文章有两篇：<br>• Haoshu Fang, Shuqin Xie, Cewu Lu， RMPE: Regional Multi-person Pose Estimation， arXiv:1612.00137 [cs.CV]<br>• Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh， Realtime Multi-person 2D Pose Estimation using Part Affinity Fields， CVPR 2017 </p>
<h4 id="4-视觉问答"><a href="#4-视觉问答" class="headerlink" title="(4) 视觉问答"></a>(4) 视觉问答</h4><p>这方面的一些文章</p>
<p>• Xiong, Caiming, Stephen Merity, and Richard Socher. “Dynamic Memory Networks for Visual and Textual Question Answering.” arXiv:1603.01417 (2016).<br>• Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, arXiv:1505.01121.<br>• Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.<br>• Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering, arXiv:1505.05612.<br>• Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han, Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction, arXiv:1511.05765<br>• Yang, Z., He, X., Gao, J., Deng, L., &amp; Smola, A. (2015). Stacked Attention Networks for Image Question Answering. arXiv:1511.02274.<br>• Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Multimodal Residual Learning for Visual QA, arXiv:1606:01455<br>• Hyeonwoo Noh and Bohyung Han, Training Recurrent Answering Units with Joint Loss Minimization for VQA, arXiv:1606.03647<br>• Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Hadamard Product for Low-rank Bilinear Pooling, arXiv:1610.04325. </p>
<h4 id="5-Mask-R-CNN"><a href="#5-Mask-R-CNN" class="headerlink" title="(5) Mask-R-CNN"></a>(5) Mask-R-CNN</h4><p>另外还有一篇是刚刚出来的 Mask RCNN，也是比较火。</p>
<p>文章在这里 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a><br>具体中文讨论在这里 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/57403701">https://www.zhihu.com/question/57403701</a><br>但是，没有代码，我们组也在复现中 </p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>请我一杯咖啡吧！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay-ssss.jpg" alt="S1NH 微信">
        <span>微信</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>S1NH
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://s1nh.org/post/cv-sjtu/" title="计算机视觉技术调查报告">http://s1nh.org/post/cv-sjtu/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/jetson-tx-2-all-you-know/" rel="prev" title="Jetson TX-2 入门 -- 全部你应该知道的">
                  <i class="fa fa-angle-left"></i> Jetson TX-2 入门 -- 全部你应该知道的
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/post/making-a-coffeebot-1/" rel="next" title="新工作：制作一个送咖啡机器人——第一周">
                  新工作：制作一个送咖啡机器人——第一周 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2015 – 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">S1NH</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/duchengyao" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>
<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"duchengyao/duchengyao.github.io","issue_term":"og:title","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js" defer></script>

</body>
</html>
