<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="manifest" href="/images/site.webmanifest">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="/lib/@fortawesome/fontawesome-free/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="/lib/@fancyapps/fancybox/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"s1nh.org","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Give an image, the task is to generate natural Question based on the image.  Another list of VQA https:&#x2F;&#x2F;github.com&#x2F;jokieleung&#x2F;awesome-visual-question-answering A survey of Image Caption in Chinese">
<meta property="og:type" content="article">
<meta property="og:title" content="A Survey of Visual Question Generation">
<meta property="og:url" content="http://s1nh.org/post/vqg-survey/index.html">
<meta property="og:site_name" content="S1NH">
<meta property="og:description" content="Give an image, the task is to generate natural Question based on the image.  Another list of VQA https:&#x2F;&#x2F;github.com&#x2F;jokieleung&#x2F;awesome-visual-question-answering A survey of Image Caption in Chinese">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qiniu.s1nh.org/vqg-intro.svg">
<meta property="og:image" content="http://qiniu.s1nh.org/vqg-traditional.svg">
<meta property="og:image" content="http://qiniu.s1nh.org/vqg-pipeline-full.svg">
<meta property="og:image" content="http://qiniu.s1nh.org/vqg-iq-inference-full.svg">
<meta property="article:published_time" content="2020-09-21T11:35:09.000Z">
<meta property="article:modified_time" content="2022-03-07T09:04:03.791Z">
<meta property="article:author" content="S1NH">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="VQG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://qiniu.s1nh.org/vqg-intro.svg">


<link rel="canonical" href="http://s1nh.org/post/vqg-survey/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://s1nh.org/post/vqg-survey/","path":"post/vqg-survey/","title":"A Survey of Visual Question Generation"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>A Survey of Visual Question Generation | S1NH</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?4c66a84272e0f7943a305accf6dbdf41"></script>




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">S1NH</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">世界在旅程的尽头终结</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0x01-Datasets"><span class="nav-number">1.</span> <span class="nav-text">0x01. Datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-VQA"><span class="nav-number">1.0.1.</span> <span class="nav-text">I. VQA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#II-VQG-COCO-Bing-Flicker"><span class="nav-number">1.0.2.</span> <span class="nav-text">II.VQG-COCO&#x2F;Bing&#x2F;Flicker</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#III-TextVQA"><span class="nav-number">1.0.3.</span> <span class="nav-text">III. TextVQA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#IV-Visual-Genome"><span class="nav-number">1.0.4.</span> <span class="nav-text">IV. Visual Genome</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#V-VizWiz"><span class="nav-number">1.0.5.</span> <span class="nav-text">V. VizWiz</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VI-Visual-Dialog"><span class="nav-number">1.0.6.</span> <span class="nav-text">VI. Visual Dialog</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x02-Researchers"><span class="nav-number">2.</span> <span class="nav-text">0x02. Researchers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x03-Architecture"><span class="nav-number">3.</span> <span class="nav-text">0x03. Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0-Rule-based-%E2%9D%8C"><span class="nav-number">3.1.</span> <span class="nav-text">0. Rule-based ❌</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-GRNN"><span class="nav-number">3.2.</span> <span class="nav-text">1. GRNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-IQ"><span class="nav-number">3.3.</span> <span class="nav-text">2. IQ</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0x04-Experiments"><span class="nav-number">4.</span> <span class="nav-text">0x04 Experiments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0xFD-Metrics"><span class="nav-number">5.</span> <span class="nav-text">0xFD Metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Word-overlap-metrics"><span class="nav-number">5.1.</span> <span class="nav-text">1. Word-overlap metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-BLEU"><span class="nav-number">5.1.1.</span> <span class="nav-text">1.1. BLEU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-ROUGE"><span class="nav-number">5.1.2.</span> <span class="nav-text">1.2. ROUGE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-METOR"><span class="nav-number">5.1.3.</span> <span class="nav-text">1.3. METOR</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Embedding-based-metrics"><span class="nav-number">5.2.</span> <span class="nav-text">2. Embedding-based metrics</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#0xFD"><span class="nav-number">6.</span> <span class="nav-text">0xFD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0xFE-Open-Source-Project"><span class="nav-number">6.1.</span> <span class="nav-text">0xFE. Open Source Project</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0xFF-Papers-sorted-by-date"><span class="nav-number">6.2.</span> <span class="nav-text">0xFF. Papers (sorted by date)</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="S1NH"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">S1NH</p>
  <div class="site-description" itemprop="description">no other developers required.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">85</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">83</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/lib/@creativecommons/vocabulary/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.92ez.com/" title="https:&#x2F;&#x2F;www.92ez.com&#x2F;" rel="noopener" target="_blank">一只猿</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://s1nh.com/" title="http:&#x2F;&#x2F;s1nh.com&#x2F;" rel="noopener" target="_blank">本站 github 镜像</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://s1nh.org/" title="http:&#x2F;&#x2F;s1nh.org&#x2F;">本站国内镜像</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/duchengyao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://s1nh.org/post/vqg-survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="S1NH">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="S1NH">
      <meta itemprop="description" content="no other developers required.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="A Survey of Visual Question Generation | S1NH">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Survey of Visual Question Generation
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-21 19:35:09" itemprop="dateCreated datePublished" datetime="2020-09-21T19:35:09+08:00">2020-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">算法与硬件</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <img data-src="http://qiniu.s1nh.org/vqg-intro.svg" width = 70% div align=center />


<p>Give an image, the task is to generate natural Question based on the image.</p>
<ul>
<li>Another list of VQA <a target="_blank" rel="noopener" href="https://github.com/jokieleung/awesome-visual-question-answering">https://github.com/jokieleung/awesome-visual-question-answering</a></li>
<li>A survey of Image Caption in Chinese <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27771046">https://zhuanlan.zhihu.com/p/27771046</a></li>
<li>Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation [<a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Albert_Gatt/publication/315696017_Survey_of_the_State_of_the_Art_in_Natural_Language_Generation_Core_tasks_applications_and_evaluation/links/58e4909caca272d62977a6b8/Survey-of-the-State-of-the-Art-in-Natural-Language-Generation-Core-tasks-applications-and-evaluation.pdf">Link</a></li>
</ul>
<span id="more"></span>

<h2 id="0x01-Datasets"><a href="#0x01-Datasets" class="headerlink" title="0x01. Datasets"></a>0x01. Datasets</h2><h4 id="I-VQA"><a href="#I-VQA" class="headerlink" title="I. VQA"></a>I. <a target="_blank" rel="noopener" href="http://visualqa.org/">VQA</a></h4><blockquote>
<p>Antol, Stanislaw, et al. “<strong>Vqa: Visual question answering.</strong>“ <em>Proceedings of the IEEE international conference on computer vision.</em> 2015.</p>
<p>Zhang, Peng, et al. “<strong>Yin and yang: Balancing and answering binary visual questions.</strong>“ <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</em> 2016.</p>
<p>Goyal, Yash, et al. “<strong>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.</strong>“ <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</em> 2017.</p>
</blockquote>
<p>VQA is a new dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer.</p>
<ul>
<li>265,016 images (COCO and abstract scenes)</li>
<li>At least 3 questions (5.4 questions on average) per image</li>
<li>10 ground truth answers per question</li>
<li>3 plausible (but likely incorrect) answers per question</li>
<li>Automatic evaluation metric</li>
</ul>
<h4 id="II-VQG-COCO-Bing-Flicker"><a href="#II-VQG-COCO-Bing-Flicker" class="headerlink" title="II.VQG-COCO/Bing/Flicker"></a>II.<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/download/details.aspx?id=53670">VQG-COCO/Bing/Flicker</a></h4><blockquote>
<p>Mostafazadeh, Nasrin, et al. “<strong>Generating Natural Questions About an Image.</strong>“ <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</em> 2016.</p>
</blockquote>
<p>This dataset is described in <a target="_blank" rel="noopener" href="http://aclanthology.info/papers/generating-natural-questions-about-an-image">http://aclanthology.info/papers/generating-natural-questions-about-an-image</a>. The dataset is comprised of 9 csv’s, organized first by the source of the image, Bing, MSCOCO, or Flickr, then by type of dataset, train, dev and test. Within each file, we organize by image_id, the link to the image, and the up to 5 natural questions authored by crowdworkers on Amazon Mechnical Turk in response to the image. Please be sure to maintain these files separately in order to report system accuracy and progress on dev and test sets. For the Bing images, the dataset includes up to 5 captions for each image link; captions for the COCO and Flickr images are available elsewhere. In addition, each of the test set files includes the human rating of the question necessary to compute the deltaBleu score (see <a target="_blank" rel="noopener" href="http://aclanthology.info/papers/deltableu-a-discriminative-metric-for-generation-tasks-with-intrinsically-diverse-targets">http://aclanthology.info/papers/deltableu-a-discriminative-metric-for-generation-tasks-with-intrinsically-diverse-targets</a>).</p>
<h4 id="III-TextVQA"><a href="#III-TextVQA" class="headerlink" title="III. TextVQA"></a>III. <a target="_blank" rel="noopener" href="https://textvqa.org/dataset">TextVQA</a></h4><h4 id="IV-Visual-Genome"><a href="#IV-Visual-Genome" class="headerlink" title="IV. Visual Genome"></a>IV. <a target="_blank" rel="noopener" href="http://visualgenome.org/api/v0/api_home.html">Visual Genome</a></h4><h4 id="V-VizWiz"><a href="#V-VizWiz" class="headerlink" title="V. VizWiz"></a>V. <a target="_blank" rel="noopener" href="https://vizwiz.org/">VizWiz</a></h4><h4 id="VI-Visual-Dialog"><a href="#VI-Visual-Dialog" class="headerlink" title="VI. Visual Dialog"></a>VI. <a target="_blank" rel="noopener" href="https://visualdialog.org/">Visual Dialog</a></h4><h2 id="0x02-Researchers"><a href="#0x02-Researchers" class="headerlink" title="0x02. Researchers"></a>0x02. Researchers</h2><ul>
<li><p> Indian Institute of Technology, <a target="_blank" rel="noopener" href="https://badripatro.github.io/index.html">Badri N. Patro</a> &amp; <a target="_blank" rel="noopener" href="https://vinaypn.github.io/">Vinay P. Namboodiri</a> <em>这哥们近期发了6篇有关VQG/VQA的文章，其中三篇被录用，两篇已经开源（不过star很少）</em></p>
</li>
<li><p>Microsoft, <strong>Nan Duan</strong>, <strong>Duyu Tang</strong>, <strong>Tong Wang</strong>(Maluuba)</p>
</li>
<li><p>Google DeepMind, Oriol Vinyals</p>
</li>
<li><p>Nasrin Mostafazadeh</p>
</li>
<li><p>Alexander Toshev</p>
</li>
</ul>
<h2 id="0x03-Architecture"><a href="#0x03-Architecture" class="headerlink" title="0x03. Architecture"></a>0x03. Architecture</h2><h3 id="0-Rule-based-❌"><a href="#0-Rule-based-❌" class="headerlink" title="0. Rule-based ❌"></a>0. Rule-based ❌</h3><h3 id="1-GRNN"><a href="#1-GRNN" class="headerlink" title="1. GRNN"></a>1. GRNN</h3><p><code>2016 Microsoft</code></p>
<img data-src="http://qiniu.s1nh.org/vqg-traditional.svg" width = 70% div align=center />




<h3 id="2-IQ"><a href="#2-IQ" class="headerlink" title="2. IQ"></a>2. IQ</h3><p><strong>with VAE</strong>/<strong>maximizes mutual information</strong>/<strong>doesn’t need to know the expected answer</strong></p>
<img data-src="http://qiniu.s1nh.org/vqg-pipeline-full.svg" width = 90% div align=center  />


<p><strong>pipeline</strong></p>
<img data-src="http://qiniu.s1nh.org/vqg-iq-inference-full.svg" width = 90% div align=center />


<h2 id="0x04-Experiments"><a href="#0x04-Experiments" class="headerlink" title="0x04 Experiments"></a>0x04 Experiments</h2><p>pass</p>
<h2 id="0xFD-Metrics"><a href="#0xFD-Metrics" class="headerlink" title="0xFD Metrics"></a>0xFD Metrics</h2><ul>
<li><p><code>word-overlap metrics</code>  BLEU, METEOR, ROUGE etc.</p>
</li>
<li><p><code>embedding-based metrics</code>  Skip-Thought, Embedding average, Vector extrema, Greedy matching etc.</p>
</li>
</ul>
<h3 id="1-Word-overlap-metrics"><a href="#1-Word-overlap-metrics" class="headerlink" title="1. Word-overlap metrics"></a>1. Word-overlap metrics</h3><h4 id="1-1-BLEU"><a href="#1-1-BLEU" class="headerlink" title="1.1. BLEU"></a>1.1. BLEU</h4><p><code>Widely used in the machine translation literature.</code></p>
<p><strong>Feature</strong></p>
<ul>
<li>Focus on precision, don’t care about recall.</li>
<li>Regardless of word order.</li>
</ul>
<p><strong>Math</strong></p>
<p>First compute brevity penalty BP shows below:</p>
<p>$$BP=\begin{cases}<br>1, &amp;if \ c &gt; r\<br>e^{\ (\ 1 - r / c\ )}, &amp;if \ c&lt;r<br>\end{cases}$$</p>
<p>Where c is the total length of the candidate translation corpus, and r is the effective reference corpus length.</p>
<p>Next, compute the geometric average of the modified <strong>n-gram precisions</strong>, $p_n$, using n-grams up to length N and positive weights $w_n$ summing to one.</p>
<p>Then, BELU is shown below:</p>
<p>$$BLEU = BP \cdot exp\left(\sum_{n=1}^N w_n \log p_n\right)$$</p>
<p>Where $exp\left(\sum_{n=1}^N w_n \log p_n\right)$ represents the weighted sum of the logarithms of the accuracy of different n-grams</p>
<p>And the ranking behavior is more immediately apparent in the log domain.</p>
<p>$$log \ BLEU = min(1-\frac{r}{c},0) + \sum_{n=1}^N w_n \log p_n$$</p>
<p><strong>Ref.</strong><br>[<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/P02-1040.pdf">1</a>] [<a target="_blank" rel="noopener" href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/">2</a>]</p>
<h4 id="1-2-ROUGE"><a href="#1-2-ROUGE" class="headerlink" title="1.2. ROUGE"></a>1.2. ROUGE</h4><p><code>Is almost same as BLEU, but caculate recall instead of precision.</code></p>
<h4 id="1-3-METOR"><a href="#1-3-METOR" class="headerlink" title="1.3. METOR"></a>1.3. METOR</h4><p><code>Not only based on exact matches but also stem, synonym, and paraphrase matches.</code></p>
<h3 id="2-Embedding-based-metrics"><a href="#2-Embedding-based-metrics" class="headerlink" title="2. Embedding-based metrics"></a>2. Embedding-based metrics</h3><ul>
<li>Skip-Thought</li>
<li>Embedding average</li>
<li>Vector extrema</li>
<li>Greedy matching</li>
</ul>
<h2 id="0xFD"><a href="#0xFD" class="headerlink" title="0xFD"></a>0xFD</h2><ul>
<li><a target="_blank" rel="noopener" href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">Conditional Variational Autoencoders</a> (<a target="_blank" rel="noopener" href="https://www.cnblogs.com/wangxiaocvpr/p/6231019.html">in chinese</a>)</li>
</ul>
<h3 id="0xFE-Open-Source-Project"><a href="#0xFE-Open-Source-Project" class="headerlink" title="0xFE. Open Source Project"></a>0xFE. Open Source Project</h3><p><strong>VQA:</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pythia">Pythia</a> (<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1Z9fsh10rFtgWe4uy8nvU4mQmqdokdIRR">Demo</a>)</li>
<li><a target="_blank" rel="noopener" href="https://delta-lab-iitk.github.io/U-CAM/">U-CAM</a></li>
</ul>
<p><strong>VQG: (sorted by stars)</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/yikang-li/iQAN">iQAN (CVPR 2018)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/chingyaoc/VQG-tensorflow">VQG 1 (ACL 2016, unofficial)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/naver/aqm-plus">AQM+ (ICLR 2019)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ranjaykrishna/iq">IQ (CVPR 2019)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/badripatro/Visual_Question_Generation">MDNVQG (EMNLP 2018)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/chennaveh/VQG">VQG 2 (ACL 2016, unofficial)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/gitlost-murali/Natural-Questions-Generation-from-Images">VQG 3 (ACL 2016, unofficial)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/danishnxt/NLP-Project-VisualQuesGen">VQG 4 (ACL 2016, unofficial)</a></li>
</ul>
<h3 id="0xFF-Papers-sorted-by-date"><a href="#0xFF-Papers-sorted-by-date" class="headerlink" title="0xFF. Papers (sorted by date)"></a>0xFF. Papers (sorted by date)</h3><ul>
<li><p>Gao, J., Galley, M., &amp; Li, L. (2019). <strong>Neural approaches to conversational ai.</strong> Foundations and Trends® in Information Retrieval, 13(2-3), 127-298.[<a target="_blank" rel="noopener" href="http://research.baidu.com/Public/ueditor/upload/file/20181029/1540796086725537.pdf">Poster</a>] [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.08267.pdf">Paper</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Kurmi, V., Kumar, S., &amp; Namboodiri, V. (2020). <strong>Deep Bayesian Network for Visual Question Generation.</strong> In The IEEE Winter Conference on Applications of Computer Vision (pp. 1566-1576).</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Patel, S., &amp; Namboodiri, V. P. (2019). <strong>Granular Multimodal Attention Networks for Visual Dialog.</strong> arXiv preprint arXiv:1910.05728.[<a target="_blank" rel="noopener" href="http://xxx.itp.ac.cn/pdf/1910.05728.pdf">Paper</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Lunayach, M., Patel, S., &amp; Namboodiri, V. P. (2019). <strong>U-cam: Visual explanation using uncertainty based class activation maps.</strong> In Proceedings of the IEEE International Conference on Computer Vision (pp. 7444-7453). [<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.pdf">Paper</a>] [<a target="_blank" rel="noopener" href="https://delta-lab-iitk.github.io/U-CAM/">Proj</a>] [<a target="_blank" rel="noopener" href="https://github.com/DelTA-Lab-IITK/U-CAM">code</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, &amp; Namboodiri, V. P. (2019). <strong>Deep Exemplar Networks for VQA and VQG.</strong> arXiv preprint arXiv:1912.09551.</p>
</li>
<li><p><strong>Patro, B. N.</strong>, &amp; Namboodiri, V. P. (2019). <strong>Probabilistic framework for solving Visual Dialog.</strong> arXiv preprint arXiv:1909.04800.[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.04800.pdf">Paper</a>]</p>
</li>
<li><p>Lee, S. W., Gao, T., Yang, S., Yoo, J., &amp; Ha, J. W. (2019). <strong>Large-Scale Answerer in Questioner’s Mind for Visual Dialog Question Generation.</strong> ICLR 2019.[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.08355">Paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/naver/aqm-plus">code</a>]</p>
</li>
<li><p>Jedoui, K., Krishna, R., Bernstein, M., &amp; Fei-Fei, L. (2019). <strong>Deep Bayesian Active Learning for Multiple Correct Outputs.</strong> arXiv preprint arXiv:1912.01119.</p>
</li>
<li><p>Krishna, R., Bernstein, M., &amp; Fei-Fei, L. (2019). <strong>Information maximizing visual question generation.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2008-2018).[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Krishna_Information_Maximizing_Visual_Question_Generation_CVPR_2019_paper.pdf">Paper</a>] [<a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/ranjaykrishna/iq/index.html">Proj</a>] [<a target="_blank" rel="noopener" href="https://github.com/ranjaykrishna/iq">Code</a>]</p>
</li>
<li><p>Fan, Z., Wei, Z., Wang, S., Liu, Y., &amp; Huang, X. J. (2018, August). A reinforcement learning framework for natural question generation using bi-discriminators. In Proceedings of the 27th International Conference on Computational Linguistics (pp. 1763-1774).[<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/C18-1150.pdf">Paper</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Kumar, S., Kurmi, V. K., &amp; Namboodiri, V. P. (2018). <strong>Multimodal differential network for visual question generation.</strong> In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 4002-4012). [<a target="_blank" rel="noopener" href="http://aclweb.org/anthology/D18-1434">Paper</a>]. [<a target="_blank" rel="noopener" href="https://badripatro.github.io/MDN-VQG/">Project Link</a>] [<a target="_blank" rel="noopener" href="https://github.com/badripatro/Visual_Question_Generation">code</a>]</p>
</li>
<li><p>[49] Li, Y., <strong>Duan, N.</strong>, Zhou, B., Chu, X., Ouyang, W., Wang, X., &amp; Zhou, M. (2018). <strong>Visual question generation as dual task of visual question answering.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6116-6124). [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.07192">Paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/yikang-li/iQAN">code</a>]</p>
</li>
<li><p>[362] Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., … &amp; Batra, D. (2017). <strong>Visual dialog.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 326-335).</p>
</li>
<li><p>Zhang, J., Wu, Q., Shen, C., Zhang, J., Lu, J., &amp; Hengel, A. V. D. (2017). <strong>Asking the difficult questions: Goal-oriented visual question generation via intermediate rewards.</strong> arXiv preprint arXiv:1711.07614. [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.07614">Paper</a>]</p>
</li>
<li><p>[41] <strong>Wang, T.</strong>, Yuan, X., &amp; Trischler, A. (2017). <strong>A joint model for question answering and question generation.</strong> arXiv preprint arXiv:1706.01450.</p>
</li>
<li><p>[59] Tang, D., <strong>Duan, N.</strong>, Qin, T., Yan, Z., &amp; Zhou, M. (2017). Question answering and question generation as dual tasks. arXiv preprint arXiv:1706.02027.</p>
</li>
<li><p>[65] Jain, U., Zhang, Z., &amp; Schwing, A. G. (2017). <strong>Creativity: Generating diverse questions using variational autoencoders.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6485-6494). [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.03493">Paper</a>].</p>
</li>
<li><p>[67] <strong>Mostafazadeh, N.</strong>, Brockett, C., Dolan, B., Galley, M., Gao, J., Spithourakis, G. P., &amp; Vanderwende, L. (2017). <strong>Image-grounded conversations: Multimodal context for natural question and response generation.</strong> IJCNLP (pp. 462–472). [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.08251">Paper</a>].</p>
</li>
<li><p>[62] <strong>Duan, N.</strong>, Tang, D., Chen, P., &amp; Zhou, M. (2017, September). Question generation for question answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 866-874).</p>
</li>
<li><p>[24] Zhang, S., Qu, L., You, S., Yang, Z., &amp; Zhang, J. (2016). <strong>Automatic Generation of Grounded Visual Questions.</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.06530">Paper</a>]</p>
</li>
<li><p>[131] Mostafazadeh, N., Misra, I., Devlin, J., Mitchell, M., He, X., &amp; Vanderwende, L. (2016). <strong>Generating natural questions about an image.</strong> In ACL, the Association for Computational Linguistics (pp. 1802-1813).[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06059">Paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/gitlost-murali/Natural-Questions-Generation-from-Images">code1</a>] [<a target="_blank" rel="noopener" href="https://github.com/chennaveh/VQG">code2</a>] [<a target="_blank" rel="noopener" href="https://github.com/chingyaoc/VQG-tensorflow">code3</a>] [<a target="_blank" rel="noopener" href="https://github.com/danishnxt/NLP-Project-VisualQuesGen">code4</a>]</p>
</li>
<li><p>Yang, Y., Li, Y., Fermuller, C., &amp; Aloimonos, Y. (2015). <strong>Neural Self Talk: Image Understanding via Continuous Questioning and Answering.</strong>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03460">Paper</a>].</p>
</li>
<li><p>Carl Saldanha, Visual Question Generation</p>
</li>
<li><p>[3410] Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015). <strong>Show and tell: A neural image caption generator.</strong> In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1411.4555.pdf">Paper</a>] [<a target="_blank" rel="noopener" href="https://github.com/KranthiGV/Pretrained-Show-and-Tell-model">code1</a>] [<a target="_blank" rel="noopener" href="https://github.com/nikhilmaram/Show_and_Tell">code2</a>] [<a target="_blank" rel="noopener" href="https://github.com/jazzsaxmafia/show_and_tell.tensorflow">code3</a>]</p>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>S1NH
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://s1nh.org/post/vqg-survey/" title="A Survey of Visual Question Generation">http://s1nh.org/post/vqg-survey/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/VQG/" rel="tag"># VQG</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/inference-using-tensorrt-backend/" rel="prev" title="Inference using TensorRT Backend.">
                  <i class="fa fa-chevron-left"></i> Inference using TensorRT Backend.
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/post/jetson-nano-reduce-memory-usage/" rel="next" title="「Jetson Nano」 Reduce Memory Usage">
                  「Jetson Nano」 Reduce Memory Usage <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2015 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">S1NH</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="/lib/animejs/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="/lib/jquery/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="/lib/@fancyapps/fancybox/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="/lib/lozad/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/lib/hexo-generator-searchdb/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"/lib/mathjax/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"duchengyao/duchengyao.github.io","issue_term":"og:title","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
